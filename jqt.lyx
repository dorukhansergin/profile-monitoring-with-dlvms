#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrartcl
\begin_preamble



\graphicspath{{./figs/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.eps}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage{cleveref}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{stfloats}
\usepackage[super]{nth}
\usepackage{subcaption}
\usepackage{caption}
% for inkscape images
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgf}



\g@addto@macro\@floatboxreset\centering


% \usepackage{hyperref}
 
 \AtBeginDocument{% Overrides ref for Cref
 	\let\ref\Cref
 }

\crefalias{prop}{proposition}

% TIKZ
\usepackage{tikz}

% -- Arrows
\usetikzlibrary{arrows}

% --Bayesnet
\usetikzlibrary{bayesnet}
\usetikzlibrary{decorations.pathreplacing}

\tikzset{
	diagonal fill/.style 2 args={fill=#2, path picture={
			\fill[#1, sharp corners] (path picture bounding box.south west) -|
			(path picture bounding box.north east) -- cycle;}},
	reversed diagonal fill/.style 2 args={fill=#2, path picture={
			\fill[#1, sharp corners] (path picture bounding box.north west) |- 
			(path picture bounding box.south east) -- cycle;}}
}

\tikzstyle{partialobs} = [latent,diagonal fill={gray!25}{gray!0}]
\end_preamble
\options headings=standardclasses
\use_default_options false
\begin_modules
theorems-ams
theorems-sec
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding T1
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "newtxmath" "auto"
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plainnat
\biblio_options isbn=false,url=false,eprint=false
\biblatex_bibstyle chicago-authordate
\biblatex_citestyle chicago-authordate
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author -1806609307 "Hao yan"
\author -767166615 "Dorukhan Sergin"
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% INPUT PREAMBLES
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset CommandInset include
LatexCommand include
filename "preamble/preamble_glossary.lyx"

\end_inset


\change_inserted -767166615 1590439094

\end_layout

\begin_layout Title

\change_inserted -767166615 1590439190
Toward a Better 
\change_inserted -1806609307 1590809304
Monitoring 
\change_inserted -767166615 1590439190
Statistic
\change_deleted -1806609307 1590809384
 Formulation
\change_inserted -767166615 1590439190
 for Profile Monitoring with Variational Autoencoders
\end_layout

\begin_layout Abstract

\change_deleted -1806609307 1590774964
Due to the recent success of deep learning models, nonlinear profile monitoring
 schemes based on deep neural networks have attracted increasing interest.
 A number of such monitoring schemes were proposed specifically for variational
 autoencoders, a mainstay deep generative model.
 While these works show impressive results over classical methods, the statistic
 formulations they propose ignore shortcomings of learned lower-dimensional
 representations and computational limitations of real-world high-dimensional
 systems in their proposed formulations.
 In this work, we first manifest these issues and then overcome them with
 a novel statistic formulation that both increases out-of-control detection
 accuracy and computational efficiency.
 We demonstrate our results both on a carefully designed simulation study
 and a real-life example of image profiles obtained from a hot steel rolling
 process.
\change_inserted -1806609307 1590774964
Due to the recent success of deep learning models, nonlinear profile monitoring
 schemes based on deep generative models specifically for variational autoencode
rs.
 While these works show impressive results over classical methods, the proposed
 monitoring statistic often ignore shortcomings of learned lower-dimensional
 representations and computational limitations of real-world high-dimensional
 systems.
 In this work, we first manifest these issues and then overcome them with
 a novel statistic formulation that both increases out-of-control detection
 accuracy and computational efficiency.
 We demonstrate our results both on a carefully designed simulation study
 and a real-life example of image profiles obtained from a hot steel rolling
 process.
 
\change_inserted -767166615 1590519796

\begin_inset Note Note
status open

\begin_layout Plain Layout

\change_inserted -767166615 1590509732
~100 words abstract required, right now this is 121, to check in LyX, highlight
 the text then use: Tools -> Statistics
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Abstract

\change_inserted -767166615 1590519814
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Abstract

\change_inserted -767166615 1590519956

\series bold
Keywords: 
\series default
Control charts, deep learning, high-dimensionality, latent variable modeling,
 profile monitoring
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:introduction"

\end_inset

 Profile monitoring has attracted a growing interest in the literature in
 the past decades for its ability to construct control charts with much
 better representations for certain types of process measurements
\begin_inset CommandInset citation
LatexCommand citep
key "Woodall2004-bp,Woodall2007-xs,Maleki2018-uo"
literal "false"

\end_inset

.
 A profile can be defined as a functional relationship between the response
 variables and explanatory variables or spatiotemporal coordinates.
 In this work, we focus on the case where the profiles generated from the
 process are high-dimensional (HD)
\change_inserted -767166615 1590443326

\shape italic
\emph on
, 
\change_deleted -767166615 1590443322

\shape default
\emph default
—
\change_unchanged

\shape italic
\emph on
i.e.
\shape default
\emph default
, the number of such explanatory variables or spatiotemporal coordinates
 are large.
 Specifically, we focus on sets of HD profiles for which intra-sample variation
 lies on a nonlinear low-dimensional manifold 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
 Our motivating example of such HD profiles is presented in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\change_inserted -767166615 1590510690
below,
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted -767166615 1590510751
TandF author guideline says:
\end_layout

\begin_layout Plain Layout

\change_inserted -767166615 1590510754
Tables and figures: Indicate in the text where the tables and figures should
 appear, for example by inserting [Table 1 near here].
\end_layout

\begin_layout Plain Layout

\change_inserted -767166615 1590510747
That's why you'll see phrases like below after figures in the document so
 please don'd delete them.
\change_unchanged

\end_layout

\end_inset

 
\change_unchanged
in which we exhibit a sample of surface defect image profiles collected
 from a hot steel rolling process.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/profile_examples.pdf
	width 25theight%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
A collection of 64 by 64 image profiles taken from a hot steel rolling process.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Rolling"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In literature, profile monitoring techniques can be categorized by their
 assumptions on the type of functional relationship.
 Linear profile monitoring can be considered the most basic profile monitoring
 technique, in which it is assumed that the profile can be represented by
 a linear function.
 The idea is to extract the slope and the intercept from each profile and
 monitor its coefficients 
\begin_inset CommandInset citation
LatexCommand citep
key "zhu2009monitoring"
literal "false"

\end_inset

.
 Regularization techniques can also be used in linear profile estimation.
 For example, 
\begin_inset CommandInset citation
LatexCommand citet
key "zou2012lasso"
literal "false"

\end_inset

 utilizes a multivariate linear regression model for profiles with the LASSO
 penalty and use the regression coefficients for Phase-II monitoring.
 However, the linearity assumption can be quite limiting.
 To address this challenge, nonlinear parametric models are proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "Williams2007-ty,Jensen2009-tu,Noorossana2011-oj,Maleki2018-uo"
literal "false"

\end_inset

.
 These models assume an explicit family of parameterized functions and,
 their parameters are estimated via nonlinear regression.
 In both cases, the drawback of both linear and nonlinear parametric models
 is that they assume the parametric form is known beforehand, which might
 not always be the case.
\end_layout

\begin_layout Standard
Another large body of profile monitoring research focuses on the type of
 profiles where the basis of the representation is assumed to be known,
 but the coefficients are unknown.
 For instance, to monitor smooth profiles, various non-parametric methods
 based on local kernel regression 
\begin_inset CommandInset citation
LatexCommand citep
key "zou2008monitoring,qiu2010nonparametric"
literal "false"

\end_inset

 and splines 
\begin_inset CommandInset citation
LatexCommand citep
key "chang2010statistical"
literal "false"

\end_inset

 are developed.
 To monitor the non-smooth waveform signals, a wavelet-based mixed effect
 model is proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "paynabar2011characterization"
literal "false"

\end_inset

.
 However, for all the aforementioned methods, it is assumed that the nonlinear
 variation pattern of the profile is well captured by a known basis or kernel.
 Usually, there is no guidance on selecting the right basis of the representatio
n for the original data and it normally requires many trial and error.
\end_layout

\begin_layout Standard
In the case that the basis of HD profiles are not known, dimensionality
 reduction techniques are widely used.
 Principal component analysis (PCA) is arguably the most popular method
 in this context for profile data monitoring because of its simplicity,
 scalability, and good data compression capability.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "liu1995control"
literal "false"

\end_inset

, PCA is proposed to reduce the dimensionality of the streaming data and,
 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 charts are constructed to monitor the extracted representations and residuals,
 respectively.
 To generalize PCA methods to monitor the complex correlation among the
 channels of multi-channel profiles, 
\begin_inset CommandInset citation
LatexCommand citet
key "paynabar2015change"
literal "false"

\end_inset

 propose a multivariate functional PCA method and apply change point detection
 methods on the function coefficients.
 Along this line, tensor-based PCA methods are also proposed for multi-channel
 profiles, examples including uncorrelated multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
key "paynabar2013monitoring"
literal "false"

\end_inset

 and multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
key "grasso2014profile"
literal "false"

\end_inset

.
 Finally, various tensor-based PCA methods 
\begin_inset CommandInset citation
LatexCommand citep
key "yan2015image"
literal "false"

\end_inset

 are compared and different test statistics are developed for tensor-based
 process monitoring.
\end_layout

\begin_layout Standard
The main limitation of all the aforementioned PCA-related methods is that
 the expressive power of linear transformations is very limited.
 Furthermore, each principal component represents a global variation pattern
 of the original profiles, which is not efficient at capturing the local
 spatial correlation within a single profile.
 Therefore, PCA requires much larger latent space dimensions than the dimension
 of the actual latent space, yielding a sub-optimal and overfitting-prone
 representation.
 This phenomenon hinders profile monitoring performance.
\end_layout

\begin_layout Standard
A systematic discussion of this issue is articulated in 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
 In that work, the authors identify the problems associated with assuming
 a closeness relationship in the subspace that is characterized by Euclidean
 metrics.
 They successfully observe that the intra-sample variation in complex high-dimen
sional corpora may lie on a nonlinear manifold as opposed to a linear manifold,
 which is assumed by PCA and related methods.
 However, the authors only focus on applying manifold learning for Phase-I
 analysis, while the Phase-II monitoring procedure is not touched upon
\change_deleted -767166615 1590528380
 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset


\change_unchanged
.
\end_layout

\begin_layout Standard
Deep dimensionality reduction models have been proposed as an alternative
 to classical dimensionality reduction techniques in a handful.
 Deep autoencoders have been proposed for profile monitoring for Phase-I
 analysis in 
\begin_inset CommandInset citation
LatexCommand citep
key "Howard2018-op"
literal "false"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Yan2016-wa"
literal "false"

\end_inset

 compared the performance of contractive autoencoders and denoising autoencoders
 for Phase-II monitoring.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang2018-js"
literal "false"

\end_inset

 proposed a denoising autoencoder for process monitoring.
 Aside from deterministic deep neural networks, only three works 
\begin_inset CommandInset citation
LatexCommand citep
key "wang2019systematic,Zhang2019-lu,lee2019process"
literal "false"

\end_inset

 proposed to use deep probabilistic latent variable models, specifically,
 variational autoencoders (VAE), for Phase-II monitoring.
 All of the proposed monitoring statistics in those works differ slightly,
 but they are all extensions of the classic 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

-charts of PCA
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I don't think this is exactly what we are proving
\end_layout

\end_inset

.
 We argue that there is room for improvement for the monitoring statistic
 formulations in those works for 
\change_deleted -767166615 1590444326
a number of
\change_inserted -767166615 1590444329
several
\change_unchanged
 reasons, especially when high-dimensional profiles are considered.
 In this work, we propose a new monitoring statistic formulation to address
 this issue.
\end_layout

\begin_layout Standard
The contributions of this work are as follows:
\end_layout

\begin_layout Itemize
We compare all the existing monitoring statistics for deep latent variable
 models with the focus on the VAE model and propose a better monitoring
 statistic that responds robustly and, in most cases, more accurately than
 previous formulations requiring only a single pass through the autoencoder.
\change_inserted -1806609307 1590775886

\end_layout

\begin_layout Itemize

\change_inserted -1806609307 1590809457
We give insight into the existing monitoring statistics for latent-variable
 models with a focus on variational autoencoders and classify them based
 on the latent-variable based and residual-based monitoring statistics.
 We also show how these statistics are natural extensions to the monitoring
 statistics for traditional dimension reduction methods such as probalistic
 PCA.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted -1806609307 1590776220
I don't understand why we're emphasizing this too much.
 It's not a central proof to our arguments.
 I feel like we're drawing too much attention there for no reason.
 It is also somewhat irrelevant to the first part of the sentence
\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590776221
Hao: 
\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590776601
I think this is important since JQT likes connection with existing/traditional
 works.
 We mention our major contriution is to give insight on VAE based monitoring
 statistics and show this is a natural extension.
 
\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Itemize
Through extensive experiments,
\change_inserted -1806609307 1590776328
 unlike traditional monitoring statistics,
\change_unchanged
 we demonstrate and give insight on why latent variable-based monitoring
 statistics should not be used due to the two issues related to deep 
\change_inserted -1806609307 1590775387
neural network 
\change_unchanged
encoders: 1) their representations are likely entangled 2) they will likely
 fail to extrapolate well beyond the region of in-control profiles.
\end_layout

\begin_layout Itemize
We derive two approximations on the residual-based monitoring statistics
 leveraging on the first-order and second-order Taylor expansion and compare
 with the sampling-based monitoring statistics.
 We have demonstrated that the first-order approximation of the residual-based
 monitoring statistics gives the best
\change_inserted -1806609307 1590809508
 overall
\change_unchanged
 detection accuracy and is computationally efficient.
 
\change_deleted -1806609307 1590776099

\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I don't understand why we're emphasizing this too much.
 It's not a central proof to our arguments.
 I feel like we're drawing too much attention there for no reason.
 It is also somewhat irrelevant to the first part of the sentence
\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Itemize
We demonstrate the effectiveness of this new formulation on both simulation
 and real-life case studies and conclude that residual-based monitoring
 statistics with deep learning architecture as encoders and decoders outperform
 other monitoring statistics and the traditional
\change_inserted -1806609307 1590775848
 non-deep-learning-based
\change_unchanged
 methods.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
The rest of the paper is organized as follow
\color black
s.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Background"
plural "false"
caps "false"
noprefix "false"

\end_inset

 first introduces variational autoencoders and reviews traditional 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 charts of PCA as well as 
\change_deleted -1806609307 1590776976
  
\change_unchanged
the existing monitoring statistics for VAE.

\color inherit
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology"
plural "false"
caps "false"
noprefix "false"

\end_inset

 introduces the proposed monitoring statistic and gives insights on its
 meaning and mathematical relationship with the traditional PCA methods.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Simulation-Study-Analysis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the simulation study to demonstrate the performance of the proposed
 method and give insight into why only the residual-based statistics is
 recommended and how to make sure it is computationally efficient.
 Finally,
\change_inserted -1806609307 1590777167
 
\change_unchanged

\begin_inset CommandInset ref
LatexCommand ref
reference "sec:case-study"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates the advantages of the proposed methodology on a real-life
 case 
\lang american
study
\lang english
, using images from hot-steel rolling processes.
 
\change_inserted -767166615 1590444608

\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted -767166615 1590444683
we don't conclude the paper with future work and this part is probably unnecessa
ry in the 
\begin_inset Quotes eld
\end_inset

the rest of the paper organized as follows
\begin_inset Quotes erd
\end_inset

.
 I've seen many good papers simply don't mention the conclusion section
 because it's pretty much the same across all papers.
\change_inserted -1806609307 1590776963

\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590776965
Hao: 
\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590776967
I am fine with it.
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Section
Background 
\begin_inset CommandInset label
LatexCommand label
name "sec:Background"

\end_inset


\end_layout

\begin_layout Subsection
Variational Autoencoders 
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:lvms"

\end_inset


\end_layout

\begin_layout Standard
In this section, we introduce Variational Autoencoder (VAE) 
\begin_inset CommandInset citation
LatexCommand citep
key "Kingma2013-dl"
literal "false"

\end_inset

, which is the primary modeling tool in this work.
 The Gaussian factorized latent variable model perspective of VAEs is crucial
 to understand the role of this model in the context of profile monitoring.
 This is why we begin with an introduction to latent variable modeling.
\end_layout

\begin_layout Standard
Latent variables models are powerful tools to model complex distributions
 over high-dimensional spaces.
 The underlying assumption is that there exists a low-dimensional latent
 structure that explains well the variations in the high-dimensional observed
 space.
 Typically, the density over observed variables can be decoupled into the
 distribution on the latent variables 
\begin_inset Formula $\pz$
\end_inset

 and the conditional distribution of observed variables given latent variables
 
\begin_inset Formula $p(\mbx\g\mbz)$
\end_inset

.
 Then, these distributions can be assigned to tractable families of distribution
s, such as Gaussian.

\change_inserted -1806609307 1590777333
 
\change_unchanged
This enables 
\change_deleted -1806609307 1590777266
 
\change_unchanged
a more efficient modeling of the data distribution 
\begin_inset Formula $p(\mbx)$
\end_inset

.
\end_layout

\begin_layout Standard
A typical example of latent variable models is when the joint distribution
 is Gaussian factorized as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Formula 
\begin{equation}
\begin{split}\pz & =\Norm(\mbz;0,\mbI_{r})\\
\decoding & =\Norm(\mbx;\mu_{\mbtheta}(\mbz),\sigma^{2}\mbI_{d})\\
p_{\mbtheta}(\mbx,\mbz) & =\decoding\pz
\end{split}
\label{eq:gaussian-factorized}
\end{equation}

\end_inset

In the above formulation, 
\begin_inset Formula $\mbx\in\R^{d}$
\end_inset

 are observed samples, and 
\begin_inset Formula $\mbz\in\R^{r}$
\end_inset

 are latent variables while 
\begin_inset Formula $\mu_{\mbtheta}\colon\R^{r}\to\R^{d}$
\end_inset

 is a function parameterized by 
\begin_inset Formula $\mbtheta\in\Theta$
\end_inset

, which describes the relationship between the latent variables and the
 mean of the conditional distribution.
 The Gaussian prior 
\begin_inset Formula $\pz$
\end_inset

 is typically chosen to be standard multivariate 
\change_deleted -767166615 1590445376
normal
\change_inserted -767166615 1590445377
Gaussian
\change_unchanged
 distribution to avoid degenerate solutions 
\begin_inset CommandInset citation
LatexCommand citep
key "roweis1999unifying"
literal "false"

\end_inset

 and conditional covariance is typically assumed to be isotropic 
\begin_inset Formula $\sigma^{2}I_{d}$
\end_inset

 to avoid ill-defined problems.
 The aim is to approximate the true density 
\begin_inset Formula $p_{\mbtheta}(\mbx)\approx p(\mbx)$
\end_inset

 and this approximation can be obtained through marginalization: 
\begin_inset Formula 
\[
p_{\mbtheta}(\mbx)=\int p_{\mbtheta}(\mbx,\mbz)d\mbz
\]

\end_inset

Finally, in literature, there have been discussions about whether the independen
t latent structure assumption 
\begin_inset Formula $\pz=\Norm(\mbz;0,\mbI_{r})$
\end_inset

 can lead to the discovery of the true disentangled variations, a task also
 known as disentangled representation learning 
\begin_inset CommandInset citation
LatexCommand citep
after "Sec. 3.5"
key "bengio2013representation"
literal "false"

\end_inset

.
 Disentangled representations are useful to represent variations in latent
 variations due to their ability to separate the independent factors.
 However, a discussion on what disentangled representation implies for profile
 monitoring is necessary.
 We are interested in whether and if so, how such representations will be
 critical for profile monitoring.
\end_layout

\begin_layout Standard
A famous member of the family of models described above is the probabilistic
 principal component analysis (PPCA) 
\begin_inset CommandInset citation
LatexCommand citep
key "tipping1999probabilistic"
literal "false"

\end_inset

.
 The parameters are optimized via a maximum likelihood estimation framework
 and it can be solved analytically since 
\begin_inset Formula $\mu_{\mbtheta}$
\end_inset

 is a simple linear transformation.
 This enables reusing analytical results from solutions to the classical
 PCA problem.
 The assumption of PPCA that the latent and observed variables have a strictly
 linear relationship is restrictive.
 In real-world processes, it is likely that this relationship is highly
 nonlinear.
 Deep latent variable models are a marriage of deep neural networks and
 latent variable models that aim to solve this problem.
 Deep learning has enjoyed a tremendous resurgence in the last decade due
 to their superior performance that was unprecedented for many tasks such
 as image classification 
\begin_inset CommandInset citation
LatexCommand citep
key "krizhevsky2012imagenet"
literal "false"

\end_inset

, machine translation 
\begin_inset CommandInset citation
LatexCommand citep
key "bahdanau2014neural"
literal "false"

\end_inset

, and speech recognition 
\begin_inset CommandInset citation
LatexCommand citep
key "amodei2016deep"
literal "false"

\end_inset

.
 In theory, under sufficient conditions, a two-layer multilayer perceptron
 can approximate any function on a bounded region 
\begin_inset CommandInset citation
LatexCommand citep
key "cybenko1989approximation,Hornik1991-li"
literal "false"

\end_inset

.
 However, growing the width of shallow networks in an exponential fashion
 for arbitrarily complex tasks is not practical.
 It has been shown that deeper representations can often achieve better
 expressive power than shallow networks with fewer parameters due to the
 efficient reuse of the previous layers 
\begin_inset CommandInset citation
LatexCommand citep
key "eldan2016power"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
VAE is arguably the most foundational member of the deep latent variable
 model family.
 The main difference between PPCA and VAE is that VAE replaces the linear
 transformation with a high-capacity deep neural network (called 
\shape italic
generative
\shape default
 or 
\shape italic
decoder
\shape default
).
 This is powerful in the sense that, along with a general-purpose prior
 
\begin_inset Formula $\pz$
\end_inset

, 
\change_deleted -767166615 1590458551
 
\change_unchanged
deep neural networks can transfer such prior 
\change_deleted -1806609307 1590782668
into
\change_inserted -1806609307 1590782670
to model
\change_unchanged
 a wide variety of densities
\change_inserted -1806609307 1590782695
 to model the training data
\change_unchanged
 
\begin_inset CommandInset citation
LatexCommand citep
key "kingma2019introduction"
literal "false"

\end_inset

.
 Unlike PPCA, these models will not have analytical solutions due to the
 complex nature of the neural network used.
 Like most other deep learning models, their parameters have to be optimized
 via gradient descent for maximum likelihood.
 The problem becomes even harder given the observation that the posterior
 
\begin_inset Formula $\decoding$
\end_inset

 takes meaningful values only for a small sub-region within 
\begin_inset Formula $\R^{r}$
\end_inset

.
 This makes sampling from the prior 
\begin_inset Formula $\pz$
\end_inset

 to estimate the likelihood prohibitively expensive.
 Both models work around this problem using the importance sampling framework
 
\begin_inset CommandInset citation
LatexCommand citep
after "532"
key "bishop2006pattern"
literal "false"

\end_inset

, where they introduce another network (called 
\shape italic
recognition
\shape default
 or 
\shape italic
encoder
\shape default
) to approximate a proposal distribution 
\begin_inset Formula $\encoding$
\end_inset

 —parametrized by 
\begin_inset Formula $\mbphi$
\end_inset

— which will hopefully sample latent variables from a much smaller region
 that is more likely to produce higher posterior densities for a given input
 
\begin_inset Formula $\mbx$
\end_inset

.
 In fact, PPCA can be treated as a special case of VAE, where the decoder
 is modeled by linear transformation.
\end_layout

\begin_layout Standard
One important output of a trained VAE is the likelihood estimator.
 Once the two networks are trained, the log-likelihood 
\begin_inset Formula $\log\ptheta(\mbx)$
\end_inset

 can be approximated by a Monte Carlo sampling procedure with 
\begin_inset Formula $L$
\end_inset

 iterations 
\begin_inset CommandInset citation
LatexCommand citep
after "30"
key "kingma2019introduction"
literal "false"

\end_inset

: 
\begin_inset Formula 
\begin{equation}
\log\ptheta(\mbx)\approx\log\frac{1}{L}\sum_{l=1}^{L}\frac{\ptheta(\mbx,\mbz^{(l)})}{\qphizgivenx{\mbz^{(l)}}{\mbx}}\label{eqn:SummationLL}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
VAEs are trained to maximize the so-called evidence lower bound (ELBO),
 which is deemed a proxy to the likelihood: 
\begin_inset Formula 
\begin{equation}
\begin{split}\text{ELBO} & \triangleq\log\left(p(\mbx)\right)-\KL{\encoding}{q^{*}(\mbz|\mbx)}\\
 & =\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)}
\end{split}
\label{eqn:VAELoss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\KL{\cdot}{\cdot}$
\end_inset

 denotes the Kullback-Leibler divergence (KLD) between two distributions.
 The left-hand side is the quantity of interest, while the right-hand side
 is the tractable expression that guides the updating of parameters 
\begin_inset Formula $\mbtheta,\mbphi$
\end_inset

 in an end-to-end fashion.
\end_layout

\begin_layout Subsection
Review of 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 
\change_inserted -767166615 1590521759
S
\change_deleted -767166615 1590521758
s
\change_unchanged
tatistics in PCA
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:ReviewPCA"

\end_inset

 Process monitoring via PCA is typically undertaken using the 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics 
\begin_inset CommandInset citation
LatexCommand citep
key "Chen2004-px"
literal "false"

\end_inset

.
 The 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA is defined as the reconstruction error between the real
 sample 
\begin_inset Formula $\mbx$
\end_inset

 and the reconstructed sample 
\begin_inset Formula $\tilde{\mbx}$
\end_inset

.
 Its geometric representation is how far the sample is away from the learned
 subspace of in-control samples.
 
\begin_inset Formula $\Tsq$
\end_inset

 represents how far the latent representation of that sample is away from
 the cluster of latent codes of the in-control samples.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\Tsq$
\end_inset

 statistics and 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA are defined as follows: 
\begin_inset Formula 
\begin{equation}
\begin{split}Q(\mbx) & =\gg\mbx-\tilde{\mbx}\gg^{2}\\
\Tsq_{PCA}(\mbx) & =\mbz^{\top}\mbSigma\inv_{r}\mbz=\mbx^{\top}\mbW_{r}\mbSigma\inv_{r}\mbW_{r}^{\top}\mbx,
\end{split}
\label{eqn: QTPCA}
\end{equation}

\end_inset

where matrix 
\begin_inset Formula $\mbW_{r}$
\end_inset

 is the loading matrix, and 
\begin_inset Formula $\mbSigma\inv_{r}$
\end_inset

 is the inverse of the covariance matrix when only the first 
\begin_inset Formula $r$
\end_inset

 principal components are kept.
 There are various methods to choose 
\begin_inset Formula $r$
\end_inset

 such as fixing the percentage of variation explained 
\begin_inset CommandInset citation
LatexCommand citep
after "41"
key "Chiang2001-nu"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
For processes with relatively small latent and residual dimensionality,
 the upper control limits of these statistics for the 
\begin_inset Formula $\alpha$
\end_inset

% Type-1 error tolerance is constructed by employing the normality assumptions
 of PPCA 
\begin_inset CommandInset citation
LatexCommand citep
after "43-44"
key "Chiang2001-nu"
literal "false"

\end_inset

.
 However, using such measures for high-dimensional nonlinear profiles is
 prohibitively error-prone as both 
\begin_inset Formula $r$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 will be much higher than the assumptions on chi-square distribution can
 tolerate.
 As an alternative, non-parametric methods to estimate upper percentiles
 are increasingly used for this purpose, such as simple sample percentile
 on a held-out set (i.e., validation set) or fitting kernel density estimation
 to in-control statistics.
\end_layout

\begin_layout Subsection
Review and Critique of Previously Proposed Monitoring Statistics Proposed
 for VAE
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:critique"

\end_inset

 Three works have recently considered VAE for process monitoring, all of
 which propose different statistic formulations for monitoring.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang2019-lu"
literal "false"

\end_inset

 
\change_deleted -1806609307 1590783647
formulate
\change_inserted -1806609307 1590783650
proposed to use only
\change_unchanged
 
\change_deleted -1806609307 1590783632
what they call 
\change_unchanged

\begin_inset Formula $H^{2}$
\end_inset


\change_inserted -1806609307 1590783658
 for the process monitoring
\change_unchanged
, which is basically the Mahalanobis distance of the mean of the proposal
 distribution from standard Gaussian distribution.
 
\begin_inset Formula 
\begin{equation}
H^{2}=\mu_{\mbphi}(\mbx)^{\top}\mu_{\mbphi}(\mbx)
\end{equation}

\end_inset

The major drawback of using only this statistic is that it completely ignores
 the disturbances in residual distribution.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

 claim
\change_inserted -1806609307 1590783695
ed
\change_unchanged
 to 
\change_deleted -1806609307 1590783713
extend
\change_inserted -1806609307 1590783715
use
\change_unchanged
 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 of PCA for VAE.
 For a given input 
\begin_inset Formula $\mbx$
\end_inset

, a single sample is drawn from the proposal distribution 
\begin_inset Formula $\mbz^{(l)}\sim\encoding$
\end_inset

 which is used reconstruct the input using the generative model
\change_inserted -1806609307 1590783727
 
\change_unchanged

\begin_inset Formula $\mbx^{(l)}\sim p_{\mbtheta}(\mbx\g\mbz^{(l)})$
\end_inset

.
 The proposed test statistics in this work are as follows: 
\begin_inset Formula 
\begin{equation}
\begin{aligned}T^{2} & =(\mbz^{(l)}-\bar{\mbz})^{\top}S_{\mbz}\inv(\mbz^{(l)}-\bar{\mbz})\\
SPE & =\gg\mbx^{(l)}-\mbx\gg_{2}^{2},
\end{aligned}
\end{equation}

\end_inset

where 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 are estimated over a single loop from the data.
 The proposed control charting methodology suggest
\change_deleted -1806609307 1590783911
s
\change_inserted -1806609307 1590783911
ed
\change_unchanged
 that these two statistics work in combination and at least one vote of
 either statistics is enough to make a detection
\change_inserted -767166615 1590458854
 decision
\change_unchanged
.
 However, the estimation of the 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 can be slow and unnecessary since for a well trained VAE, 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 would be approximately equal to 0 and 
\begin_inset Formula $I$
\end_inset


\change_inserted -767166615 1590458866
, respectively
\change_unchanged
.
 Instead, the estimation can be unstable when data is limited.
\end_layout

\begin_layout Standard
Finally, 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2019systematic"
literal "false"

\end_inset

 propose
\change_inserted -1806609307 1590783887
d
\change_unchanged
 the 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 statistics by focusing on the two major components of the tractable part
 of the objective function of VAE shown as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\begin_inset Formula $D$
\end_inset

 statistic is simply the KL divergence between the prior and proposal.
 For 
\begin_inset Formula $R$
\end_inset

 statistic, like 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

, they employ summary statistics over samples from proposal but also claim
 that sampling size can be fixed to one: 
\begin_inset Formula 
\begin{equation}
\begin{aligned}D & =\KL{\encoding}{p(\mbz)}\\
R & =\frac{1}{L}\sum_{l=1}^{L}-\log q_{\mbtheta}(\mbx\g\mbz^{(l)}),
\end{aligned}
\label{eq: DR}
\end{equation}

\end_inset


\begin_inset Formula $SPE$
\end_inset

 in and 
\begin_inset Formula $R$
\end_inset

 are essentially the same quantities up to a constant, which makes them
 identical in the context of monitoring statistics.

\change_inserted -767166615 1590458942
 This is why we will refer to them as 
\begin_inset Formula $SPE/R$
\end_inset

 throughout the rest of the paper.
\change_unchanged

\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:methodology"

\end_inset


\change_deleted -767166615 1590458643
 
\change_unchanged

\end_layout

\begin_layout Subsection
Proposed Monitoring Statistic
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:proposed-statistic"

\end_inset


\end_layout

\begin_layout Standard
Log-likelihood, 
\begin_inset Formula $\log\ptheta(\mbx)$
\end_inset

, arises as a natural monitoring statistic candidate in the context of VAEs.
 That is, given a well trained VAE, in-control samples should have relatively
 larger log-likelihood than out-of-control samples.
 However, the required number of Monte Carlo samples—
\begin_inset Formula $L$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:SummationLL"
plural "false"
caps "false"
noprefix "false"

\end_inset

— can be prohibitively large to get meaningful estimates of the likelihood
 
\begin_inset CommandInset citation
LatexCommand citep
key "Kingma2013-dl"
literal "false"

\end_inset

.
 This will make it intractable to monitor high throughput systems in real-time.
 To address this issue, ELBO defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be used for a reasonable approximation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)}\label{eqn: VAEELBO}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
A natural choice of the monitoring statistics is to separate the ELBO into
 two terms 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 and 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

.
 To understand the role of these two terms in process monitoring, we revisit
 the assumptions of the model described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Let us formally represent an out-of-control distribution as 
\begin_inset Formula $p_{\delta}(\mbx)\neq p(\mbx)$
\end_inset

.
 Since 
\begin_inset Formula $p(\mbx)=\int p(\mbx\g\mbz)p(\mbz)d\mbz$
\end_inset

, we can observe two sources of out-of-control behaviors: disturbances in
 latent distribution 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

 and disturbances in residual distribution 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

.
 Note that various combinations of these two disturbances cover disturbances
 in the entire process.
 One can argue that 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 can detect the disturbances in the residual space 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

 and 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 can detect the disturbance in the latent space 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

.

\change_inserted -1806609307 1590793364
 This is actually true for linear latent variable models.

\change_unchanged
 
\change_inserted -1806609307 1590793376
Since 
\change_deleted -1806609307 1590793377
W
\change_inserted -1806609307 1590793377
w
\change_unchanged
e know that for
\change_inserted -1806609307 1590793383
 simpler
\change_unchanged
 
\change_deleted -1806609307 1590793386
processes
\change_inserted -1806609307 1590793386
data
\change_unchanged
 that can be accurately modeled by PCA, both terms play an important role
 
\begin_inset CommandInset citation
LatexCommand citep
key "kim2003process"
literal "false"

\end_inset

 for process monitoring.
 
\change_deleted -1806609307 1590793430
We argue that t
\change_inserted -1806609307 1590793430
T
\change_unchanged
his 
\change_inserted -1806609307 1590793514
also 
\change_unchanged
holds 
\change_deleted -1806609307 1590793512
true
\change_unchanged
 for PPCA 
\change_deleted -1806609307 1590793518
too
\change_inserted -1806609307 1590793466
 and will become the 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics in the traditional monitoring framework
\change_unchanged
.
 To prove this, we link the 
\change_inserted -1806609307 1590793533
the 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 
\change_deleted -1806609307 1590793537
monitoring
\change_unchanged
 statistics of 
\change_inserted -1806609307 1590793483
P
\change_unchanged
PCA (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:ReviewPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

) 
\change_deleted -1806609307 1590793480
to PPCA 
\change_unchanged
using the ELBO framework.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop: T2Q"

\end_inset

 We know from the definition of PPCA 
\begin_inset CommandInset citation
LatexCommand citep
key "tipping1999probabilistic"
literal "false"

\end_inset

 that the prior, encoding and decoding functions are normally distributed
 as: 
\begin_inset Formula 
\[
\begin{split}p(\mbz) & =\Norm(0,\mbI),\\
\decoding & =\Norm(\mbW\mbz,\sigma^{2}\mbI).\label{eq:Gaussian}
\end{split}
\]

\end_inset

In this case, from PPCA, the encoder 
\change_inserted -1806609307 1590784288
can be solved analytically as 
\change_deleted -1806609307 1590784291
also follows the
\change_inserted -1806609307 1590784293
another
\change_unchanged
 normal distribution as 
\begin_inset Formula $\encoding=\Norm(\mu_{\mbphi}(\mbx),\Sigma_{z})$
\end_inset

, where 
\begin_inset Formula $\mu_{\mbphi}(\mbx)=\mbM^{-1}\mbW^{\top}\mbx$
\end_inset

, 
\begin_inset Formula $\Sigma_{z}=\sigma^{2}\mbM^{-1}$
\end_inset

, and 
\begin_inset Formula $\mbM=\mbW^{\top}\mbW+\sigma^{2}\mbI$
\end_inset

.
 Then, the two monitoring statistics
\change_inserted -1806609307 1590793653
 defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn: VAEELBO"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_unchanged
 can be derived as: 
\begin_inset Formula 
\begin{equation}
\KL{\encoding}{p(\mbz)}=\frac{1}{2}\gg\mu_{\mbphi}(\mbx)\gg^{2}+C_{1},\label{eqn:KL_PPCA}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\E_{\mbz\sim q_{\mbphi}}\log\decoding\propto\gg\mbx-\mbW\mu_{\mbphi}(\mbx)\gg^{2}+C_{2},\label{eqn:E_PPCA}
\end{equation}

\end_inset

where 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{2}$
\end_inset

 are constants that doesn't depend on 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
The proof is given in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:PoofOfPropTQ"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that the constants do not affect the profile monitoring decision.
 Thus, the test statistics 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 is equivalent to the 
\begin_inset Formula $T^{2}$
\end_inset

 Statistics of PCA as defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn: QTPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

, and 
\begin_inset Formula $\E_{\mbz\sim q_{\mbphi}}\log\decoding$
\end_inset

 is equivalent the 
\begin_inset Formula $Q$
\end_inset

 statistic in the PCA case.
 Observe that previously proposed formulations mentioned in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

 rely—directly or indirectly—on this framework.
 Statistics 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $SPE$
\end_inset

 are based on 
\begin_inset Formula $Q$
\end_inset

 or in the case of PPCA 
\begin_inset Formula $\E_{\mbz\sim q_{\mbphi}}\log\decoding$
\end_inset

.
 Let us call these 
\emph on
residual-based statistics
\emph default
.
 The statistics 
\begin_inset Formula $H^{2},T^{2}$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 are based on 
\begin_inset Formula $T^{2}$
\end_inset

 of PCA or 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 of PPCA.
 We call these 
\emph on
latent-variable based statistics
\emph default
, as they rely exclusively on latent representations.
\end_layout

\begin_layout Standard
Our first major claim is that latent variable-based statistics are not useful
 for profile monitoring when deep neural network based encoders are used
 to produce latent representations
\change_deleted -1806609307 1590809621
.
 We argue three
\change_inserted -1806609307 1590809626
 with the following 
\change_unchanged
reasons
\change_deleted -1806609307 1590809612
 for that
\change_unchanged
:
\change_deleted -767166615 1590459540
 
\change_inserted -767166615 1590459600

\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted -767166615 1590459650
I'm not sure about these three, it seems to be drifting from the argument
 we're trying to make.
 Except number 3
\change_inserted -1806609307 1590784380

\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590784382
Hao: 
\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590784442
1.
 I think these three make sense.
 
\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590785833
1) We have talked about one also in the simulation.
 And I actually think 1 holds mostly for high-dimensional systems and especially
 for deep learning models.
 So I merged 1 and 2 now.
 Right now, we only have two points to make.
 
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
In high-dimensional complex processes, most of the change or disturbances
 should be expected on residual distribution.
 According to 
\begin_inset CommandInset citation
LatexCommand citet
key "severson2016perspectives"
literal "false"

\end_inset

 faults in complex real-life processes tend to alter the existing relationship
 between latent sources of variation as opposed to pushing to most extreme
 cases in the latent variational sources.

\change_inserted -1806609307 1590784765
 This effect is amplified for
\change_unchanged
 
\change_inserted -1806609307 1590784539
the
\change_deleted -1806609307 1590784525
A
\change_unchanged
 deep autoencoder
\change_inserted -1806609307 1590809637
s since it
\change_unchanged
 typically require
\change_inserted -1806609307 1590784547
s
\change_unchanged
 a much smaller latent dimension compared to PCA, due to better data compression
 capabilities compared to the traditional PCA methods.
 This leaves a much smaller chance for change happening in the latent space.
\end_layout

\begin_layout Enumerate
The mean encoder 
\begin_inset Formula $\mu_{\mbphi}$
\end_inset

—modeled by the deep neural network—is likely to lack two important features:
 disentangled representations and extrapolation capabilities.
 These two qualities are required so that extreme values of latent variables
 consistently converge out of the in-control zone.
 We illustrate this in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:entang-extrap"
plural "false"
caps "false"
noprefix "false"

\end_inset

 below.
 Observe how Point C is falsely identified as in-control when learned representa
tions are not disentangled or when they fail to extrapolate.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figs/Disentangled_Extrapolated.pdf
	lyxscale 50
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Different failure mode possibilities of learned two-dimensional latent space
 representation of the independent source of variation.
 
\series bold
Top:
\series default
 A hypothetical in-control latent space (gray area) and a range of the independe
nt source of variation (ABC line).
 Points A and B are extreme values of in-control range and point C denotes
 an out-of-control sample.
 
\series bold
Bottom Left:
\series default
 A learned representation that fails to extrapolate.
 
\series bold
Bottom Right: 
\series default
A learned representation that is entangled (
\change_deleted -767166615 1590442823
i.e.
\change_inserted -767166615 1590442823
i.e.,
\change_unchanged
 not axis-parallel).
 
\begin_inset CommandInset label
LatexCommand label
name "fig:entang-extrap"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Theorem 1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "locatello2018challenging"
literal "false"

\end_inset

 prove
\change_inserted -1806609307 1590785959
d
\change_unchanged
 that without proper inductive biases injected into the model about sources
 of variation, it is impossible to find disentangled representations.
 Unfortunately, injecting such inductive biases requires unrealistically
 accurate anticipation of variations among in-control samples as well as
 specialized neural network structures, both of which are extremely challenging
 tasks given the potential complexity of the processes that generate high-dimens
ional profiles.
 Moreover, even if the representations are disentangled, correct mappings
 of 
\begin_inset Formula $\encoding$
\end_inset

 may still not be obtained
\change_inserted -1806609307 1590788882
 outside the training regions
\change_unchanged
 due to 
\change_inserted -1806609307 1590788888
the 
\change_unchanged
failure to extrapolate.
 Deep neural networks approximate well only at a bounded domain defined
 by where the training set—in our case, the in-control set—is densely sampled
 from.
 The behavior of the function is often unpredictable outside 
\change_deleted -1806609307 1590789059
that
\change_inserted -1806609307 1590789061
the training
\change_unchanged
 domain.
 In other words, it may not extrapolate well beyond the domain of training
 samples.
 We refer interested readers to 
\begin_inset CommandInset ref
LatexCommand ref
reference "app:rosenbrock"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted -1806609307 1590785937
,
\change_unchanged
 where we replicated this phenomenon on a toy example.
 To see why this is a problem, first note that the encoder 
\begin_inset Formula $\encoding$
\end_inset

 will only be trained with profiles densely sampled from the bounded region
 of in-control samples, for which 
\begin_inset Formula $\pz$
\end_inset

 are high.
 The behavior of the encoder is uncertain for profiles coming from dense
 regions of the out-of-control latent structure 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

.
 We expect increased false negatives should the model falsely map these
 profiles onto high-density regions of 
\begin_inset Formula $\pz$
\end_inset

 but not 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

.
\end_layout

\begin_layout Standard
Unlike latent variable-based statistics, extrapolation 
\change_deleted -1806609307 1590788951
and disentanglement
\change_unchanged
 issues 
\change_deleted -1806609307 1590788963
in 
\begin_inset Formula $\encoding$
\end_inset

 along with extrapolation issues in
\change_unchanged
 
\change_inserted -1806609307 1590788968
in 
\change_unchanged

\begin_inset Formula $\decoding$
\end_inset


\change_inserted -1806609307 1590788978
 and 
\begin_inset Formula $\encoding$
\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted -1806609307 1590789020
I felt the most important one is the extrapolation issues
\change_unchanged

\end_layout

\end_inset


\change_unchanged
 
\change_inserted -1806609307 1590789087
actually 
\change_unchanged
help 
\change_inserted -1806609307 1590789001
the 
\change_unchanged
residual-based statistic detect faults better.
 
\change_inserted -1806609307 1590793797
We will see this in two cases: 1) if the change happens in the residual
 space, 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

, it is clear that the residual-based statistics 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 will be effective at capturing this.

\change_unchanged
 
\change_inserted -1806609307 1590793785
2) 
\change_unchanged
More interestingly, this holds true even when the disturbance is purely
 on the latent structure
\change_inserted -1806609307 1590790758
 
\begin_inset Formula $p_{\delta}(\mbz)\neq p(z)$
\end_inset


\change_unchanged
.
 
\change_inserted -1806609307 1590793956
This is true no matter if the encoder can correctly map 
\begin_inset Formula $\encoding$
\end_inset

 the out-of-control sample 
\begin_inset Formula $\boldsymbol{x}^{oc}$
\end_inset

 into the in-control region 
\begin_inset Formula $\pz$
\end_inset

.

\change_unchanged
 
\change_deleted -1806609307 1590793878
First, 
\change_inserted -1806609307 1590793880
a) 
\change_unchanged
if the encoder 
\begin_inset Formula $\encoding$
\end_inset

 map the out-of-control sample 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 into the latent space 
\begin_inset Formula $\boldsymbol{z}$
\end_inset

 in the in-control domain
\change_inserted -1806609307 1590793961
 (i.e., 
\begin_inset Formula $\pz$
\end_inset

)
\change_unchanged
, this will lead to the increase of the monitoring statistics based on the
 residual space for better detection power.
 
\change_deleted -1806609307 1590795018
Second,
\change_inserted -1806609307 1590795019
b)
\change_unchanged
 even if the mapping was correct and it maps 
\begin_inset Formula $\boldsymbol{z}$
\end_inset

 outside the in-control region
\change_inserted -1806609307 1590795038
 (i.e., 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

)
\change_unchanged
, the decoder 
\begin_inset Formula $\decoding$
\end_inset

 have not been trained for any 
\begin_inset Formula $\boldsymbol{z}$
\end_inset

 outside 
\change_deleted -1806609307 1590795049
the in-control regions
\change_inserted -1806609307 1590795037
 
\begin_inset Formula $p(z)$
\end_inset


\change_unchanged
, which still leads to a large statistics to capture disturbances in latent
 variations.
 
\change_deleted -1806609307 1590795096
, note that the encoder 
\begin_inset Formula $\encoding$
\end_inset

 is optimized to produce samples more from where 
\begin_inset Formula $\pz$
\end_inset

 is large.
\change_inserted -1806609307 1590809690
In conclusion, we will recommend using the residual-based monitoring statistics
 for both types of changes.
\change_unchanged

\end_layout

\begin_layout Standard
As of now, we have 
\change_deleted -1806609307 1590795127
enough
\change_inserted -1806609307 1590795131
justified the
\change_unchanged
 reasons to recommend the use of residual-based statistics as the only type
 of test statistics for the profile monitoring application that uses 
\change_inserted -1806609307 1590795151
deep autoencoders such as 
\change_unchanged
VAE.
 Indeed, variations of 
\change_deleted -1806609307 1590795729
this
\change_inserted -1806609307 1590795732
the residual-based
\change_unchanged
 
\change_deleted -1806609307 1590795734
type
\change_inserted -1806609307 1590795736
statistics
\change_unchanged
 have been 
\change_inserted -1806609307 1590795750
previously 
\change_unchanged
proposed 
\change_inserted -1806609307 1590795757
in the literature
\change_deleted -1806609307 1590795743
previously
\change_unchanged
: 
\begin_inset Formula $SPE$
\end_inset

 of 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2019systematic"
literal "false"

\end_inset

.
 However, they both use random samples from the proposal distribution to
 estimate the expectation.
 This approach may require a large number of samples to be generated
\change_inserted -1806609307 1590809701
,
\change_unchanged
 and thus a large number of the forward passes on the decoder network, which
 is prohibitively expensive in terms of computation.
 Instead, we propose a Taylor expansion based approximation as the proposed
 statistic.
 First, observe that 
\begin_inset Formula $\log\decoding\propto\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}+C$
\end_inset

 for all 
\begin_inset Formula $\mbx$
\end_inset

 and 
\begin_inset Formula $\mbz$
\end_inset

 because of the common isotropic covariance assumption.
 The constant 
\begin_inset Formula $C$
\end_inset

 can be discarded as noneffective in terms of control charting because it
 would only translate the limits and the statistics by the same amount for
 any given 
\begin_inset Formula $\mbx$
\end_inset

 and 
\begin_inset Formula $\mbz$
\end_inset

.
 
\change_inserted -1806609307 1590796103
We can define 
\begin_inset Formula $\E_{\mbz\sim q_{\phi}}\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset

 as the expected reconstruction error (ERE) 
\change_deleted -1806609307 1590795816
Let us call the function 
\begin_inset Formula $\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset

 expected reconstruction error (ERE) as it is akin to reconstruction loss
 commonly used in deterministic autoencoders.
 
\change_inserted -1806609307 1590796091

\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted -1806609307 1590795930
I think our definitions are changed for ERE, now ERE is the expected value
 and what you mentioned is ERE1.
 
\change_unchanged

\end_layout

\end_inset


\change_unchanged
We argue that, the Taylor expansion for the first
\change_inserted -1806609307 1590796091
-order and second-order
\change_unchanged
 moment of 
\change_inserted -1806609307 1590796091
the 
\change_unchanged
ERE
\change_inserted -1806609307 1590796091
 
\begin_inset Formula $\E_{\mbz\sim q_{\phi}}\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset


\change_unchanged
 given the random variable 
\begin_inset Formula $\mbz\sim\encoding$
\end_inset

 can be 
\change_inserted -1806609307 1590796117
derived analytically as follows.
 
\change_deleted -1806609307 1590796039
used to approximate 
\begin_inset Formula $\E_{\mbz\sim q_{\phi}}\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset


\change_unchanged
.

\change_inserted -1806609307 1590796091
 
\change_unchanged

\end_layout

\begin_layout Proposition
The first and second-order Taylor Expansion (denoted by 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 respectively)
\change_inserted -1806609307 1590034488
 
\change_unchanged
for the 
\change_deleted -1806609307 1590796133
first moment of the
\change_unchanged
 function 
\change_deleted -1806609307 1590796143

\begin_inset Formula $\gg\mbx-\mbmu_{\mbtheta}(\mbz)\gg_{2}^{2}$
\end_inset


\change_inserted -1806609307 1590796143

\begin_inset Formula $\E_{\mbz\sim q_{\phi}}\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset


\change_unchanged
 given the random variable 
\begin_inset Formula $\mbz\sim\encoding$
\end_inset

 where 
\begin_inset Formula $\decoding=\Norm(\mu_{\mbphi}(\mbx),\diag(\mbsigma_{\mbphi}(x)))$
\end_inset

 can be derived
\change_inserted -1806609307 1590796168
 analytically
\change_unchanged
 as:
\begin_inset Formula 
\begin{equation}
ERE_{1}=\gg\mbx-\mbmu_{\mbtheta}(\mbmu_{\mbphi}(\mbx))\gg_{2}^{2}\label{eq:ere-1}
\end{equation}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
ERE_{2}=\gg\mbx-\mbmu_{\mbtheta}(\mbmu_{\mbphi}(\mbx))\gg_{2}^{2}+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\diag(\mbsigma_{\mbphi}(x)))\label{eq:ere-2}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{H}_{z}$
\end_inset

 is the Hessian of the function 
\begin_inset Formula $\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset

 with respect to 
\begin_inset Formula $\mbz$
\end_inset

.

\change_deleted -767166615 1590116253
 
\change_unchanged

\end_layout

\end_deeper
\begin_layout Standard
The derivation is given in 
\begin_inset CommandInset ref
LatexCommand ref
reference "app:ere"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Given a trained VAE, 
\begin_inset Formula $ERE_{1}$
\end_inset

 can be computed efficiently by forward passing the new profile from the
 process 
\begin_inset Formula $\mbx$
\end_inset

 through 
\begin_inset Formula $\mbmu_{\mbphi}$
\end_inset

 and 
\begin_inset Formula $\mbmu_{\mbtheta}$
\end_inset

 successively and calculating the squared prediction error, without any
 sampling.
 
\begin_inset Formula $ERE_{2}$
\end_inset

 requires the additional computation of the diagonal of the Hessian 
\begin_inset Formula $\mathbf{H}_{z}$
\end_inset

 and a relatively less expensive trace operation since the covariance is
 diagonal.
 We will evaluate the use of both
\change_inserted -1806609307 1590796237
 
\change_unchanged

\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 for process monitoring.
\end_layout

\begin_layout Subsection
Profile Monitoring Procedure
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:methodology:procedure"

\end_inset

 A typical profile monitoring follows two phases: Phase-I analysis and Phase-II
 analysis.
 Phase-I analysis focuses on understanding the process variability by training
 an appropriate in-control mode
\change_deleted -1806609307 1590809725
,
\change_unchanged
 and selecting an appropriate control limit.
 In our case, Phase-I analysis results in a trained model (i.e.
\change_inserted -767166615 1590442792
,
\change_unchanged
 an encoder and a decoder) and an Upper Control Limit (UCL) to help set
 up the control chart for each of the monitoring statistics.
 In Phase-II, the system is exposed to new profiles generated by the process
 in real-time to decide whether these profiles are in-control or out-of-control.
 Our experimentation plan, outlined below, is formulated to emulate this
 scenario to effectively assess the performance of any combination of a
 model, a test statistic and a disturbance scenario to generate the out-of-contr
ol samples.
\end_layout

\begin_layout Itemize
Obtain 
\change_deleted -767166615 1590533354
IC
\change_inserted -767166615 1590533354
in-control
\change_unchanged
 dataset 
\begin_inset Formula $\dataset$
\end_inset

 and partition it into train, validation and test sets 
\begin_inset Formula $\dataset^{trn}$
\end_inset

,
\begin_inset Formula $\dataset^{val}$
\end_inset

,
\begin_inset Formula $\dataset^{tst}$
\end_inset

.
\end_layout

\begin_layout Itemize
Train VAE using samples from 
\begin_inset Formula $\dataset^{trn}$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculate test statistic for all 
\begin_inset Formula $\mbx\in\dataset^{val}$
\end_inset

 and take it's 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

95
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 percentile as the UCL.
\end_layout

\begin_layout Itemize
Start admitting profiles
\change_inserted -1806609307 1590796484
 online
\change_unchanged
 from the process.
 Calculate test statistic using the trained VAE.
 If test statistic is over UCL, identify the sample as 
\change_deleted -767166615 1590533423
OC
\change_inserted -767166615 1590533423
out-of-control
\change_unchanged
.
\end_layout

\begin_layout Standard
We train 10 different model instances with different seeds to account for
 inherent randomness due to the weight initialization of deep neural networks.
\end_layout

\begin_layout Subsection
Neural Network Architectures and Training 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-Architectures"

\end_inset


\end_layout

\begin_layout Standard
In this work, we use convolutional 
\change_inserted -1806609307 1590796592
neural networks for the 
\change_unchanged
encoders and decoders in our VAE 
\change_deleted -1806609307 1590796600
architecture
\change_inserted -1806609307 1590796603
model
\change_unchanged
 to represent the spatial neighborhood structures of the profiles.
 Introduced in 
\begin_inset CommandInset citation
LatexCommand citet
key "lecun1989backpropagation"
literal "false"

\end_inset

, convolutional layers have enabled tremendous performance increase in certain
 neural network applications where the data is of a certain spatial neighborhood
 structure such as images or audio waveform.
 They exploit an important observation of such data, where the learner should
 be equivariant to translations.
 This is an important injection of inductive bias into the network that
 largely reduces the number of parameters compared to the fully connected
 network by the use of parameter sharing.
 It eventually increases the statistical learning efficiency, especially
 for small samples.
 It must be noted, however, convolutional layers are not equivariant to
 scale and rotation as they are to translation.
 Knowing what sort of inductive biases is injected into these layers is
 important for the understanding of disentanglement, which we will introduce
 later in this paper.
\end_layout

\begin_layout Standard
We use the encoder-decoder structure outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The layers used that builds the model architectures used in this study
 are summarized as follows:
\end_layout

\begin_layout Itemize
C(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional layer with arguments referring to the number of output
 channels 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

 and size of zero-padding 
\begin_inset Formula $P$
\end_inset

.

\change_deleted -767166615 1589833794
 
\change_unchanged

\end_layout

\begin_layout Itemize
CT(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional transpose layer with arguments referring to the number
 of output channels 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

, and size of zero-padding 
\begin_inset Formula $P$
\end_inset

.

\change_deleted -767166615 1589833794
 
\change_unchanged

\end_layout

\begin_layout Itemize
FC(
\begin_inset Formula $I,O$
\end_inset

): Fully connected layer with arguments referring to input dimension 
\begin_inset Formula $I$
\end_inset

 and output dimension 
\begin_inset Formula $O$
\end_inset

.

\change_deleted -767166615 1589833794
 
\change_unchanged

\end_layout

\begin_layout Itemize
A: Activation function.
 Leaky ReLU with a negative slope of 
\begin_inset Formula $0.2$
\end_inset

.
\end_layout

\begin_layout Standard
Here, C(), CT(), and FC() are considered the linear transformation layers
 while R(), LR(), and S() are considered the nonlinear activation layers.
 Strided convolutions can be used to decrease the spatial dimensions in
 the encoders.
 Pooling layers are typically not recommended in autoencoder-like architectures
 
\begin_inset CommandInset citation
LatexCommand citep
key "radford2015unsupervised"
literal "false"

\end_inset

.
 Convolutional transpose layers are used to upscale latent codes back to
 ambient dimensions.
\end_layout

\begin_layout Standard
The sequential order of the computational graphs used for this study is
 summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The encoder will output 
\begin_inset Formula $2r$
\end_inset

 nodes, which is a concatenation of the inferred posterior mean 
\begin_inset Formula $\mbmu_{\mbphi}(\mbx)$
\end_inset

and variance 
\begin_inset Formula $\diag(\mbsigma(\mbx))$
\end_inset

, both are of length 
\begin_inset Formula $r$
\end_inset

.
 The number of epochs per training is fixed at 
\begin_inset Formula $1000$
\end_inset

 and the learning rate and batch size are fixed at 
\begin_inset Formula $0.001$
\end_inset

 and 
\begin_inset Formula $64$
\end_inset

 respectively.
 Adam algorithm is used for first-order gradient optimization with parameters
 
\begin_inset Formula $(\beta_{1,}\beta_{2})=(0.9,0.999)$
\end_inset

.
 The model checkpoint is saved at every epoch where a better validation
 loss is observed.
 The latest checkpoint is used as the final model.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\newcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture details of deep neural networks used in this study
\begin_inset CommandInset label
LatexCommand label
name "tab:model-architectures"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top" width="50text%">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Module 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Architecture
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Encoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
C(32, 4, 2, 1) - A - C(32, 4, 2, 1) - A - C(64, 4, 2, 1) - A - C(64, 4,
 2, 1) - A - C(64, 4, 1, 0) - FC(256, 
\begin_inset Formula $2r$
\end_inset

)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Decoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FC(
\begin_inset Formula $r$
\end_inset

, 256) - A - CT(64, 4, 0, 0) - A - CT(64, 4, 2, 1) - A - C(32, 4, 2, 1)
 - CT(32, 4, 2, 1) - A - CT(1, 4, 2, 1)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\change_inserted -1806609307 1589528604

\end_layout

\begin_layout Section
Simulation Study Analysis and Results 
\begin_inset CommandInset label
LatexCommand label
name "sec:Simulation-Study-Analysis"

\end_inset


\end_layout

\begin_layout Standard
In this section, we propose to evaluate the proposed methodology via the
 simulation study to test our claims we make in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in a controlled environment over the data generating process.
 For every experiment mentioned in this section, we follow the procedure
 outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and we use VAE models with the architecture described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-Architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Simulation Setup
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:simsetting"

\end_inset

 We first evaluate the performance of the deep latent variable models in
 a simulation setting inspired by the work of 
\begin_inset CommandInset citation
LatexCommand citet
key "Shi2016-tg"
literal "false"

\end_inset

.
 The simulation procedure produces 2D 
\change_inserted -1806609307 1590796730
structured 
\change_unchanged
point clouds that resemble the scanned topology of a gasket bead.
 Let each pixel on a 
\begin_inset Formula $64$
\end_inset

 by 
\begin_inset Formula $64$
\end_inset

 grid be denoted by a tuple 
\begin_inset Formula $\mbp=(p_{0},p_{1})$
\end_inset

.
 The values of the tuples stretch from 
\begin_inset Formula $0$
\end_inset

 to 
\begin_inset Formula $1$
\end_inset

, equally spaced, left to right and bottom-up.
 Each tuple takes a value based on its location through a function 
\begin_inset Formula $\mbp\mapsto f(\mbp;\czero,r)+\epsilon$
\end_inset

, where 
\begin_inset Formula $\epsilon\sim\Norm(0,1\times10^{-2})$
\end_inset

 is i.i.d Gaussian noise.
 The function 
\begin_inset Formula $f$
\end_inset

 is parametrized by the horizontal center location of the bead 
\begin_inset Formula $\czero$
\end_inset

, and the radius of the bead 
\begin_inset Formula $r$
\end_inset

.
 The vertical center of the bead is fixed to be at the center.
 Given any parameter set 
\begin_inset Formula $\{c_{0},r\}$
\end_inset

, each pixel 
\begin_inset Formula $\mbp$
\end_inset

 can be evaluated with the following logic: 
\begin_inset Formula 
\begin{equation}
\begin{split}g(\mbp;c_{0},r) & =1-\frac{(p_{0}-\czero)}{r}^{2}-\frac{(p_{1}-0.5)}{r}^{2}\\
f(\mbp;c_{0},r) & =\begin{cases}
\sqrt{g(\mbp;c_{0},r)} & \mbox{if }g(\mbp;c_{0},r)\geq0\\
0 & \mbox{if }g(\mbp;c_{0},r)<0
\end{cases}
\end{split}
\label{eq:gasketfun}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The samples are best visualized as grayscale images as shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gasketgrid"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted -767166615 1590510644
 below
\change_unchanged
.
 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset CommandInset label
LatexCommand label
name "fig:gasketgrid"

\end_inset

 
\begin_inset Graphics
	filename figs/gasket.pdf
	lyxscale 50
	width 90line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Gasket profiles depicted as grayscale images simulated with radius and center
 location they coincide with on the axes.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We define the sources of variation in 
\change_deleted -767166615 1590533356
IC
\change_inserted -767166615 1590533356
in-control
\change_unchanged
 gasket beads by two latent variables sampling from independent Gaussian
 distributions: 
\begin_inset Formula 
\begin{equation}
\begin{split}\czero\sim\Norm(0.5,1\times10^{-2})\\
r\sim\Norm(0.2,6.25\times10^{-4})
\end{split}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Finally, we will consider the following four types of 
\change_deleted -767166615 1590533418
OC
\change_inserted -767166615 1590533418
out-of-control
\change_unchanged
 variation patterns for the system:
\end_layout

\begin_layout Itemize

\series bold
Location shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $\czero$
\end_inset

 is altered by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $\czero\sim\Norm(0.5+\delta\times10^{-2},1\times10^{-2})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Width shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $a$
\end_inset

 is perturbed by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $r\sim\Norm(0.2+\delta\times10^{-4},6.25\times10^{-4})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Mean shift
\series default
: all of the pixels are added an additive disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $f(\mbp;c_{0},r)\leftarrow f(\mbp;c_{0},r)+\delta$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Magnitude shift:
\series default
 all of the pixels are added a multiplicative disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $f(\mbp;c_{0},r)\leftarrow f(\mbp;c_{0},r)*\delta$
\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\delta$
\end_inset

 is the intensity of the change.
 Note that the location shift and width shift represent disturbances in
 latent distribution 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

.
 An important distinction between the two is that location equivariance
 is injected into convolutional networks but not scale equivariance, therefore
 we expect different reactions to these changes by deep convolutional latent
 variable models in terms of disentanglement.
 The other two cases, mean shift and magnitude shift, represent disturbances
 in the conditional distribution 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)$
\end_inset

.
 The training, validation, and testing 
\change_deleted -767166615 1590533358
IC
\change_inserted -767166615 1590533358
in-control
\change_unchanged
 or 
\change_deleted -767166615 1590533420
OC
\change_inserted -767166615 1590533420
out-of-control
\change_unchanged
 samples are generated of size 500 each.
\end_layout

\begin_layout Subsection
On the 
\change_inserted -767166615 1590521774
D
\change_deleted -767166615 1590521774
d
\change_unchanged
isentanglement and 
\change_inserted -767166615 1590521776
E
\change_deleted -767166615 1590521776
e
\change_unchanged
xtrapolation 
\change_inserted -767166615 1590521779
P
\change_deleted -767166615 1590521778
p
\change_unchanged
erformance of the 
\change_inserted -767166615 1590521781
E
\change_deleted -767166615 1590521780
e
\change_unchanged
ncoder 
\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:recognition"

\end_inset


\end_layout

\begin_layout Standard
To investigate the disentanglement and extrapolation performance of the
 recognition network, we employ the following procedure.
 First, we train a VAE with a 2-dimensional latent code (for easier visualizatio
n) to convergence using in-control samples as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Then, we a set of profiles with varying center location 
\begin_inset Formula $c_{0}$
\end_inset

 and radius 
\begin_inset Formula $r$
\end_inset

 into the recognition network
\change_inserted -1806609307 1590803120
 (i.e., encoder network)
\change_unchanged
 to obtain their respective proposal distributions.
 The points are picked inside and outside the tolerance region of the two
 quality characteristics to be compared against their mapping onto the represent
ation space.
 Finally, we sample 150 points from the proposals and plot them on the represent
ation space.
 The results are shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted -767166615 1590510668
 below
\change_unchanged
.
 There are two important observations we would like to point out:
\end_layout

\begin_layout Itemize

\series bold
Representations are entangled
\series default
.
 For any of the fixed sources of variation, the variation in the other source
 is not axis-parallel and therefore
\change_inserted -1806609307 1590809778
,
\change_unchanged
 the variations are entangled in the learned space.

\change_deleted -767166615 1590429518
 
\change_unchanged

\end_layout

\begin_layout Itemize

\series bold
The encoder cannot extrapolate for extreme values
\series default
.

\series bold
 
\change_deleted -767166615 1590429518
 
\series default
 
\change_unchanged
Observe how for the extremely high values of center location 
\begin_inset Formula $c$
\end_inset

 or extremely low values of radius 
\begin_inset Formula $r$
\end_inset

 are the overall variation pattern starts getting distorted.
\end_layout

\begin_layout Standard
Given these two observations and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fixed-c-varying-r"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can conclude right away that gaskets with extremely small radius 
\begin_inset Formula $r$
\end_inset

 will likely go undetected if only the latent variable-based statistic is
 used.
 This is an undesired consequence of the two aforementioned issues.
 The model seems to be more accurately responsive to the extreme values
 of center location 
\begin_inset Formula $c$
\end_inset

.
 However, this is most likely due to translational equivariance injected
 into the model through convolutional layers 
\begin_inset CommandInset citation
LatexCommand citep
key "locatello2018challenging"
literal "false"

\end_inset

.
 In many real-life applications, the nature of variations in profiles will
 be more complex than simple translations in 
\change_inserted -1806609307 1590809795
the 
\change_unchanged
spatial or temporal domain.
 For such variations, it would be infeasible to inject required predicates
 due to the rigid architectures of deep neural networks.
 We already have
\change_inserted -1806609307 1590809805
 the
\change_unchanged
 radius as the evidence because convolutional layers are not scale
\change_deleted -1806609307 1590809813
 
\change_inserted -1806609307 1590809813
-
\change_unchanged
equivariant.

\change_deleted -767166615 1590429351
 
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figs/fixed_radii.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Fixed 
\begin_inset Formula $r$
\end_inset

, varying 
\begin_inset Formula $c$
\end_inset


\change_inserted -767166615 1590118516

\begin_inset CommandInset label
LatexCommand label
name "fig:Fixed-r-varying-c"

\end_inset


\change_unchanged

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figs/fixed_center_locs.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Fixed 
\begin_inset Formula $c$
\end_inset

, varying 
\begin_inset Formula $r$
\end_inset


\change_inserted -767166615 1590118526

\begin_inset CommandInset label
LatexCommand label
name "fig:Fixed-c-varying-r"

\end_inset


\change_unchanged

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Figure depicting the entangled representations and extrapolation issues
 of the encoder of a VAE with two-dimensional latent code trained on in-control
 samples with.

\series bold
 
\series default
For each subfigure, plots on the left show where real factors of variation
 are sampled from and figure on the right is what the VAE encoder infers
 as the mean of the proposal distribution 
\begin_inset Formula $\encoding$
\end_inset

.
 
\series bold
Top: 
\series default
Real factors of variation are generated at three fixed levels of radius
 
\begin_inset Formula $r$
\end_inset

 and varying values of center location 
\begin_inset Formula $c$
\end_inset

 on the left figure.
 Corresponding inferred mean plotted on the right graph.
 
\series bold
Bottom:
\change_inserted -1806609307 1590809824

\series default
 
\change_unchanged
Similar to (b) but with fixed center location 
\begin_inset Formula $c$
\end_inset

 at three levels and varying 
\begin_inset Formula $r$
\end_inset

.
 The tolerable region is represented by the gray dashed circle, a curve
 of isodistant points in terms of Mahalanobis distance to 
\change_deleted -767166615 1590533389
in control
\change_inserted -767166615 1590533389
in-control
\change_unchanged
 distribution.
 
\change_deleted -1806609307 1589953705
 
\change_unchanged
Isodistant curve for standard Gaussian that is probability-wise equivalent
 to the one on the left is depicted as a gray dashed circle.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:proposals"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Overall, it is in agreement with our rationale behind the proposed statistic
 outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In other words, without extrapolation and disentanglement, we do not expect
 any
\change_inserted -1806609307 1590803218
 latent-variable-based
\change_unchanged
 monitoring statistic based purely on the output of the recognition network—such
 as 
\begin_inset Formula $H^{2}$
\end_inset

, 
\begin_inset Formula $T^{2}$
\end_inset

 or 
\begin_inset Formula $D$
\end_inset

 discussed in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

— to be robust enough to be safely employed in a process control mission.
\end_layout

\begin_layout Standard
We also want to make a remark that we tried the beta-VAE framework explained
 in 
\begin_inset CommandInset citation
LatexCommand citep
key "higgins2017beta"
literal "false"

\end_inset

 with 
\series bold

\begin_inset Formula $\beta=4$
\end_inset


\series default
 to remedy the problem of entanglement but obtained similar results.
 Finding disentangled representations for deep generative models is still
 an open research area and to the best of our knowledge, solutions with
 theoretical guarantees does not exist.
\end_layout

\begin_layout Subsection
On the 
\change_inserted -767166615 1590521787
E
\change_deleted -767166615 1590521786
e
\change_unchanged
xtrapolation 
\change_inserted -767166615 1590521788
P
\change_deleted -767166615 1590521788
p
\change_unchanged
erformance of the 
\change_inserted -767166615 1590521791
D
\change_deleted -767166615 1590521791
d
\change_unchanged
ecoder 
\change_deleted -767166615 1590521799
network
\change_unchanged

\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:generator"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:manifold_vae"
plural "false"
caps "false"
noprefix "false"

\end_inset

 hints us about the extrapolation performance of the decoder of the same
 VAE described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

 trained on in-control samples described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 It should be cross-examined with 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\change_inserted -767166615 1590510941
above 
\change_unchanged
as the encoder and decoder are tightly coupled to each other.
 We observe two important behavior: the posterior gets distorted beyond
 two or three standard deviations, and the representations are partially
 entangled in line with the behavior of its encoder depicted in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To see how this will help to detect disturbances in latent space, consider
 a gasket that is extremely small in term
\change_inserted -1806609307 1590809840
s
\change_unchanged
 of the radius (i.e.
\change_inserted -767166615 1590442798
,
\change_unchanged
 small 
\begin_inset Formula $r$
\end_inset

) or at the very margins of the grid in terms of center location (i.e.
\change_inserted -767166615 1590442805
,
\change_unchanged
 center location 
\begin_inset Formula $c$
\end_inset

 far from 0.5).
 Looking at 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:manifold_vae"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can observe that the decoder simply cannot generate such a sample because
 it does not extrapolate well in either of latent dimensions.
 This will in turn
\change_inserted -1806609307 1590809852
,
\change_unchanged
 produce a larger reconstruction error, and thus a larger monitoring statistic
 that will likely to fall outside the control limit.
 Recall once again that the disturbance described is purely on the latent
 distribution 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

 and yet our proposed monitoring statistic will capture this behavior well
 thanks to the extrapolation issues in the decoder.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/manifold_vae.pdf
	width 90line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Latent space traversal and the response of the decoder of a VAE with 2-dimension
al latent codes and trained with in-control gasket samples.
 Each row represents which latent dimension is traversed while the other
 dimension is fixed at zero.
 Each column represents what value is assigned to that latent dimension
 that is represented by the row label.
 Each image in each cell is generated by the decoder using that specific
 latent variable combination.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:manifold_vae"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
On the 
\change_inserted -767166615 1590521803
E
\change_deleted -767166615 1590521803
e
\change_unchanged
stimation of log-likelihood 
\change_inserted -767166615 1590521807
U
\change_deleted -767166615 1590521807
u
\change_unchanged
nder 
\change_inserted -767166615 1590521809
I
\change_deleted -767166615 1590521809
i
\change_unchanged
mportance 
\change_inserted -767166615 1590521811
S
\change_deleted -767166615 1590521811
s
\change_unchanged
ampling
\end_layout

\begin_layout Standard
Earlier, we claimed that it would take too many Monte Carlo sampling iterations
 to get a meaningful estimate of ERE 
\change_deleted -767166615 1590542722
defiend
\change_inserted -767166615 1590542722
defined
\change_unchanged
 as 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

.
 In this section, we test that claim on a random in-control sample 
\begin_inset Formula $\mbx$
\end_inset

 using the proposal distribution 
\begin_inset Formula $\mbz\sim\encoding$
\end_inset

 which is obtained via the encoder of the same VAE model we have been using
 in this section.
 The results of the sampling-based estimation of ERE, first-order approximation
 
\begin_inset Formula $ERE_{1}$
\end_inset

, and second-order approximation 
\begin_inset Formula $ERE_{2}$
\end_inset

 are shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Estimation-comparison-between"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted -767166615 1590510962
 below
\change_unchanged
.
 The key observation is that it takes at least a few tens of Monte Carlo
 iterations to get a stable and accurate estimation.
 At that level, the single pass through the encoder is negligible.
 This means using sampling will be more costly at least 
\change_deleted -767166615 1590105102
 
\change_unchanged
50 samples to achieve the same accuracy as the first-order approximation
 that we suggest and at least 80 samples to get the accuracy of the second-order
 approximation.
 Another important observation is that second-order approximation is a bit
 more accurate than first-order approximation since it is closer to the
 sample average approximation, but their difference is insignificant, and
 it requires much more computation.
 In the next subsection, we will evaluate the performance of 
\begin_inset Formula $ERE_{2}$
\end_inset

 and 
\begin_inset Formula $ERE_{1}$
\end_inset

 in Phase-II monitoring to evaluate whether the added computational complexity
 for 
\begin_inset Formula $ERE_{2}$
\end_inset

 is justifiable.

\change_deleted -767166615 1590105102
 
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/mc_vs_foa.pdf
	lyxscale 90
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimation comparison between Monte Carlo sampling, first-order approximation
 
\change_deleted -767166615 1590116243
 
\change_unchanged
and second-order approximation.
 95% confidence interval band is shown in the gray band and is based on
 simulations with ten different seeds.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Estimation-comparison-between"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison of 
\change_inserted -767166615 1590521815
D
\change_deleted -767166615 1590521814
d
\change_unchanged
etection 
\change_inserted -767166615 1590521816
P
\change_deleted -767166615 1590521816
p
\change_unchanged
erformance of 
\change_inserted -767166615 1590521818
P
\change_deleted -767166615 1590521818
p
\change_unchanged
roposed 
\change_inserted -767166615 1590521820
S
\change_deleted -767166615 1590521820
s
\change_unchanged
tatistics
\end_layout

\begin_layout Standard
We now compare the proposed statistics based on how accurately they detect
 profiles from out-of-control processes outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that for all statistics that require sampling, we obtain a single
 sample and calculate the statistic based on that to keep the computational
 demand the same for all statistics and emulate the computational constraints
 of a real-life case.
 A preliminary result we must check is the robustness of the statistics
 by making sure all proposed statistics have false alarm rates on the held-out
 in-control test set, which should also be less than the desired rate 5%.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:far"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates that this is the case for all of them.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
False alarm rates on the held-out dataset averaged over 10 replications
 per model and monitoring statistic.
 Standard deviations are in parentheses.
\begin_inset CommandInset label
LatexCommand label
name "tab:far"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="6">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Statistic
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERE
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SPE/R
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
D
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
H2
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
T2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.041(0.006)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.051(0.005)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.044(0.004)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.052(0.005)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.043(0.009)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Through 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we observe a clear superiority of 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 over other methods when the disturbance is on the observable space (top
 row).
 Latent variable-based statistics 
\begin_inset Formula $D$
\end_inset

, 
\begin_inset Formula $H^{2}$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

 fail in this case since that they are purely computed using the proposal
 distribution latent variables.
 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 also outperform
\change_deleted -1806609307 1590803388
s
\change_unchanged
 
\begin_inset Formula $SPE/R$
\end_inset

, although by a smaller margin it has with the latent variable-statistics.
 Between 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset


\change_inserted -1806609307 1590806763
,
\change_unchanged
 it's hard to claim 
\change_deleted -1806609307 1590808658
whether
\change_inserted -1806609307 1590808658
which
\change_unchanged
 one works 
\change_deleted -1806609307 1590808664
than
\change_inserted -1806609307 1590808671
better 
\change_deleted -1806609307 1590808668
 the other as
\change_inserted -1806609307 1590808669
since
\change_unchanged
 their mean performances are quite close to each other.
\end_layout

\begin_layout Standard
For the latter two disturbances occurring purely on latent dimensions, results
 are presented in the bottom row of 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The key observations can be listed as follows:
\end_layout

\begin_layout Itemize
We observe mixed results but generally 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

, 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $H^{2}$
\end_inset

 tend to perform better than 
\begin_inset Formula $SPE/R$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

.
 A commonality between the former three is that they don't rely on random
 samples, supporting our argument against this practice.
\end_layout

\begin_layout Itemize
Observe the radius shift
\change_deleted -1806609307 1590809878
 
\change_inserted -1806609307 1590809878
-
\change_unchanged
type disturbance show in the bottom left figure.
 Even though 
\begin_inset Formula $H^{2}$
\end_inset

 performs better on positive intensities (larger radii), it completely misses
 negative intensities (smaller radii).
 We foresaw this result in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To reiterate, disentanglement and the lack of extrapolation in the encoder
 is the reason behind this.
 We would also suggest that this result can extend to all the latent-variable
 based statistics.
\end_layout

\begin_layout Itemize
Unlike latent variable-based statistics, 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 and 
\begin_inset Formula $SPE/R$
\end_inset

 behave more robustly against varying intensities.
 In other words, the detection rate increase with increased intensities
 consistently.
 Among these, we observe that 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 consistently outperform 
\begin_inset Formula $SPE/R$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 perform very similarly.
 In this case, we conclude that the second-order information does not help
 too much for the Phase-II monitoring.
 The reason behind this is that the second-order information 
\change_inserted -1806609307 1590808727
also comes from the 
\change_unchanged
encode
\change_inserted -1806609307 1590808737
r
\change_deleted -1806609307 1590808742
 the variance of the encoded latent variables, which are required by the
 encoders
\change_unchanged
.
 However, given
\change_inserted -1806609307 1590808749
 that the
\change_unchanged
 encoders 
\change_inserted -1806609307 1590808760
is trained on in-control samples and 
\change_deleted -1806609307 1590808766
would
\change_inserted -1806609307 1590808767
may
\change_unchanged
 provide inaccurate information in the 
\change_deleted -767166615 1590533401
OC
\change_inserted -767166615 1590533401
out-of-control
\change_unchanged
 regions, the second-order information for 
\change_deleted -767166615 1590533402
OC
\change_inserted -767166615 1590533402
out-of-control
\change_unchanged
 samples would be bias.
 Therefore, it does not provide additional gain for
\change_deleted -1806609307 1590809889
 the
\change_unchanged
 monitoring performance.

\change_deleted -767166615 1590105309
 
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float figure
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/disturbance_on_pxz_vae_only.pdf
	lyxscale 70
	width 100line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Fault detection rates (y-axis) for varying intensities (x-axis) of different
 disturbance types (quadrants).
 Bands represent 95% confidence interval estimated around mean detection
 rates.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:disturbance_on_pxz"

\end_inset


\end_layout

\end_inset


\change_inserted -1806609307 1589531563

\end_layout

\begin_layout Standard

\change_deleted -1806609307 1590808798
Note that
\change_inserted -1806609307 1590808801
As mentioned,
\change_unchanged
 in a real-life process, disturbances 
\change_deleted -767166615 1590105314
 
\change_unchanged
on the residual space is often more likely than the disturbance in the 
\change_deleted -767166615 1590105314
 
\change_unchanged
latent space
\change_deleted -1806609307 1590808815
 would be very unlikely
\change_unchanged
.
 Therefore, we would recommend the use of residual-based monitoring statistics.
 Among all residual-based monitoring statistics, we conclude that 
\begin_inset Formula $ERE_{1}$
\end_inset

 
\change_deleted -767166615 1590105314
 
\change_unchanged
performance the best considering the accuracy, robustness and computational
 demand.

\change_inserted -1806609307 1590808950
 This will be further validated through the case study analysis.
 
\change_unchanged

\end_layout

\begin_layout Section
Case Study Analysis & Results
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:case-study"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% TODO (@DS): how many anomaly samples do we have?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% TODO (@DS): Add a figure to illustrate both normal and abnormal samples,
 with one image in each class
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our dataset consists of defect image profiles from a hot-steel rolling process,
 which is shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There are 13 classes of surface defect types identified by the domain engineers.
 Four of these classes—0,1,9 and 11—are considered minor defects and they
 constitute our in-control set.
 There are in total 338 images in these classes.
 The other nine classes make up the 
\change_deleted -767166615 1590533435
out of control
\change_inserted -767166615 1590533435
out-of-control
\change_unchanged
 cases and they have in combination 3351 images to report detection accuracy
 for.
 We randomly partition the 
\change_deleted -767166615 1590533360
IC
\change_inserted -767166615 1590533360
in-control
\change_unchanged
 corpus to fix train, validate and test sets with 60%-20%-20% relative sizes,
 respectively.
 The rest of the procedure followed is outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Same as in the simulation study, to account for randomness in weight initializa
tion, we replicate the experiment with 10 different seeds.
 For comparison, we also include the monitoring performance with the traditional
 PCA method with the same residual-based control chart, denoted as PCA-Q.
 The results are summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted -767166615 1590511345
 below
\change_unchanged
.
\end_layout

\begin_layout Standard

\change_inserted -767166615 1590521612
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout

\change_inserted -767166615 1589935896
Dr.
 Yan,
\end_layout

\begin_layout Plain Layout

\change_inserted -767166615 1589945278
I don't see how 1 and 2 will help answer our research question.
 Linear layers may require more number of parameters (I'm assuming by number
 you mean the dimensionality of latent variable becuase it can be mistaken
 to the trainable parameters and there, deep learning clearly has more parameter
s) but what is your rationale behind why this hurts detection performance?
 2) Yes, this is true but we already mentioned the linear-nonlinear manifold
 story.
 I think that's enough.
 Also, I don't even know why we include performance against PCA when all
 other papers already demonstrated that.
 Can't we just refer to their work? It's really not central to our discussion.
 If they want we can add it later in the revision.
 If they wouldn't require that but they see in Rolling, they will ask the
 same thing for Gasket and for absolutely no reason we would have to go
 through the additional responsibility of doing that for Gasket.
 If you are convinced, please go ahead and delete the second table and the
 single paragraph about PCA comparison above.
\change_inserted -1806609307 1590043031

\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590043038
Hi, Dorukhan, 
\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590043267
I talked with Kamran before and he really likes the special issue to show
 the advantage of the deep learning over traditional methods.
 Another reason is that reviewers will ask this question for a high chance...
 
\end_layout

\begin_layout Plain Layout

\change_inserted -1806609307 1590043220
Even though this has been shown by other people.
 I still like to keep this.
 However, I felt that making it as two tables are indeed cubersome since
 there are many redudant information.
 So what I did is simply merge these two tables.
 BUt I agree that we can keep it short and so I just least one sentence
 and give a citation.

\change_deleted -767166615 1590105150
 
\change_inserted -767166615 1590105150

\end_layout

\begin_layout Plain Layout

\change_inserted -767166615 1590105208
Thanks for pointing this out! I like learning about these unwritten rules
 of publishing, even though probably I won't be doing this after I graduate.
\change_inserted -1806609307 1590043220

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted -767166615 1590521614
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted -767166615 1590521646
I had to rotate this sideways beacuse it couldn't fit the required margins
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways true
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Summary of fault detection rates on out-of-control cases averaged over 10
 replications per model and monitoring statistic.
 Standard deviations are in parentheses.
 Bolded values represent the maximum average across different statistics.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:rolling_results"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="13" columns="8">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
VAE
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
PCA
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Statistic
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
D
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
H2
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
T2
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SPE/R
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERE
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERE2
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Q
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fault ID
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.37(0.03)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.44(0.06)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.50
\series default
(0.06)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.17(0.06)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.23(0.04)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.03(0.03)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.84(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.85(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.86
\series default
(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.78(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.62(0.02)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.75
\series default
(0.05)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.71(0.05)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.56(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.58(0.07)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.62(0.09)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.99(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06(0.03)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15(0.08)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.05(0.05)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.79(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.52(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.13(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.17
\series default
(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.64(0.02)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.70
\series default
(0.07)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.69(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.34(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.49(0.03)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.57
\series default
(0.05)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.57
\series default
(0.04)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.29(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.79(0.01)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.02)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.02)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.69(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.00)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.71(0.04)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.77
\series default
(0.02)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.76(0.02)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.56(0.00)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can observe that 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 consistently outperforms all other 
\change_inserted -1806609307 1590809004
monitoring 
\change_unchanged
statistic formulations.
 The divide between residual-based statistics and latent variable-based
 statistics observed in
\change_inserted -1806609307 1590809923
 the
\change_unchanged
 simulation study is 
\change_deleted -1806609307 1590809014
replicated
\change_inserted -1806609307 1590809016
further validated
\change_unchanged
 here too.
 The inferiority of latent variable-based statistics are much more obvious
 here
\change_inserted -1806609307 1590809030
 in the real case study
\change_unchanged
, as we observe for most out-of-control classes the detection rate is simply
 zero.
 This observation further validate
\change_inserted -1806609307 1590809933
s
\change_unchanged
 our claims that in practice, for deep autoencoders, the change happens
 in the residual space rather than the latent space.
 The advantage of VAE over PCA is mainly due to the better representative
 power and data compression ability of deep autoencoders compared to PCA.
 
\change_deleted -1806609307 1590809068
We replicated
\change_inserted -1806609307 1590809074
It is worth noting that
\change_unchanged
 the superiority of VAE over PCA for process monitoring
\change_deleted -1806609307 1590809088
 the same way it
\change_unchanged
 was
\change_inserted -1806609307 1590809091
 also
\change_unchanged
 demonstrated in the earlier works 
\change_deleted -1806609307 1590809096
mentioned in this study
\change_inserted -1806609307 1590809109
in a large number of applications
\change_unchanged
 
\begin_inset CommandInset citation
LatexCommand citep
key "Zhang2019-lu,wang2019systematic,lee2019process"
literal "false"

\end_inset

.

\change_deleted -1806609307 1590809048
 .
\change_inserted -767166615 1590511384

\end_layout

\begin_layout Standard
To support our claim of the ineffectiveness of latent variable-based statistics
\change_deleted -1806609307 1590809127
 
\change_deleted -767166615 1590105921
 
\change_unchanged
, we refer the reader to 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:kde"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted -767166615 1590511432
 below
\change_unchanged
.
 We observe how well separated the statistics are for 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $SPE/R$
\end_inset

 while for latent variable-based statistics the obtained values are mostly
 overlapping.
 Note that we omitted 
\begin_inset Formula $ERE_{2}$
\end_inset


\change_inserted -1806609307 1590809142
 
\change_unchanged
because it was almost identical to 
\begin_inset Formula $ERE_{1}$
\end_inset

.
 To obtain a deeper understanding of the results, we point out 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Output-of-the"
plural "false"
caps "false"
noprefix "false"

\end_inset

 below for the original images and their reconstructions.
 
\change_inserted -1806609307 1590809946
The 
\change_deleted -1806609307 1590809947
D
\change_inserted -1806609307 1590809947
d
\change_unchanged
ecoder is persistent on generating samples that look like in-control rolling
 samples with little fidelity to how the original defect sample looks like.
 When 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Original-profiles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Reconstructions-via-VAE"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are cross-examined, it is apparent why reconstruction error would be high.
 On the contrary, 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inferred-means"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows that most latent representations fall into the region that would
 be considered in-control from a profile monitoring perspective.
 We observe instances of class 3,5,6 and 7 generate the latent variables
 in the 
\change_deleted -767166615 1590533406
OC
\change_inserted -767166615 1590533406
out-of-control
\change_unchanged
 regions.
 However, even for these classes, 
\begin_inset Formula $SPE/R$
\end_inset

, 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 yields much better detection power than 
\begin_inset Formula $D$
\end_inset

, 
\begin_inset Formula $H_{2}$
\end_inset

, and 
\begin_inset Formula $T_{2}$
\end_inset

, as it can be seen in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In conclusion, we would like to suggest the use of 
\begin_inset Formula $ERE_{1}$
\end_inset

 for deep autoencoders, which is consistent with our findings in the simulation
 study.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/SPE.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$ERE_{1}$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:ERE}
\end_layout

\begin_layout Plain Layout

 
\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout

 
\backslash
hspace{0.1
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

 
\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/R.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$SPE/R$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:spe-r}
\end_layout

\begin_layout Plain Layout

 
\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout

 
\end_layout

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/H2.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$H^{2}$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:h2}
\end_layout

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout


\backslash
hfill
\end_layout

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/D.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$D$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:D}
\end_layout

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout


\backslash
hfill
\end_layout

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/T2.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$T^{2}$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:T2}
\end_layout

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Kernel density estimation plots of statistics obtained for in-control and
 out-of-control steel defect profiles, per each proposed statistic type.
\begin_inset CommandInset label
LatexCommand label
name "fig:kde"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "25text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/casestudy_orig.pdf
	width 100text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Original profiles
\begin_inset CommandInset label
LatexCommand label
name "fig:Original-profiles"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "25text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/casestudy_recons.pdf
	width 82col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Reconstructions via VAE
\begin_inset CommandInset label
LatexCommand label
name "fig:Reconstructions-via-VAE"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "50text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/casestudy_scat.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Inferred means
\begin_inset CommandInset label
LatexCommand label
name "fig:Inferred-means"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Output of the VAE decoder and the encoder for randomly select rolling profiles.
 
\series bold
Left: 
\series default
Original profiles visualized.
 Each row is a class of defect profile and each column is a randomly selected
 from that class.
 
\series bold
Middle: 
\series default
Reconstructions of the samples with one-to-one correspondence to the samples
 on the image to the left.
 
\series bold
Right: 
\series default
Inferred mean locations of each of the defects visualized on left.
 Points are annotated by their class IDs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Output-of-the"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion 
\begin_inset CommandInset label
LatexCommand label
name "sec:conclusions"

\end_inset


\end_layout

\begin_layout Standard
In this paper, we focus on evaluating Phase-II monitoring statistics proposed
 so far in the literature for VAE and demonstrate that they were not performing
 optimally in terms of accuracy and/or computational feasibility.
 First, we point out two important issues related to the latent variables
 learned by the encoder of VAE, namely, entanglement and failure to extrapolate.
 Based on these issues, we argue that latent variable-based statistics should
 be discarded altogether with both conceptual explanations and real experiments.
 Second, we point out that the residual-based statistics based on sampling
 will require too many samples to be computationally feasible.
 Finally, we propose a novel formulation by deriving the Taylor expansion
 of expected reconstruction error that addresses both accuracy and computational
 efficiency.
\end_layout

\begin_layout Standard
To support our claim, our simulation study first demonstrates the entanglement
 and lack of extrapolation of latent variable-based statistics and its inferior
 Phase-II monitoring performance.
 It further shows that the derived statistics based on the residual space
 is more robust and more accurate than all the other statistics proposed
 so far.
 Finally, we validate the superiority of our formulation on a real-life
 case study, where steel defect image profiles are used.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "./bibliography"

\end_inset


\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%dummy comment inserted by tex2lyx to ensure that this paragraph is not
 empty
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
refalias
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

section
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Proof of Proposition 3.1 
\begin_inset CommandInset label
LatexCommand label
name "sec:PoofOfPropTQ"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted -1806609307 1590809972
The 
\change_unchanged
Kullback-Leibler divergence between two multivariate Gaussian distributions
 has a closed-form solution.
 If we define these distributions as 
\begin_inset Formula $p_{0}=N(\mbz;\mbmu_{0},\mbSigma_{0})$
\end_inset

 and 
\begin_inset Formula $p_{1}=N(\mbz;\mbmu_{1},\mbSigma_{1})$
\end_inset

 where 
\begin_inset Formula $\mbmu$
\end_inset

 and 
\begin_inset Formula $\mbSigma$
\end_inset

 are respective mean vectors and covariance matrices, then according to
 
\begin_inset CommandInset citation
LatexCommand citep
key "hershey2007approximating"
literal "false"

\end_inset

 the closed-form solution will be the following: 
\begin_inset Formula 
\begin{align}
\KL{p_{0}}{p_{1}} & =\frac{1}{2}[\log\frac{\g\mbSigma_{1}\g}{\g\mbSigma_{0}\g}+Tr(\mbSigma_{1}\inv\mbSigma_{0})-r+(\mbmu_{0}-\mbmu_{1})^{\top}\mbSigma_{1}\inv(\mbmu_{0}-\mbmu_{1})]\label{eq:kld-closed-form}
\end{align}

\end_inset

Since 
\begin_inset Formula $\encoding=\Norm(\mbmu(\mbx),\mbSigma_{z})$
\end_inset

 and 
\begin_inset Formula $p(\mbz)=\Norm(0,\mbI)$
\end_inset

, we can derive that 
\begin_inset Formula 
\begin{align}
\KL{\encoding}{p(\mbz)} & =\frac{1}{2}\left[-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r\right]+\frac{1}{2}\mbmu(\mbx)^{\top}\mbmu(\mbx)\nonumber \\
 & =\frac{1}{2}\mbmu(\mbx)^{\top}\mbmu(\mbx)+C,\label{eq:kld-prior}
\end{align}

\end_inset

where 
\begin_inset Formula $C=-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r$
\end_inset

 is a constant, which doesn't depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Standard
To derive the SPE statistics, we will derive
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2}\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbx^{\top}\mbx-2\mbz^{\top}\mbW\mbx+\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\label{eq: spew}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Here, we know that 
\begin_inset Formula 
\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}tr(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & tr\left(\mbW^{\top}\mbW\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz\mbz^{\top})\right)\nonumber \\
= & tr\left(\mbW^{\top}\mbW(\mbmu(\mbx)\mbmu(\mbx)^{\top}+\Sigma_{z})\right)\nonumber \\
= & \mbmu(\mbx)^{\top}\mbW^{\top}\mbW\mbmu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\label{eq: tracezwwz}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Therefore, by plugging 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: tracezwwz"
plural "false"
caps "false"
noprefix "false"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: spew"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have 
\begin_inset Formula 
\begin{align}
\mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2} & =\mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
 & =\mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mbmu(\mbx)^{\top}\mbW^{\top}\mbW\mbmu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\nonumber \\
 & =\|\mbx-\mbW\mbmu(\mbx)\|^{2}+C\label{eq:q-to-ere}
\end{align}

\end_inset

where 
\begin_inset Formula $C=tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)$
\end_inset

 that does not depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Section
A Toy Example to Demonstrate Out-of-distribution Behavior of Neural Networks
 
\begin_inset CommandInset label
LatexCommand label
name "app:rosenbrock"

\end_inset


\end_layout

\begin_layout Standard
Assume using a multilayer perceptron, we are trying to approximate the famous
 Rosenbrock function 
\begin_inset Formula $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$
\end_inset

 given 
\begin_inset Formula $(a,b)=(1,100)$
\end_inset

.
 In this small experiment, we sample tuples of two-dimensional points from
 a bounded region 
\begin_inset Formula $(x_{i},y_{i})\in[-1,3]\times[-2,3]$
\end_inset

.
 We use a multilayer perceptron with six hidden layers and a hundred neurons
 in each layer.
 Half of the points are used in training, and the other half is used as
 a validation set to optimize hyper-parameters.
 Using the trained network, we plot the actual Rosenbrock function along
 with the neural network approximation in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rosenbrock"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Notice how well the function is approximated for the region 
\begin_inset Formula $[-1,3]\times[-2,3]$
\end_inset

, but there is a serious discrepancy between the approximated and the real
 outside of the region.
 This is a small yet to the point example of out-of-distribution issues
 with neural networks.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figs/rosenbrock.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Rosenbrock function (green surface) approximated by an 
\change_deleted -767166615 1590460400
MLP
\change_inserted -767166615 1590460403
multilayer perceptron
\change_deleted -767166615 1590460401
 
\change_unchanged
(red surface) given training (black crosses) and validation (black dots)
 samples form a bounded region 
\begin_inset Formula $(x_{i},y_{i})\in[-1,3]\times[-2,3]$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Rosenbrock"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
ERE Testing Statistic Derivation 
\change_deleted -767166615 1590429630
 
\change_unchanged

\begin_inset CommandInset label
LatexCommand label
name "app:ere"

\end_inset


\change_inserted -767166615 1589846675

\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
here needs a bit more neat explanation of everything, such as what q (it's
 a normal distrbution with mean 
\begin_inset Formula $\mbmu_{\mbphi}(\mbx)$
\end_inset

 and covariance 
\begin_inset Formula $\diag(\mbsigma_{\mbphi}(\mbx))$
\end_inset

) is and what Hz is etc.
 I think what you mean by R''(z) is Hz.
 I'm not sure where exactly trace is introduced, but probably the derivation
 here led to that https://stats.stackexchange.com/questions/34477/expected-value-a
nd-variance-of-trace-function .
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To derive the 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

, we first define 
\begin_inset Formula $R(z)=\|y-\mbmu_{\mbtheta}(z)\|^{2}$
\end_inset

 as the reconstruction error (RE).
 The quantity we would like approximate is 
\begin_inset Formula $E_{\mbz\sim q_{\phi}}R(\mbz)$
\end_inset

 where 
\begin_inset Formula $\encoding=\Norm(\mbmu_{\mbphi}(\mbx),\Sigma_{z})$
\end_inset

.
 We are looking for the Taylor expansion of the expected RE (ERE) around
 
\begin_inset Formula $z_{0}=\mbmu_{\mbphi}(\mbx)$
\end_inset

, 
\change_deleted -767166615 1590442815
i.e.
\change_inserted -767166615 1590442815
i.e.,
\change_unchanged
 the first moment.
 For notational simplicity, we use 
\begin_inset Formula $H_{z}$
\end_inset

 to denote the Hessian 
\begin_inset Formula $R''(\mbmu_{\mbphi}(\mbx))$
\end_inset

.
 The derivation is formalized as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
E_{\mbz\sim q_{\phi}}R(\mbz) & =R(\mbmu_{\mbphi}(\mbx))+R'(\mbmu_{\mbphi}(\mbx))E_{\mbz\sim q_{\phi}}[\mbz-\mbmu_{\mbphi}(\mbx)])\nonumber \\
 & \quad+\frac{1}{2}E_{\mbz\sim q_{\phi}}[(\mbz-\mbmu_{\mbphi}(\mbx))^{\top}\mathbf{H}_{z}(\mbz-\mbmu_{\mbphi}(\mbx))]+O(\|(\mbz-\mbmu_{\mbphi}(\mbx)\|^{3}\nonumber \\
 & \simeq R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}E_{\mbz\sim q_{\phi}}[(\mbz-\mbmu_{\mbphi}(\mbx))^{\top}\mathbf{H}_{z}(\mbz-\mbmu_{\mbphi}(\mbx))]\nonumber \\
 & =R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}E[(\mbz-\mbmu_{z})(\mbz-\mbmu_{z})^{T}])\nonumber \\
 & =R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\Sigma_{z})\label{eq:derived-ere2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Note for 
\begin_inset Formula $ERE_{1}$
\end_inset

, the second term 
\begin_inset Formula $\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\Sigma_{z})$
\end_inset

 drops and we are left with 
\begin_inset Formula $R(\mbmu_{\mbphi}(\mbx))$
\end_inset

 only.
 For 
\begin_inset Formula $ERE_{2}$
\end_inset

, since 
\begin_inset Formula $\Sigma_{z}$
\end_inset

 is a diagonal matrix, 
\begin_inset Formula $\mathrm{tr}(\mathbf{H}_{z}S_{z})=\mathrm{tr}(diag(\mathbf{H}_{z})S_{z})=\sum_{i}(\mathbf{H}_{z})_{ii}(S_{z})_{ii}$
\end_inset

 holds.
\end_layout

\begin_layout Standard
We can utilize this result to compute 
\begin_inset Formula $ERE_{2}$
\end_inset

, in a more computationally efficient manner.

\change_deleted -767166615 1590430571
 
\change_unchanged

\end_layout

\end_body
\end_document
