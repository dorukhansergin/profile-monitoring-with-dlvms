#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrartcl
\begin_preamble



\graphicspath{{./figs/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.eps}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage{cleveref}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{stfloats}
\usepackage[super]{nth}
% for inkscape images
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgf}



\g@addto@macro\@floatboxreset\centering


% \usepackage{hyperref}
 
 \AtBeginDocument{% Overrides ref for Cref
 	\let\ref\Cref
 }

\crefalias{prop}{proposition}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
theorems-sec
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plainnat
\biblatex_bibstyle authoryear
\biblatex_citestyle authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% INPUT PREAMBLES
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset CommandInset include
LatexCommand include
filename "preamble/preamble_glossary.lyx"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:introduction"

\end_inset

 Profile monitoring has attracted a growing interest in the literature in
 the past decades 
\begin_inset CommandInset citation
LatexCommand citep
key "Woodall2004-bp,Woodall2007-xs,Maleki2018-uo"
literal "false"

\end_inset

 for its ability to construct control charts with much better representations
 for certain types of process measurements.
 A profile can be defined as a functional relationship between the response
 variables and explanatory variables or spatiotemporal coordinates.
 In this work, we focus on the case where the profiles generated from the
 process are high-dimensional (HD)â€”
\shape italic
i.e.
\shape default
, the number of such explanatory variables or spatiotemporal coordinates
 are large.
 Specifically, we focus on sets of HD profiles for which intra-sample variation
 lies on a nonlinear low-dimensional manifold 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
 Our motivating example of such HD profiles is presented in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in which we exhibit a sample of surface defect image profiles collected
 from a hot steel rolling process.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/profile_examples.pdf
	width 25theight%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
A collection of 64 by 64 image profiles taken from a hot steel rolling process.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Rolling"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In literature, profile monitoring techniques can be categorized by their
 assumptions on the type of the functional relationship that they assume.
 Linear profile monitoring can be considered the most basic profile monitoring
 technique, in which it is assumed that the profile can be represented by
 a linear function.
 The idea is to extract the slope and the intercept from each profile and
 monitor its coefficients 
\begin_inset CommandInset citation
LatexCommand citep
key "zhu2009monitoring"
literal "false"

\end_inset

.
 Regularization techniques can also be used in linear profile estimation.
 For example, Zou 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
etal
\end_layout

\end_inset

 utilize a multivariate linear regression model for profiles with the LASSO
 penalty and use the regression coefficients for Phase-II monitoring 
\begin_inset CommandInset citation
LatexCommand citep
key "zou2012lasso"
literal "false"

\end_inset

.
 However, the assumption of linear functional relationship can be quite
 limiting.
 To address this challenge, nonlinear parametric models are proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "Williams2007-ty,Jensen2009-tu,Noorossana2011-oj,Maleki2018-uo"
literal "false"

\end_inset

.
 These models assume an explicit family of parameterized functions and,
 their parameter estimations are estimated via nonlinear regression.
 In any cases, the drawback of all parametric models is that these models
 assume the parametric form is known beforehand, which might not always
 be the case.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% Literature: non-parametric Methods for certain types of profiles
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Another large body of profile monitoring research focuses on the type of
 profile data where the basis of the representation is assumed to be known
 but the coefficients are unknown.
 For instance, to monitor smooth profiles, various non-parametric methods
 based on local kernel regression 
\begin_inset CommandInset citation
LatexCommand citep
key "zou2008monitoring,qiu2010nonparametric,zou2009nonparametric"
literal "false"

\end_inset

 and splines 
\begin_inset CommandInset citation
LatexCommand citep
key "chang2010statistical"
literal "false"

\end_inset

 are developed.
 To monitor the non-smooth wave-form signals, a wavelet-based mixed effect
 model is proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "paynabar2011characterization"
literal "false"

\end_inset

.
 However, for all the aforementioned methods, it is assumed that the nonlinear
 variation pattern of the profile is well captured by a known basis or kernel.
 Usually, there is no guidance on selecting the right basis of the representatio
n for the original data and it normally requires many trial and error.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%Here, we focus on the HD profiles where we cannot assume a parametric function
 form and the basis representation is unknown.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

In the case that the basis of HD profiles are not known, dimensionality
 reduction techniques are widely used.
 Principal component analysis (PCA) is arguably the most popular method
 in this context for profile data monitoring because of its simplicity,
 scalability, and good data compression capability.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "liu1995control"
literal "false"

\end_inset

, PCA is proposed to reduce the dimensionality of the streaming data and,
 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 charts are constructed to monitor the extracted representations and residuals,
 respectively.
 To generalize PCA methods to monitor the complex correlation among the
 channels of multi-channel profiles, 
\begin_inset CommandInset citation
LatexCommand citet
key "paynabar2015change"
literal "false"

\end_inset

 propose a multivariate functional PCA method and apply change point detection
 methods on the function coefficients.
 Along this line, tensor-based PCA methods are also proposed for multi-channel
 profiles, examples including uncorrelated multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
key "paynabar2013monitoring"
literal "false"

\end_inset

 and multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
key "grasso2014profile"
literal "false"

\end_inset

.
 Finally, various tensor-based PCA methods 
\begin_inset CommandInset citation
LatexCommand citep
key "yan2015image"
literal "false"

\end_inset

 are compared and different test statistics are developed for tensor-based
 process monitoring.
\end_layout

\begin_layout Standard
The main limitation of all the aforementioned PCA-related methods is that
 the expressive power of linear transformations is very limited.
 Furthermore, each principal component represents a global variation pattern
 of the original profiles, which is not efficient at capturing the local
 spatial correlation within a single profile.
 Therefore, PCA requires a much larger latent space dimensions than the
 dimension of the actual latent space, yielding a sub-optimal and overfitting-pr
one representation.
 This phenomena hinders the profile monitoring performance.
\end_layout

\begin_layout Standard
A systematic discussion of this issue is articulated in 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
 In that work, the authors identify the problems associated with assuming
 a closeness relationship in the subspace that is characterized by Euclidean
 metrics.
 They successfully observe that the intra-sample variation in complex high-dimen
sional corpora may lie on a nonlinear manifold as opposed to a linear manifold
 which is assumed by PCA and related methods.
 However, the authors only focus on applying manifold learning for Phase-I
 analysis, while Phase-II monitoring procedure is not touched upon 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Deep dimensionality reduction models have been proposed as an alternative
 to classical dimensionality reduction techniques in a handful.
 Deep autoencoders have been proposed for profile monitoring for Phase-I
 analysis in 
\begin_inset CommandInset citation
LatexCommand citet
key "Howard2018-op"
literal "false"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Yan2016-wa"
literal "false"

\end_inset

 compared the performance of contractive autoencoders and denoising autoencoders
 for Phase-II monitoring.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang2018-js"
literal "false"

\end_inset

 proposed a denoising autoencoder for process monitoring.
 Aside from deterministic deep neural networks, only three works 
\begin_inset CommandInset citation
LatexCommand citep
key "wang2019systematic,Zhang2019-lu,lee2019process"
literal "false"

\end_inset

 proposed to use deep probabilistic latent variable models, specifically,
 variational autoencoders (VAE), for Phase-II monitoring.
 All of the proposed monitoring statistics in those works differ slightly
 but conceptually they are all extensions of the classic 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

-charts of PCA.
 We argue that there is room for improvement for the monitoring statistic
 formulations in those works for a number of reasons, especially when high-dimen
sional profiles are considered.
 In this work, we propose a new monitoring statistic formulation to address
 this issue.
 The contributions of this work can be listed as follows:
\end_layout

\begin_layout Itemize
We demonstrate why latent variable-based monitoring statistics should not
 be used due to the two issues related to deep encoders: (1) their representatio
ns are likely entangled, (2) they will likely fail to extrapolate well beyond
 the region of in-control profiles.
\end_layout

\begin_layout Itemize
We demonstrate why observable variable-based monitoring statistics should
 not be estimated via sampling due to prohibitively expensive computational
 demand.
\end_layout

\begin_layout Itemize
We propose a better monitoring statistic that responds robustly and in most
 cases more accurately than previous formulations.
 We demonstrate the effectiveness of this new formulation on both simulation
 and real-life case studies.
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Subsection
Variational Autoencoders
\end_layout

\begin_layout Standard
In this section, we introduce Variational Autoencoder (VAE) 
\begin_inset CommandInset citation
LatexCommand citep
key "Kingma2013-dl"
literal "false"

\end_inset

 which is the primary modeling tool in this work.
 In order to do that, we believe a brief introduction to Gaussian factorized
 latent variable models is necessary.
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:lvms"

\end_inset

 Latent variables models are powerful tools to model complex distributions
 over high-dimensional spaces.
 The underlying assumption is that there exists a low-dimensional latent
 structure that explains well the variations in the high-dimensional observed
 space.
 Typically, the density over observed variables can be decoupled into the
 distribution on the latent variables and the conditional distribution of
 observed variables given latent variables can be assigned as tractable
 families of distributions, which will be much more efficient than modeling
 the data distribution directly.
\end_layout

\begin_layout Standard
A typical example of latent variable models is when the joint distribution
 is Gaussian factorized as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Formula 
\begin{equation}
\begin{split}\pz & =\Norm(\mbz;0,\mbI_{r})\\
\decoding & =\Norm(\mbx;\mu_{\mbtheta}(\mbz),\sigma^{2}\mbI_{d})\\
p_{\mbtheta}(\mbx,\mbz) & =\decoding\pz
\end{split}
\label{eq:gaussian-factorized}
\end{equation}

\end_inset

In the above formulation 
\begin_inset Formula $\mbx\in\R^{d}$
\end_inset

 are observed samples, 
\begin_inset Formula $\mbz\in\R^{r}$
\end_inset

 are latent variables while 
\begin_inset Formula $\mu_{\mbtheta}\colon\R^{r}\to\R^{d}$
\end_inset

 is a function parameterized by 
\begin_inset Formula $\mbtheta\in\Theta$
\end_inset

, that describes the relationship between the latent variables and the mean
 of the conditional distribution.
 The Gaussian prior 
\begin_inset Formula $\pz$
\end_inset

 is typically chosen to be standard to avoid degenerate solutions 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 307"
key "roweis1999unifying"
literal "false"

\end_inset

 and conditional covariance is typically assumed to be isotropic 
\begin_inset Formula $\sigma^{2}I_{d}$
\end_inset

 to avoid ill-defined problems.
 The aim is to approximate the true density 
\begin_inset Formula $p_{\mbtheta}(\mbx)\approx p(\mbx)$
\end_inset

 and this approximation can be obtained through marginalization: 
\begin_inset Formula 
\[
p_{\mbtheta}(\mbx)=\int p_{\mbtheta}(\mbx,\mbz)d\mbz
\]

\end_inset

Finally, in literature, there has been discussions about whether the independent
 latent structure assumption 
\begin_inset Formula $\pz=\Norm(\mbz;0,\mbI_{r})$
\end_inset

 can lead to the discovery of the true disentangled variations, a task also
 known as disentangled representation learning 
\begin_inset CommandInset citation
LatexCommand citep
after "Sec. 3.5"
key "bengio2013representation"
literal "false"

\end_inset

.
 Disentangled representations are useful to represent variations in latent
 variations due to its ability to separate out the independent factors.
 We are interested in whether and if so, how, such representations will
 be critical for profile monitoring.
\end_layout

\begin_layout Standard
A famous member of the family of models described above is the probabilistic
 principal component analysis (PPCA) 
\begin_inset CommandInset citation
LatexCommand citep
key "tipping1999probabilistic"
literal "false"

\end_inset

.
 The parameters are optimized via a maximum likelihood estimation framework
 and it can be solved analytically due to the fact that 
\begin_inset Formula $\mu_{\mbtheta}$
\end_inset

 is a simple linear transformation enabling the optimization to reuse results
 from original solutions to the PCA problem.
\end_layout

\begin_layout Standard
The assumption of PPCA that the latent and observed variables have a strictly
 linear relationship can be quite restricting.
 In real-world processes, it is likely that this relationship is highly
 nonlinear.
 Deep latent variable models are a marriage of deep neural networks and
 latent variable models that aim to solve this problem.
 Deep learning has enjoyed a tremendous resurgence in the last decade due
 to their superior performance that was unprecedented for many tasks such
 as image classification 
\begin_inset CommandInset citation
LatexCommand citep
key "krizhevsky2012imagenet"
literal "false"

\end_inset

, machine translation 
\begin_inset CommandInset citation
LatexCommand citep
key "bahdanau2014neural"
literal "false"

\end_inset

, and speech recognition 
\begin_inset CommandInset citation
LatexCommand citep
key "amodei2016deep"
literal "false"

\end_inset

.
 In theory, under sufficient conditions, a two layer multilayer perceptron
 can approximate any function on a bounded region 
\begin_inset CommandInset citation
LatexCommand citep
key "cybenko1989approximation,Hornik1991-li"
literal "false"

\end_inset

.
 Growing the width of shallow networks in an exponential fashion for arbitrarily
 complex tasks is not practical.
 It has been shown that deeper representations can often achieve the better
 expressive power than shallow networks with less parameters due to the
 efficient reuse of the previous layers 
\begin_inset CommandInset citation
LatexCommand citep
key "eldan2016power"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
VAE is arguably the most foundational member of the deep latent variable
 model family.
 The main difference between PPCA and VAE is that the latter replace the
 linear transformation with a high-capacity deep neural network (called
 
\shape italic
generative
\shape default
 or 
\shape italic
decoder
\shape default
).
 This is powerful in the sense that along with a general purpose prior 
\begin_inset Formula $\pz$
\end_inset

 a wide variety of densities can be modeled 
\begin_inset CommandInset citation
LatexCommand citep
key "kingma2019introduction"
literal "false"

\end_inset

.
 Unlike PPCA, these models will not have analytical solutions due to the
 complex nature of the neural network used.
 Like most other deep learning models, their parameters have to be optimized
 via gradient descent for maximum likelihood.
 The problem becomes even harder given the observation that the posterior
 
\begin_inset Formula $\decoding$
\end_inset

 takes meaningful values only for a small sub-region within 
\begin_inset Formula $\R^{r}$
\end_inset

.
 This makes sampling from the prior 
\begin_inset Formula $\pz$
\end_inset

 to estimate the likelihood prohibitively expensive.
 Both models work around this problem using the importance sampling framework
 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 532"
key "bishop2006pattern"
literal "false"

\end_inset

, where they introduce another network (called 
\shape italic
recognition
\shape default
 or 
\shape italic
encoder
\shape default
) to approximate a proposal distribution 
\begin_inset Formula $\encoding$
\end_inset

 â€”parametrized by 
\begin_inset Formula $\mbphi$
\end_inset

â€” which will hopefully sample latent variables from a much smaller region
 that is more likely to produce higher posterior densities for a given input
 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Standard
The ultimate output of a trained VAE is the likelihood estimator.
 Once the two networks are trained, the log-likelihood 
\begin_inset Formula $\log\ptheta(\mbx)$
\end_inset

 can be approximated by a Monte Carlo sampling procedure with 
\begin_inset Formula $L$
\end_inset

 iterations 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 30"
key "kingma2019introduction"
literal "false"

\end_inset

: 
\begin_inset Formula 
\begin{equation}
\log\ptheta(\mbx)\approx\log\frac{1}{L}\sum_{l=1}^{L}\frac{\ptheta(\mbx,\mbz^{(l)})}{\qphizgivenx{\mbz^{(l)}}{\mbx}}\label{eqn:SummationLL}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
VAEs are trained to maximize the so-called evidence lower bound (ELBO),
 which is deemed a proxy to the likelihood: 
\begin_inset Formula 
\begin{equation}
\begin{split}\text{ELBO} & \triangleq\log\left(p(\mbx)\right)-\KL{\encoding}{q^{*}(\mbz|\mbx)}\\
 & =\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)}
\end{split}
\label{eqn:VAELoss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\KL{\cdot}{\cdot}$
\end_inset

 denotes the Kullback-Leibler divergence (KLD) between two distributions.
 The left-hand side is the quantity of interest while the right-hand side
 is the tractable expression that guides the updating of parameters 
\begin_inset Formula $\mbtheta,\mbphi$
\end_inset

 in an end-to-end fashion.
\end_layout

\begin_layout Subsection
Convolutional Layers
\end_layout

\begin_layout Standard
In this work, we use convolutional encoders and decoders in our VAE architecture.
 Introduced in 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun1989backpropagation"
literal "false"

\end_inset

, convolutional layers have enabled tremendous performance increase in certain
 neural network applications where the data is of a certain spatial neighborhood
 structure such as images or audio waveform.
 They exploit an important observation of such data, where the learner should
 be equivariant to translations.
 This is an important injection of inductive bias into the network that
 largely reduce the number of parameters compared to the fully connected
 network by the use of parameter sharing.
 It eventually increase the statistical learning efficiency, especially
 for small samples.
 It must be noted however, convolutional layers are not equivariant to scale
 and rotation as they are to translation.
 Knowing what sort of inductive biases are injected into these layers is
 important for the understanding of disentanglement, which we will introduce
 later in this paper.
\end_layout

\begin_layout Subsection
Review of 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics in PCA
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:ReviewPCA"

\end_inset

 Process monitoring via PCA is typically undertook using the so-called 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics 
\begin_inset CommandInset citation
LatexCommand citep
key "Chen2004-px"
literal "false"

\end_inset

.
 The 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA is defined as the reconstruction error between the real
 sample 
\begin_inset Formula $\mbx$
\end_inset

 and the reconstructed sample 
\begin_inset Formula $\tilde{\mbx}$
\end_inset

.
 Its geometric representation is how far the sample is away from the learned
 subpsace of in-control (IC) samples.
 
\begin_inset Formula $\Tsq$
\end_inset

 represents how far the sample is away from the cluster of latent codes
 of the IC samples.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\Tsq$
\end_inset

 statistics and 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA are defined as follows: 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%TODO: give numbers to each line or not?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% No need
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Formula 
\begin{equation}
\begin{split}Q(\mbx) & =\gg\mbx-\tilde{\mbx}\gg^{2}\\
\Tsq_{PCA}(\mbx) & =\mbz^{\top}\mbSigma\inv_{r}\mbz=\mbx^{\top}\mbW_{r}\mbSigma\inv_{r}\mbW_{r}^{\top}\mbx,
\end{split}
\label{eqn: QTPCA}
\end{equation}

\end_inset

where matrix 
\begin_inset Formula $\mbW_{r}$
\end_inset

 is the loading matrix, and 
\begin_inset Formula $\mbSigma\inv_{r}$
\end_inset

 is the inverse of the covariance matrix when only the first 
\begin_inset Formula $r$
\end_inset

 principal components are kept.
 There are various methods to choose 
\begin_inset Formula $r$
\end_inset

 such as fixing the percentage of variation explained 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 41"
key "Chiang2001-nu"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
For processes with relatively small latent and residual dimensionality,
 the upper control limits of these statistics for the 
\begin_inset Formula $\alpha$
\end_inset

% Type-1 error tolerance is constructed by employing the normality assumptions
 of PPCA 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 43-44"
key "Chiang2001-nu"
literal "false"

\end_inset

.
 However, using such measures for high-dimensional nonlinear profiles is
 prohibitively error-prone as both 
\begin_inset Formula $r$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 will be much higher than the assumptions on chi-square distribution can
 tolerate.
 As an alternative, non-parameteric methods to estimate upper percentiles
 are increasingly used for this purpose, such as simple sample percentile
 on a held-out set or fitting kernel density estimation to in-control statistics.
\end_layout

\begin_layout Subsection
Review and Critique of Previously Proposed Monitoring Statistics Proposed
 for VAE
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:critique"

\end_inset

 Three works have considered VAE for process monitoring, all of which propose
 different statistic formulations for monitoring.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang2019-lu"
literal "false"

\end_inset

 formulate what they call 
\begin_inset Formula $H^{2}$
\end_inset

 which is basically the Mahalanobis distance of the mean of the proposal
 distribution from standard Gaussian distribution.
 
\begin_inset Formula 
\begin{equation}
H^{2}=\mu_{\mbphi}(\mbx)^{\top}\mu_{\mbphi}(\mbx)
\end{equation}

\end_inset

The major drawback of using only this statistic is that it completely ignores
 the disturbances in residual distribution.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

 claim to extend 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $SPE$
\end_inset

 of PCA for VAE.
 For a given input 
\begin_inset Formula $\mbx$
\end_inset

 and a singe sample drawn from proposal 
\begin_inset Formula $\mbz^{(l)}\sim\encoding$
\end_inset

 and a reconstruction based on that sample 
\begin_inset Formula $\mbx^{(l)}\sim p_{\mbtheta}(\mbx\g\mbz^{(l)})$
\end_inset

, the proposed test statistics in this work are as follows: 
\begin_inset Formula 
\begin{equation}
\begin{aligned}T^{2} & =(\mbz^{(l)}-\bar{\mbz})^{\top}S_{\mbz}\inv(\mbz^{(l)}-\bar{\mbz})\\
SPE & =\gg\mbx^{(l)}-\mbx\gg_{2}^{2},
\end{aligned}
\end{equation}

\end_inset

where 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 are estimated over a single loop from the data.
 It is unclear why they would use such an extra step since for a well trained
 VAE, 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 would be approximately equal to the mean and covariance of the standard
 Gaussian distribution.
 Instead, they take on the additional risk associated with the estimation.
 The proposed control charting methodology suggests that these two statistics
 work in combination and at least one vote of either statistics is enough
 to make a detection.
 The authors do not mention how false alarm rate can be fixed given this
 methodology.
\end_layout

\begin_layout Standard
Finally, 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2019systematic"
literal "false"

\end_inset

 propose the 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 statistics by focusing on the two major components of the tractable part
 of the objective function of VAE shown as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\begin_inset Formula $D$
\end_inset

 statistic is simply the KL divergence between the prior and proposal.
 For 
\begin_inset Formula $R$
\end_inset

 statistic, like 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

, they employ summary statistics over samples from proposal but also claim
 that sampling size can be fixed to one: 
\begin_inset Formula 
\begin{equation}
\begin{aligned}D & =\KL{\encoding}{p(\mbz)}\\
R & =\frac{1}{L}\sum_{l=1}^{L}-\log q_{\mbtheta}(\mbx\g\mbz^{(l)}),
\end{aligned}
\label{eq: DR}
\end{equation}

\end_inset


\begin_inset Formula $SPE$
\end_inset

 in and 
\begin_inset Formula $R$
\end_inset

 are essentially the same quantities up to a constant, which makes them
 identical in the context of monitoring statistic because the rankings for
 the same testing samples given the same model will be the same for both.
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:methodology"

\end_inset

 
\end_layout

\begin_layout Subsection
Proposed Monitoring Statistic
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:proposed-statistic"

\end_inset


\end_layout

\begin_layout Standard
Log-likelihood, 
\begin_inset Formula $\log\ptheta(\mbx)$
\end_inset

, arises as a natural candidate for monitoring statistic in the context
 of DLVMs.
 That is, given a well trained DLVM, in-control samples should have relatively
 higher log-likelihood than out-of-control samples.
 However, the required number of Monte Carlo samplesâ€”
\begin_inset Formula $L$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:SummationLL"
plural "false"
caps "false"
noprefix "false"

\end_inset

â€” can be prohibitively large to get meaningful estimates of the likelihood
 
\begin_inset CommandInset citation
LatexCommand citep
key "Kingma2013-dl"
literal "false"

\end_inset

, which do not satisfy the real-time monitoring requirement for high throughput
 systems.
 To address this issue, ELBO defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be used for a reasonable approximation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)}
\]

\end_inset


\end_layout

\begin_layout Standard
To understand the role of both terms in process monitoring, we revisit the
 assumptions of the model described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Let us formally represent an out-of-control distribution as 
\begin_inset Formula $p_{\delta}(\mbx)\neq p(\mbx)$
\end_inset

.
 Since 
\begin_inset Formula $p(\mbx)=\int p(\mbx\g\mbz)p(\mbz)d\mbz$
\end_inset

, we can observe two sources of out-of-control behaviours: disturbances
 in latent distribution 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

 and disturbances in observable distribution 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

.
 Note that various combinations of these two disturbances cover disturbances
 in the entire process.
 One can argue that 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 can detect the disturbances in the observable space 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

 and 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 represents the change in the latent space 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

.
 We know that for processes that can be accurately modeled by PCA, both
 terms play an important role 
\begin_inset CommandInset citation
LatexCommand citep
key "kim2003process"
literal "false"

\end_inset

 for process monitoring.
 We argue that this holds true for PPCA too.
 To prove this, we link the monitoring statistics of PCA (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:ReviewPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

) to PPCA using the ELBO framework.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop: T2Q"

\end_inset

 We know from the definition of PPCA 
\begin_inset CommandInset citation
LatexCommand citep
key "tipping1999probabilistic"
literal "false"

\end_inset

 that the prior, encoding and decoding functions are normally distributed
 as: 
\begin_inset Formula 
\[
\begin{split}p(\mbz) & =\Norm(0,\mbI)\\
\decoding & =\Norm(\mbW\mbz,\sigma^{2}\mbI)\label{eq:Gaussian}
\end{split}
\]

\end_inset

In this case, from PPCA, the encoder also follows the normal distribution
 as 
\begin_inset Formula $\encoding=\Norm(\mu_{\mbphi}(\mbx),\Sigma_{z})$
\end_inset

, where 
\begin_inset Formula $\mu_{\mbphi}(\mbx)=\mbM^{-1}\mbW^{\top}\mbx$
\end_inset

 and 
\begin_inset Formula $\Sigma_{z}=\sigma^{2}\mbM^{-1})$
\end_inset

, where 
\begin_inset Formula $\mbM=\mbW^{\top}\mbW+\sigma^{2}\mbI$
\end_inset

.
 Then, the two monitoring statistics can be defined as: 
\begin_inset Formula 
\begin{equation}
\KL{\encoding}{p(\mbz)}=\frac{1}{2}\gg\mu_{\mbphi}(\mbx)\gg^{2}+C_{1}\label{eqn:KL_PPCA}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\E_{\mbz\sim q_{\mbphi}}\log\decoding\propto\gg\mbx-\mbW\mu_{\mbphi}(\mbx)\gg^{2}+C_{2}\label{eqn:E_PPCA}
\end{equation}

\end_inset

where 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{2}$
\end_inset

 are constants that doesn't depend on 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
The proof is given in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:PoofOfPropTQ"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that the constants do not affect out-of-control decisions.
 Thus, the test statistics 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 is equivalent to the 
\begin_inset Formula $T^{2}$
\end_inset

 Statistics of PCA as defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn: QTPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

, and 
\begin_inset Formula $\E_{\mbz\sim q_{\mbphi}}\log\decoding$
\end_inset

 is equivalent the 
\begin_inset Formula $Q$
\end_inset

 statistic.
 Observe that previously proposed formulations mentioned in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

 relyâ€”directly or indirectlyâ€”on this framework.
 Statistics 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $SPE$
\end_inset

 are based on 
\begin_inset Formula $Q$
\end_inset

 or in the case of PPCA 
\begin_inset Formula $\E_{\mbz\sim q_{\mbphi}}\log\decoding$
\end_inset

.
 Let us call these 
\emph on
observable variable-based statistics
\emph default
.
 Likewise, 
\begin_inset Formula $H^{2},T^{2}$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 is based on 
\begin_inset Formula $T^{2}$
\end_inset

 of PCA or 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 of PPCA.
 We shall call these statistics, 
\emph on
latent-variable based statistics
\emph default
, as they rely exclusively on latent representations.
\end_layout

\begin_layout Standard
Our first major claim is that latent variable-based statistics are not useful
 for profile monitoring when deep neural network based encoders are used
 to produce latent representations.
 We cite two reasons for that.
 First, for such high-dimensional complex processes, most of the change
 or disturbances should be expected on residual distribution.
 According to 
\begin_inset CommandInset citation
LatexCommand citet
key "severson2016perspectives"
literal "false"

\end_inset

 faults in complex real-life processes tend to alter the existing relationship
 between latent sources of variation and what is observed, as opposed to
 pushing to most extreme cases in the latent variational sources.
\end_layout

\begin_layout Standard
Second, the mean encoder 
\begin_inset Formula $\mu_{\mbphi}$
\end_inset

â€”which is a deep encoderâ€”is likely to lack two important features: disentangled
 representations and extrapolation capabilities.
 These two qualities are required so that extreme values of latent variables
 consistently converge out of in-control zone.
 We illustrate this in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:entang-extrap"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Observe how Point C is falsely identified as in-control when learned representa
tions are not disentangled or when they fail to extrapolate.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/Disentangled_Extrapolated.pdf
	lyxscale 50
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Different ways how an independent source of variation is learned.
 
\series bold
Top Left:
\series default
 A hypothetical in-control latent space (gray area) and a range of independent
 source of variation (line).
 Points A and B are extreme values of in-control and point C is an out-of-contro
l sample.
 
\series bold
Top Right: 
\series default
An ideal learned representation.
 
\series bold
Bottom Left:
\series default
 A learned representation fails to extrapolate.
 
\series bold
Bottom Right: 
\series default
A learned representation that is entangled.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:entang-extrap"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Theorem 1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "locatello2018challenging"
literal "false"

\end_inset

 proves that without proper inductive biases injected into the model about
 sources of variation, it is impossible to find disentangled representations.
 Unfortunately, injecting such inductive biases requires detailed anticipation
 of variations among in-control samples as well as specialized neural network
 structures, both of which are extremely challenging tasks given the potential
 complexity of the processes that generates high-dimensional profiles.
\end_layout

\begin_layout Standard
Moreover, even if the representations are disentangled, correct mappings
 of 
\begin_inset Formula $\encoding$
\end_inset

 may still not be obtained due to failure to extrapolate.
 Deep neural networks approximates well only at a bounded domain defined
 by where the training setâ€”in our case, the in-control setâ€”is densely sampled
 from.
 The behavior of the function is often unpredictable outside that domain.
 In other words, it may not extrapolate well beyond the domain of training
 samples.
 We refer interested readers to 
\begin_inset CommandInset ref
LatexCommand ref
reference "app:rosenbrock"
plural "false"
caps "false"
noprefix "false"

\end_inset

 where we replicated this phenomena on a toy example.
 To see why this is a problem, first note that the encoder 
\begin_inset Formula $\encoding$
\end_inset

 will only be trained with profiles densely sampled from the bounded region
 of in-control samples, for which 
\begin_inset Formula $\pz$
\end_inset

 are high.
 The behavior of the encoder is uncertain for profiles coming from dense
 regions of out-of-control latent structure 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

.
 We expect increased false negatives should the model falsely map these
 profiles onto high density regions of 
\begin_inset Formula $\pz$
\end_inset

 but not 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

.
\end_layout

\begin_layout Standard
Unlike latent variable-based statistics, extrapolation and disentanglement
 issues in 
\begin_inset Formula $\encoding$
\end_inset

 along with extrapolation issues in 
\begin_inset Formula $\decoding$
\end_inset

 help observable variable-based statistic detect faults better.
 More interestingly, this holds true even when the disturbance is purely
 on the latent structure.
 Incorrect mappings by 
\begin_inset Formula $\encoding$
\end_inset

 will lead to incorrect generations by 
\begin_inset Formula $\decoding$
\end_inset

 and thus larger statistic that will fall beyond control limits.
 Second, even if the mapping was correct, we might have extrapolation issues
 in the decoder 
\begin_inset Formula $\decoding$
\end_inset

 which is another possibility for this statistic to capture disturbances
 in latent variations.
 To see why this is the case, note that the encoder 
\begin_inset Formula $\encoding$
\end_inset

 is optimized to produce samples more from where 
\begin_inset Formula $\pz$
\end_inset

 is large.
 In turn, the decoder 
\begin_inset Formula $\decoding$
\end_inset

 will mostly be trained on samples from where 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

 is low.
 For disturbances in observable region 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

, it is trivial to see how 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 will be effective at capturing faults.
\end_layout

\begin_layout Standard
As of now, we have enough reasons to recommend the use of observable variable-ba
sed statistics as the only type of test statistic for a profile monitoring
 application that uses VAE.
 Indeed, variations of this type have been proposed previously: 
\begin_inset Formula $SPE$
\end_inset

 of 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2019systematic"
literal "false"

\end_inset

.
 However, they both use random samples from the proposal distribution to
 estimate the expectation.
 This approach may require a large number of samples to be generated and
 thus a large number of forward passes on the decoder network, which is
 prohibitively expensive in terms of computation.
 We propose using first order Taylor expansion instead.
 We call this approximation 
\begin_inset Formula $ERE$
\end_inset

, which stands for expected reconstruction error due to the resemblance
 of this statistic to the reconstruction error in deterministic autoencoders.
 
\begin_inset Formula $ERE$
\end_inset

 can be derived as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
ERE & \triangleq\E_{\mbz\sim q_{\mbtheta}}\log\decoding\label{eqn:propstat:final}\\
 & =\E_{\mbz\sim p_{\mbtheta}}\left[\log q_{\mbtheta}(\mbx\g\mbz-(\mu_{\mbphi}-\mbz))\right]\\
 & \approx\log\ptheta(\mbx\g\mu_{\mbphi}(\mbx))\\
 & \propto\gg\mbx-\mu_{\mbtheta}(\mu_{\mbphi}(\mbx))\gg_{2}^{2}+C_{3}
\end{align}

\end_inset

The constant 
\begin_inset Formula $C_{3}$
\end_inset

 can be ignored given control charting logistics.
 Given a trained VAE, this quantity can be computed by forward passing the
 new profile from the process 
\begin_inset Formula $\mbx$
\end_inset

 through 
\begin_inset Formula $\mu_{\mbphi}$
\end_inset

 and 
\begin_inset Formula $f_{\mbtheta}$
\end_inset

 successively and calculating the squared prediction error, without any
 reparameterization.
\end_layout

\begin_layout Subsection
Profile Monitoring Procedure
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:methodology:procedure"

\end_inset

 A typical profile monitoring follows two phases: Phase-I analysis, and
 Phase-II analysis.
 Phase-I analysis focuses on understanding the process variability by training
 an appropriate in-control mode, and selecting an appropriate control limit.
 In our case, Phase-I analysis results in a trained model (i.e.
 an encoder and a decoder) and an Upper Control Limit (UCL) to help setup
 the control chart for each of the monitoring statistics.
 In Phase-II, the system is exposed to new profiles generated by the process
 to decide whether these profiles are in-control or out-of-control.
 Our experimentation plan, outlined below, is formulated to emulate this
 scenario to effectively assess the performance of any combination of a
 model, a test statistic and a disturbance scenario.
 
\end_layout

\begin_layout Itemize
Obtain IC dataset 
\begin_inset Formula $\dataset$
\end_inset

 and partition it into train, validation and test sets 
\begin_inset Formula $\dataset^{trn}$
\end_inset

,
\begin_inset Formula $\dataset^{val}$
\end_inset

,
\begin_inset Formula $\dataset^{tst}$
\end_inset

 
\end_layout

\begin_layout Itemize
Train VAE using samples from 
\begin_inset Formula $\dataset^{trn}$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculate test statistic for all 
\begin_inset Formula $\mbx\in\dataset^{val}$
\end_inset

 and take it's 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

95
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 percentile as the UCL.
\end_layout

\begin_layout Itemize
Start admitting profiles from the process.
 Calculate test statistic using the trained VAE.
 If test statistic is over UCL, identify the sample as OC.
 
\end_layout

\begin_layout Standard
We train 10 different model instances with different seeds to account for
 inherent randomness due to weight initialization of deep neural networks.
\end_layout

\begin_layout Subsection
Neural Network Architectures 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-Architectures"

\end_inset


\end_layout

\begin_layout Standard
We use the encoder-decoder structure outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The layers used that builds the model architectures used in this study
 are summarized as follows:
\end_layout

\begin_layout Itemize
C(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional layer with arguments referring to number of output channels
 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

 and size of zero-padding 
\begin_inset Formula $P$
\end_inset

.
 
\end_layout

\begin_layout Itemize
CT(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional transpose layer with arguments referring to the number
 of output channels 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

, and size of zero-padding 
\begin_inset Formula $P$
\end_inset

.
 
\end_layout

\begin_layout Itemize
FC(
\begin_inset Formula $I,O$
\end_inset

): Fully connected layer with arguments referring to input dimension 
\begin_inset Formula $I$
\end_inset

 and output dimension 
\begin_inset Formula $O$
\end_inset

.
 
\end_layout

\begin_layout Itemize
A: Activation function.
 Leaky ReLU with a negative slope 
\begin_inset Formula $0.2$
\end_inset

.
\end_layout

\begin_layout Standard
Here, C(), CT(), and FC() are considered the linear transformation layers
 while R(), LR(), and S() are considered the nonlinear activation layers.
 Strided convolutions can be used to decrease the spatial dimensions in
 the encoders.
 Pooling layers are typically not recommended in autoencoder-like architectures
 
\begin_inset CommandInset citation
LatexCommand citep
key "radford2015unsupervised"
literal "false"

\end_inset

.
 Convolutional transpose layers are used to upscale latent codes back to
 observable dimensions.
\end_layout

\begin_layout Standard
The sequential order of the computational graphs used for this study are
 summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The encoder will output 
\begin_inset Formula $2r$
\end_inset

 nodes which is a concatenation of the inferred posterior mean and variance,
 both are of length 
\begin_inset Formula $r$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\newcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture details of deep neural networks used in this study
\begin_inset CommandInset label
LatexCommand label
name "tab:model-architectures"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top" width="50text%">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Module 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Architecture
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Encoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
C(32, 4, 2, 1) - A - C(32, 4, 2, 1) - A - C(64, 4, 2, 1) - A - C(64, 4,
 2, 1) - A - C(64, 4, 1, 0) - FC(256, 
\begin_inset Formula $2r$
\end_inset

) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Decoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FC(
\begin_inset Formula $r$
\end_inset

, 256) - A - CT(64, 4, 0, 0) - A - CT(64, 4, 2, 1) - A - C(32, 4, 2, 1)
 - CT(32, 4, 2, 1) - A - CT(1, 4, 2, 1) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Simulation Study Analysis and Results
\end_layout

\begin_layout Standard
In this section, we propose the setup and results of our simulation study.
 The rationale behind conducting a simulation study is to have full control
 and information over the data generating process.
 This way, we can test our claims we make in
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 For every experiment mentioned in this section, we follow the procedure
 outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and we use VAE models with the architecture described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-Architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Simulation Setup
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:simsetting"

\end_inset

 We first evaluate the performance of the deep latent variable models in
 a simulation setting inspired by the work of 
\begin_inset CommandInset citation
LatexCommand citet
key "Shi2016-tg"
literal "false"

\end_inset

.
 The simulation procedure produces 2D point clouds that resemble the scanned
 topology of a gasket bead.
 Let each pixel on a 
\begin_inset Formula $64$
\end_inset

 by 
\begin_inset Formula $64$
\end_inset

 grid be denoted by a tuple 
\begin_inset Formula $\mbp=(p_{0},p_{1})$
\end_inset

.
 The values of the tuples stretch from 
\begin_inset Formula $0$
\end_inset

 to 
\begin_inset Formula $1$
\end_inset

, equally spaced, left to right and bottom-up.
 Each tuple takes a value based on its location through a function 
\begin_inset Formula $\mbp\mapsto f(\mbp;\czero,r)+\epsilon$
\end_inset

, where 
\begin_inset Formula $\epsilon\sim\Norm(0,1\times10^{-2})$
\end_inset

 is i.i.d Gaussian noise.
 The function 
\begin_inset Formula $f$
\end_inset

 is parametrized by the horizontal center location of the bead 
\begin_inset Formula $\czero$
\end_inset

, and the radius of the bead 
\begin_inset Formula $r$
\end_inset

.
 The vertical center of the bead is fixed to be at the center.
 Given any parameter set 
\begin_inset Formula $\{c_{0},r\}$
\end_inset

, each pixel 
\begin_inset Formula $\mbp$
\end_inset

 can be evaluated with the following logic: 
\begin_inset Formula 
\begin{equation}
\begin{split}g(\mbp;c_{0},r) & =1-\frac{(p_{0}-\czero)}{r}^{2}-\frac{(p_{1}-0.5)}{r}^{2}\\
f(\mbp;c_{0},r) & =\begin{cases}
\sqrt{g(\mbp;c_{0},r)} & \mbox{if }g(\mbp;c_{0},r)\geq0\\
0 & \mbox{if }g(\mbp;c_{0},r)<0
\end{cases}
\end{split}
\label{eq:gasketfun}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The samples are best visualized as grayscale images as shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gasketgrid"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Float figure
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:gasketgrid"

\end_inset

 
\begin_inset Graphics
	filename figs/gasket.pdf
	lyxscale 50
	width 90line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Gasket profiles depicted as grayscale images simulated with radius and center
 location they coincide with on the axes.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We define the sources of variation in IC gasket beads by two latent variables
 sampling from independent Gaussian distributions: 
\begin_inset Formula 
\begin{equation}
\begin{split}\czero\sim\Norm(0.5,1\times10^{-2})\\
r\sim\Norm(0.2,6.25\times10^{-4})
\end{split}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Finally, we will consider the following four types of OC variation patterns
 for the system:
\end_layout

\begin_layout Itemize

\series bold
Location shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $\czero$
\end_inset

 is altered by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $\czero\sim\Norm(0.5+\delta\times10^{-2},1\times10^{-2})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Width shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $a$
\end_inset

 is perturbed by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $r\sim\Norm(0.2+\delta\times10^{-4},6.25\times10^{-4})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Mean shift
\series default
: all of the pixels are added an additive disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $f(\mbp;c_{0},r)\leftarrow f(\mbp;c_{0},r)+\delta$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Magnitude shift:
\series default
 all of the pixels are added an multiplicative disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $f(\mbp;c_{0},r)\leftarrow f(\mbp;c_{0},r)*\delta$
\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\delta$
\end_inset

 is the intensity of the change.
 Note that location shift and width shift represent disturbances in latent
 distribution 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

.
 An important distinction between the two is that location equivariance
 is injected into convolutional networks but not scale equivariance, therefore
 we expect different reactions to these changes by deep convolutional latent
 variable models in terms of disentanglement.
 The other two cases, mean shift and magnitude shift, represent disturbances
 in the conditional distribution 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)$
\end_inset

.
 The training, validation, and testing IC or OC samples are generated of
 size 500 each.
\end_layout

\begin_layout Subsection
On the disentanglement and extrapolation performance of the encoder
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:recognition"

\end_inset

 To investigate the disentanglement and extrapolation performance of the
 recognition network we employ the following procedure.
 First, we train a VAE with 2-dimensional latent code (for easier visualization)
 to convergence using in-control samples as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Then, we a set of profiles with varying center location 
\begin_inset Formula $c_{0}$
\end_inset

 and radius 
\begin_inset Formula $r$
\end_inset

 into the recognition network to obtain their respective proposal distributions.
 The points are picked inside and outside the tolerance region of the two
 quality characteristics to be compared against their mapping onto the represent
ation space.
 Finally, we sample 150 points from the proposals and plot them on the represent
ation space.
 The results are shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There are two important observations we would like to point out:
\end_layout

\begin_layout Itemize

\series bold
Representations are partially entangled
\series default
.
 For every fixed center location, the direction of the axis on which radius
 change differs.
 The direction would have been independent from the fixed value of center
 location should the representations were disentangled.
 We may claim partial disentanglement for location but that would be quite
 unlikely if it wasn't for the translational equivariance injected in convolutio
nal layers 
\begin_inset CommandInset citation
LatexCommand citep
after "Thm. 1"
key "locatello2018challenging"
literal "false"

\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Encoder cannot extrapolate for extremely values of radius.
 
\series default
Observe how from point 7 to 1, from point 6 to 0 and from point 8 to 2 the
 proposal means are converging to a point in the in-control region.
 This signals that the representations will not be able to extrapolate for
 even smaller values of radii.
\end_layout

\begin_layout Standard
Given these two observations, we can conclude right away that extremely
 small gaskets (in terms of radius) will likely be gone undetected.
 On the other hand, samples coming from some tolerable radius rangesâ€”such
 as point 7â€” will be falsely identified as out-of-control.
 Observe for that point, how most sample latent codes fall outside the tolerable
 region in the inferred space.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/proposals_gray.pdf
	width 100line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Figure depicting the behavior of the recongition network of a VAE trained
 with in-control gasket sampled.
 
\series bold
Left: 
\series default
Nine gasket profiles whose latent variables are picked from in and out of
 the tolerable region of radius and center location.
 The tolerable region is represented by the gray dashed circle, a curve
 of isodistant points in terms of Mahalanobis distance to in control distributio
n.
 
\series bold
Right:
\series default
 150 latent codes sampled from each proposal (associated with the same id
 numbers on the top).
 Isodistant curve for standard Gaussian that is probability-wise equivalent
 to the one on the left is depicted as a gray dashed circle.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:proposals"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Overall, our are in agreement with our rationale behind the proposed statistic
 outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In other words, without extrapolation and disentanglement, we do not expect
 any monitoring statistic based purely on the output of the recognition
 networkâ€”such as 
\begin_inset Formula $H^{2}$
\end_inset

, 
\begin_inset Formula $T^{2}$
\end_inset

 or 
\begin_inset Formula $D$
\end_inset

 discussed in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

â€” to be robust enough to be safely employed in a process control mission.
\end_layout

\begin_layout Standard
We also want to make a remark that we tried the beta-VAE framework explained
 in 
\begin_inset CommandInset citation
LatexCommand citep
key "higgins2017beta"
literal "false"

\end_inset

 with 
\series bold

\begin_inset Formula $\beta=4$
\end_inset


\series default
 to remedy the problem of entanglement but obtained similar results.
\end_layout

\begin_layout Subsection
On the extrapolation performance of the decoder network
\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:generator"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:manifold_vae"
plural "false"
caps "false"
noprefix "false"

\end_inset

 hints us about the extrapolation performance of the decoder of the same
 VAE described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

 trained on in-control samples described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 It should be cross-examined with 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as the encoder and decoder are tightly coupled to each other.
 We observe two important behavior: the posterior gets distorted beyond
 two or three standard deviations and the representations are partially
 entangled in line with the behavior of its encoder depicted in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To see how this will help detecting disturbances in latent space, consider
 a gasket that is extremely large or at the very margins of the grid.
 Since the decoder cannot generate such a sample, there will be large pixel-to-p
ixel differences.
 This will in turn produce a larger monitoring statistic that will likely
 to fall outside the control limit.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/manifold_vae.pdf
	width 90line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Latent space traversal and the response of the decoder of a VAE with 2-dimension
al latent codes and trained with in-control gasket samples.
 Each row represents which latent dimension is traversed while the other
 dimension is fixed at zero.
 Each column represents what value is assigned to that latent dimension
 that is represented by the row label.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:manifold_vae"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
On the estimation of log-likelihood under importance sampling
\end_layout

\begin_layout Standard
Earlier, we claimed that it would take too many Monte Carlo sampling iterations
 to get a meaningful estimate of 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

.
 In this section we test that claim on a random sample 
\begin_inset Formula $\mbx$
\end_inset

 using the proposal distribution 
\begin_inset Formula $\mbz\sim\encoding$
\end_inset

 which is obtained via the encoder of the same VAE model we have been using
 in this section.
 See 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Estimation-comparison-between"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for the results.
 The key observation is that it takes at least a few tens of Monte Carlo
 iterations to get a stable and accurate estimation.
 At that level, the single pass through encoder is negligible.
 This means using sampling will be more costly at least a factor of few
 tens to achieve the same accuracy as first-order approximation that we
 suggest.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/mc_vs_foa.pdf
	lyxscale 90
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimation comparison between Monte Carlo sampling (black line) and first
 order approximation (black dashed line).
 95% confidence interval band is shown in gray band and is based on simulations
 with ten different seeds.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Estimation-comparison-between"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison of detection performance of proposed statistics
\end_layout

\begin_layout Standard
We now compare the proposed statistics based on how accurately they detect
 profiles from out-of-control processes outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that for all statistics that require sampling, we obtain a single
 sample and calculate the statistic based on that.
 This way we keep the computational demand same for all statistics and emulate
 the computational constraints of a real-life case.
 A preliminary result we must check is the robustness of the statistics
 by making sure all proposed statistics have roughly similar false alarm
 rate on the held-out in-control test set, which should also be reasonably
 close to the desired rate 5%.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:far"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates that this is the case for all of them.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
False alarm rates on held-out dataset averaged over 10 replications per
 model and monitoring statistic.
 Standard deviations are in parentheses.
\begin_inset CommandInset label
LatexCommand label
name "tab:far"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="6">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Statistic 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SPE/R 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
D 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
H2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
T2 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.041(0.006) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.051(0.005) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.044(0.004) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.052(0.005) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.043(0.009) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Through 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we observe a clear superiority of 
\begin_inset Formula $ERE$
\end_inset

 over other methods when the disturbance is on the observable space (top
 row).
 Latent variable-based statistics 
\begin_inset Formula $D$
\end_inset

, 
\begin_inset Formula $H^{2}$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

 fail in this case since that they are purely computed using the proposal
 distribution latent variables.
 
\begin_inset Formula $ERE$
\end_inset

 also outperforms 
\begin_inset Formula $SPE/R$
\end_inset

, although by a smaller margin it has with the latent variable-statistics.
\end_layout

\begin_layout Standard
For the latter two disturbances occurring purely on latent dimensions, results
 are presented in the bottom row of 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The key observations can be listed as follows:
\end_layout

\begin_layout Itemize
We observe mixed results but generally 
\begin_inset Formula $ERE$
\end_inset

, 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $H^{2}$
\end_inset

 tend to perform better than 
\begin_inset Formula $SPE/R$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

.
 A commonality between the former three is that they don't rely on random
 samples, supporting our argument against this practice.
\end_layout

\begin_layout Itemize
Observe the radius shift type disturbance show in bottom left.
 Even though 
\begin_inset Formula $H^{2}$
\end_inset

 performs better on positive intensities (larger radii), it completely misses
 negative intensities (smaller radii).
 We foresaw this result in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To reiterate, disentanglement and failure to extrapolate in encoder is
 the reason behind this.
 Observe that this result extend to all the latent-variable based statistics.
\end_layout

\begin_layout Itemize
Unlike latent variable-based statistics, 
\begin_inset Formula $ERE$
\end_inset

 and 
\begin_inset Formula $SPE/R$
\end_inset

 behave more robustly against varying intensities.
 Among the two, we observe that 
\begin_inset Formula $ERE$
\end_inset

 consistently outperforms 
\begin_inset Formula $SPE/R$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/disturbance_on_pxz_vae_only.pdf
	lyxscale 70
	width 100line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Fault detection rates (y-axis) for varying intensities (x-axis) of different
 disturbance types (quadrants).
 Bands represent 95% confidence interval estimated around mean detection
 rates.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:disturbance_on_pxz"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that in a real-life process, disturbances purely on either observable
 or latent space would be very unlikely.
 This is why overall, we conclude that 
\begin_inset Formula $ERE$
\end_inset

 is the best statistic among all when a combination of accuracy, robustness
 and computational demand is considered.
\end_layout

\begin_layout Section
Case Study Analysis & Results
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:case-study"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% TODO (@DS): how many anomaly samples do we have?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% TODO (@DS): Add a figure to illustrate both normal and abnormal samples,
 with one image in each class
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our dataset consists of center-cropped image profiles from a hot-steel rolling
 process, which is shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There are 13 classes of surface types identified by the domain engineers.
 Four of these classesâ€”0,1,9 and 11â€” are considered as in-control.
 There are in total 338 images in these classes.
 The other nine classes make up the out of control cases and they have in
 combination 3351 images to report detection accuracy for.
 We randomly partition the IC corpus to fix train, validate and test sets
 with 60%-20%-20% relative sizes respectively.
 The rest of the procedure followed is outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Same as in simulation study, to account for randomness in weight initialization
, we replicate the experiment with 10 different seeds.
 The results are summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 They show strict similarity with the results in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Formula $SPE$
\end_inset

 dominates for each of the candidate DLVM and 
\begin_inset Formula $R$
\end_inset

 is the runner-up while statistics based on encoders suffer quite low and
 often none detection performance.
\end_layout

\begin_layout Standard
For completeness, we also include a comparison of our proposed statistic
 against the baseline method PCA where 90% of the variance is retained.
\end_layout

\begin_layout Standard
To support our claim of ineffectiveness of the statistics based on the latent
 space such as 
\begin_inset Formula $H^{2}$
\end_inset

 statistics, we refer the reader to 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:T2vsQ"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:T2vsQ-a"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the 2-D latent code of the AAE model given a set of OC samples.
 Since the 2-D latent code of OC samples are also close to the standard
 normal distribution, 
\begin_inset Formula $H^{2}$
\end_inset

 statistics will not be able to detect these OC behaviors.
 The ineffectiveness of 
\begin_inset Formula $H^{2}$
\end_inset

 statistics is validated in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:T2vsQ-b"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where the density of 
\begin_inset Formula $H^{2}$
\end_inset

 distribution for OC and IC samples are largely overlapped.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:T2vsQ-c"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the reconstruction error of the images given the same 2-D latent
 code.
 From these images, it is clear that the reconstructed images are very different
 from the OC images, where 
\begin_inset Formula $SPE$
\end_inset

 can be used to capture such changes.
 The effectiveness of 
\begin_inset Formula $SPE$
\end_inset

 is also shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:T2vsQ-d"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where the distributions of IC and OC samples do not have much overlap
 at all.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Summary of fault detection accuracies on out-of-control cases averaged over
 10 replications per model and monitoring statistic.
 Standard deviations are in parentheses.
 Bolded values represent the maximum within each model, across different
 statistics.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:rolling_results"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="12" columns="6">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Statistic 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SPE 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
R 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
D 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
H2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
T2 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fault ID 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.44
\series default
(0.06) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.37(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.85
\series default
(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.84(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.17(0.06) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.23(0.04) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.03(0.03) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.75
\series default
(0.05) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.62(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.58(0.07) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.62(0.09) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.79(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15(0.08) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.05(0.05) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.17
\series default
(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.13(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.70
\series default
(0.07) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.64(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.57
\series default
(0.05) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.49(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.79(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.77
\series default
(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.71(0.04) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.00) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
DLVM detection rate performance on hot steel rolling faulty cases compared
 against PCA with 90% variance retained and 
\begin_inset Formula $Q$
\end_inset

 statistic is used.
 Best performers bolded on each row.
 Standard deviation over 10 replication is in .
\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="12" columns="4">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
PCA 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
AAE 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
VAE 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fault ID 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.33(0.19) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.44
\series default
(0.06) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.78(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.84(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.85
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.56(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.61(0.09) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.75
\series default
(0.05) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.99(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.99(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.52(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.73(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.17
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.34(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.51(0.11) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.70
\series default
(0.07) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.29(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.54(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.57
\series default
(0.05) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.69(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.82
\series default
(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.80(0.02) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.56(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.76(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.77
\series default
(0.02) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "tab:rolling:vsPCA"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:conclusions"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "./bibliography"

\end_inset


\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%dummy comment inserted by tex2lyx to ensure that this paragraph is not
 empty
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
refalias
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

section
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Proof of 
\begin_inset CommandInset ref
LatexCommand ref
reference "prop: T2Q"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:PoofOfPropTQ"

\end_inset

 Kullback-Leibler divergence between two multivariate Gaussian distributions
 has a closed form solution.
 If we define these distributions as 
\begin_inset Formula $p_{0}=N(\mbz;\mbmu_{0},\mbSigma_{0})$
\end_inset

 and 
\begin_inset Formula $p_{1}=N(\mbz;\mbmu_{1},\mbSigma_{1})$
\end_inset

 where 
\begin_inset Formula $\mbmu$
\end_inset

 and 
\begin_inset Formula $\mbSigma$
\end_inset

 are respective mean vectors and covariance matrices, then according to
 
\begin_inset CommandInset citation
LatexCommand citep
key "hershey2007approximating"
literal "false"

\end_inset

 the closed form solution will be the following: 
\begin_inset Formula 
\begin{align*}
\KL{p_{0}}{p_{1}} & =\frac{1}{2}[\log\frac{\g\mbSigma_{1}\g}{\g\mbSigma_{0}\g}+Tr(\mbSigma_{1}\inv\mbSigma_{0})-r\\
 & +(\mu_{0}-\mu{1})^{\top}\mbSigma_{1}\inv(\mu_{0}-\mu{1})]
\end{align*}

\end_inset

Since 
\begin_inset Formula $\encoding=\Norm(\mu(\mbx),\mbSigma_{z})$
\end_inset

 and 
\begin_inset Formula $p(\mbz)=\Norm(0,\mbI)$
\end_inset

, we can derive that 
\begin_inset Formula 
\begin{align*}
\KL{\encoding}{p(\mbz)} & =\frac{1}{2}\left[-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r\right]+\frac{1}{2}\mu(\mbx)^{\top}\mu(\mbx)\\
 & =\frac{1}{2}\mu(\mbx)^{\top}\mu(\mbx)+C,
\end{align*}

\end_inset

where 
\begin_inset Formula $C=-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r$
\end_inset

 is a constant, which doesn't depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Standard
To derive the SPE statistics, we will derive
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2}\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbx^{\top}\mbx-2\mbz^{\top}\mbW\mbx+\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mbx^{\top}\mbx-2\mu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\label{eq: spew}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Here, we know that 
\begin_inset Formula 
\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}tr(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & tr\left(\mbW^{\top}\mbW\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz\mbz^{\top})\right)\nonumber \\
= & tr\left(\mbW^{\top}\mbW(\mu(\mbx)\mu(\mbx)^{\top}+\Sigma_{z})\right)\nonumber \\
= & \mu(\mbx)^{\top}\mbW^{\top}\mbW\mu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\label{eq: tracezwwz}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Therefore, by plugging 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: tracezwwz"
plural "false"
caps "false"
noprefix "false"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: spew"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2} & =\mbx^{\top}\mbx-2\mu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\\
 & =\mbx^{\top}\mbx-2\mu(\mbx)^{\top}\mbW\mbx+\mu(\mbx)^{\top}\mbW^{\top}\mbW\mu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\\
 & =\|\mbx-\mbW\mu(\mbx)\|^{2}+C\\
\end{align*}

\end_inset

where 
\begin_inset Formula $C=tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)$
\end_inset

 that does not depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Section
A Toy Example to Demonstrate Out-of-distribution Behavior of Neural Networks
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "app:rosenbrock"

\end_inset


\end_layout

\begin_layout Section
Derived Testing Statistics for SPE
\end_layout

\begin_layout Standard
Here, we define 
\begin_inset Formula $R(z)=\|y-g(z)\|^{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
E_{\mbz\sim q_{\theta}}R(\mbz) & =R(\mu_{z})+R'(\mbz)E[(\mbz-\mu(\mbx))]+R''(\mbz)\frac{1}{2}E[(\mbz-\mu(\mbx))^{\top}H_{z}(\mbz-\mu(\mbx))]\\
 & =R(\mu_{z})+R''(\mbz)\frac{1}{2}E[(\mbz-\mu(\mbx))^{\top}H_{z}(\mbz-\mu(\mbx))]\\
 & =R(\mu_{z})+\frac{1}{2}tr(H_{z}E[(\mbz-\mu_{z})(\mbz-\mu_{z})^{T}])\\
 & =R(\mu_{z})+\frac{1}{2}tr(H_{z}\Sigma_{z})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $\Sigma_{z}$
\end_inset

 is a diagonal matrix, 
\begin_inset Formula $tr(H_{z}S_{z})=tr(diag(H_{z})S_{z})=\sum_{i}(H_{z})_{ii}(S_{z})_{ii}$
\end_inset

, and only diagonal element of 
\begin_inset Formula $H_{z}$
\end_inset

 is needed, so it can be computed efficiently.
\end_layout

\end_body
\end_document
