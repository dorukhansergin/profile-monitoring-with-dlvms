#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrartcl
\begin_preamble



\graphicspath{{./figs/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.eps}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage{cleveref}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{stfloats}
\usepackage[super]{nth}
\usepackage{subcaption}
\usepackage{caption}
% for inkscape images
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgf}



\g@addto@macro\@floatboxreset\centering


% \usepackage{hyperref}
 
 \AtBeginDocument{% Overrides ref for Cref
 	\let\ref\Cref
 }

\crefalias{prop}{proposition}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
theorems-sec
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plainnat
\biblatex_bibstyle authoryear
\biblatex_citestyle authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author -1806609307 "Hao yan"
\author 2089042059 "Dora"
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% INPUT PREAMBLES
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset CommandInset include
LatexCommand include
filename "preamble/preamble_glossary.lyx"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:introduction"

\end_inset

 Profile monitoring has attracted a growing interest in the literature in
 the past decades 
\begin_inset CommandInset citation
LatexCommand citep
key "Woodall2004-bp,Woodall2007-xs,Maleki2018-uo"
literal "false"

\end_inset

 for its ability to construct control charts with much better representations
 for certain types of process measurements.
 A profile can be defined as a functional relationship between the response
 variables and explanatory variables or spatiotemporal coordinates.
 In this work, we focus on the case where the profiles generated from the
 process are high-dimensional (HD)â€”
\shape italic
i.e.
\shape default
, the number of such explanatory variables or spatiotemporal coordinates
 are large.
 Specifically, we focus on sets of HD profiles for which intra-sample variation
 lies on a nonlinear low-dimensional manifold 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
 Our motivating example of such HD profiles is presented in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in which we exhibit a sample of surface defect image profiles collected
 from a hot steel rolling process.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/profile_examples.pdf
	width 25theight%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
A collection of 64 by 64 image profiles taken from a hot steel rolling process.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Rolling"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In literature, profile monitoring techniques can be categorized by their
 assumptions on the type of 
\change_deleted 2089042059 1589427970
the 
\change_unchanged
functional relationship
\change_deleted 2089042059 1589427989
 that they assume
\change_unchanged
.
 Linear profile monitoring can be considered the most basic profile monitoring
 technique, in which it is assumed that the profile can be represented by
 a linear function.
 The idea is to extract the slope and the intercept from each profile and
 monitor its coefficients 
\begin_inset CommandInset citation
LatexCommand citep
key "zhu2009monitoring"
literal "false"

\end_inset

.
 Regularization techniques can also be used in linear profile estimation.
 For example, Zou 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
etal
\end_layout

\end_inset

 utilize a multivariate linear regression model for profiles with the LASSO
 penalty and use the regression coefficients for Phase-II monitoring 
\begin_inset CommandInset citation
LatexCommand citep
key "zou2012lasso"
literal "false"

\end_inset

.
 However, the assumption of 
\change_inserted 2089042059 1589428022
a 
\change_unchanged
linear functional relationship can be quite limiting.
 To address this challenge, nonlinear parametric models are proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "Williams2007-ty,Jensen2009-tu,Noorossana2011-oj,Maleki2018-uo"
literal "false"

\end_inset

.
 These models assume an explicit family of parameterized functions and,
 their parameter 
\change_deleted 2089042059 1589428132
estimations
\change_unchanged
 are estimated via nonlinear regression.
 In 
\change_deleted 2089042059 1589428141
any
\change_inserted 2089042059 1589428141
both
\change_unchanged
 cases, the drawback of 
\change_deleted 2089042059 1589428148
all
\change_inserted 2089042059 1589428151
both linear and nonlinear
\change_unchanged
 parametric models is that 
\change_deleted 2089042059 1589428161
these models 
\change_inserted 2089042059 1589428162
they 
\change_unchanged
assume the parametric form is known beforehand, which might not always be
 the case.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% Literature: non-parametric Methods for certain types of profiles
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Another large body of profile monitoring research focuses on the type of
 profile
\change_inserted 2089042059 1589428317
s
\change_deleted 2089042059 1589428318
 data
\change_unchanged
 where the basis of the representation is assumed to be known but the coefficien
ts are unknown.
 For instance, to monitor smooth profiles, various non-parametric methods
 based on local kernel regression 
\begin_inset CommandInset citation
LatexCommand citep
key "zou2008monitoring,qiu2010nonparametric,zou2009nonparametric"
literal "false"

\end_inset

 and splines 
\begin_inset CommandInset citation
LatexCommand citep
key "chang2010statistical"
literal "false"

\end_inset

 are developed.
 To monitor the non-smooth wave-form signals, a wavelet-based mixed effect
 model is proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "paynabar2011characterization"
literal "false"

\end_inset

.
 However, for all the aforementioned methods, it is assumed that the nonlinear
 variation pattern of the profile is well captured by a known basis or kernel.
 Usually, there is no guidance on selecting the right basis of the representatio
n for the original data and it normally requires many trial and error.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%Here, we focus on the HD profiles where we cannot assume a parametric function
 form and the basis representation is unknown.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

In the case that the basis of HD profiles are not known, dimensionality
 reduction techniques are widely used.
 Principal component analysis (PCA) is arguably the most popular method
 in this context for profile data monitoring because of its simplicity,
 scalability, and good data compression capability.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "liu1995control"
literal "false"

\end_inset

, PCA is proposed to reduce the dimensionality of the streaming data and,
 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 charts are constructed to monitor the extracted representations and residuals,
 respectively.
 To generalize PCA methods to monitor the complex correlation among the
 channels of multi-channel profiles, 
\begin_inset CommandInset citation
LatexCommand citet
key "paynabar2015change"
literal "false"

\end_inset

 propose a multivariate functional PCA method and apply change point detection
 methods on the function coefficients.
 Along this line, tensor-based PCA methods are also proposed for multi-channel
 profiles, examples including uncorrelated multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
key "paynabar2013monitoring"
literal "false"

\end_inset

 and multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
key "grasso2014profile"
literal "false"

\end_inset

.
 Finally, various tensor-based PCA methods 
\begin_inset CommandInset citation
LatexCommand citep
key "yan2015image"
literal "false"

\end_inset

 are compared and different test statistics are developed for tensor-based
 process monitoring.
\end_layout

\begin_layout Standard
The main limitation of all the aforementioned PCA-related methods is that
 the expressive power of linear transformations is very limited.
 Furthermore, each principal component represents a global variation pattern
 of the original profiles, which is not efficient at capturing the local
 spatial correlation within a single profile.
 Therefore, PCA requires a much larger latent space dimensions than the
 dimension of the actual latent space, yielding a sub-optimal and overfitting-pr
one representation.
 This phenomena hinders the profile monitoring performance.
\end_layout

\begin_layout Standard
A systematic discussion of this issue is articulated in 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
 In that work, the authors identify the problems associated with assuming
 a closeness relationship in the subspace that is characterized by Euclidean
 metrics.
 They successfully observe that the intra-sample variation in complex high-dimen
sional corpora may lie on a nonlinear manifold as opposed to a linear manifold
 which is assumed by PCA and related methods.
 However, the authors only focus on applying manifold learning for Phase-I
 analysis, while Phase-II monitoring procedure is not touched upon 
\begin_inset CommandInset citation
LatexCommand citep
key "Shi2016-tg"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Deep dimensionality reduction models have been proposed as an alternative
 to classical dimensionality reduction techniques in a handful.
 Deep autoencoders have been proposed for profile monitoring for Phase-I
 analysis in 
\begin_inset CommandInset citation
LatexCommand citet
key "Howard2018-op"
literal "false"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Yan2016-wa"
literal "false"

\end_inset

 compared the performance of contractive autoencoders and denoising autoencoders
 for Phase-II monitoring.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang2018-js"
literal "false"

\end_inset

 proposed a denoising autoencoder for process monitoring.
 Aside from deterministic deep neural networks, only three works 
\begin_inset CommandInset citation
LatexCommand citep
key "wang2019systematic,Zhang2019-lu,lee2019process"
literal "false"

\end_inset

 proposed to use deep probabilistic latent variable models, specifically,
 variational autoencoders (VAE), for Phase-II monitoring.
 All of the proposed monitoring statistics in those works differ slightly
 but 
\change_deleted -1806609307 1589499705
conceptually
\change_unchanged
 they are all extensions of the classic 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

-charts of PCA
\change_inserted -1806609307 1589499720
, which will be proved rigorously in our paper
\change_unchanged
.
 We argue that there is room for improvement for the monitoring statistic
 formulations in those works for a number of reasons, especially when high-dimen
sional profiles are considered.
 In this work, we propose a new monitoring statistic formulation to address
 this issue.
 
\change_inserted -1806609307 1589479177

\end_layout

\begin_layout Standard
The contributions of this work are as follows: 
\change_inserted -1806609307 1589479308

\end_layout

\begin_layout Itemize

\change_deleted -1806609307 1589479317
w
\change_inserted -1806609307 1589479317
W
\change_unchanged
e compare all the existing monitoring statistics for deep latent variable
 models with the focus on 
\change_inserted -1806609307 1589499746
the 
\change_unchanged
VAE model and propose a better monitoring statistic that responds robustly
 and
\change_inserted -1806609307 1589499759
,
\change_unchanged
 in most cases
\change_inserted -1806609307 1589499770
,
\change_unchanged
 more accurately than previous formulations requiring only a single pass
 through the autoencoder.
 
\change_inserted -1806609307 1589479315

\end_layout

\begin_layout Itemize
Through extensive experiments, we demonstrate and give insight on
\change_deleted -1806609307 1589479274
 1)
\change_unchanged
 why latent variable-based monitoring statistics shou
\change_deleted -1806609307 1589479338
l
\change_unchanged
d not be used due to the two issues related to deep encoders: 
\change_inserted -1806609307 1589479343
1) 
\change_unchanged
their representations are likely entangled 
\change_deleted -1806609307 1589479347
and 
\change_inserted -1806609307 1589479349
2) 
\change_unchanged
they will likely fail to extrapolate well beyond the region of in-control
 profiles.
 
\change_deleted -1806609307 1589479281
2
\change_unchanged
) 
\change_inserted -1806609307 1589479336

\end_layout

\begin_layout Itemize
We also demonstrate why 
\change_deleted -1806609307 1589523243
observable variable
\change_inserted -1806609307 1589523243
residual
\change_unchanged
-based monitoring statistics should not be estimated via sampling due to
 prohibitively expensive computational demand
\change_deleted -1806609307 1589479358
.

\change_inserted -1806609307 1589479358
 and give a mathematical proof that latent variable-based monitoring statistics
 and residual-based statistics will become classic 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

-charts for the linear case.
 
\change_unchanged

\end_layout

\begin_layout Itemize
We demonstrate the effectiveness of this new formulation on both simulation
 and real-life case studies.
 
\change_deleted -1806609307 1589479369
We conclude that 
\change_inserted -1806609307 1589479382

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589479388
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589499978
In conclusion, the rest of the paper is organized as follows.

\color red
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Background"
plural "false"
caps "false"
noprefix "false"

\end_inset

 first reviews the related literature in the variational autoencoders and
 traditional 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 chart in the PCA method and then briefly introduce the existing monitoring
 statistics for VAE.

\color inherit
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology"
plural "false"
caps "false"
noprefix "false"

\end_inset

 introduces the proposed monitoring statistics and gives insights on its
 meaning and mathematical relationship with the traditional PCA methods.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Simulation-Study-Analysis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the simulation study to demonstrate the performance of the proposed
 method and give insight into why only the residual-based statistics is
 recommended.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:case-study"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates the advantage of the proposed methodology using images from
 hot-steel rolling processes.
 Finally, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:conclusions"
plural "false"
caps "false"
noprefix "false"

\end_inset

 concludes the paper with future work.
\end_layout

\begin_layout Section
Background
\change_inserted -1806609307 1589491547
 
\begin_inset CommandInset label
LatexCommand label
name "sec:Background"

\end_inset


\change_unchanged

\end_layout

\begin_layout Subsection
Variational Autoencoders
\change_inserted -1806609307 1589439437
 
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:lvms"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
In this section, we introduce Variational Autoencoder (VAE) 
\begin_inset CommandInset citation
LatexCommand citep
key "Kingma2013-dl"
literal "false"

\end_inset


\change_inserted -1806609307 1589500164
,
\change_unchanged
 which is the primary modeling tool in this work.
 
\change_deleted -1806609307 1589500249
In order to
\change_inserted -1806609307 1589500249
To
\change_unchanged
 
\change_deleted -1806609307 1589439053
do
\change_inserted -1806609307 1589439056
achieve
\change_unchanged
 that, we believe a brief introduction to Gaussian factorized latent variable
 models is necessary.
\end_layout

\begin_layout Standard

\change_deleted -1806609307 1589439436
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:lvms"

\end_inset


\change_unchanged
 Latent variables models are powerful tools to model complex distributions
 over high-dimensional spaces.
 The underlying assumption is that there exists a low-dimensional latent
 structure that explains well the variations in the high-dimensional observed
 space.
 Typically, the density over observed variables can be decoupled into the
 distribution on the latent variables and the conditional distribution of
 observed variables given latent variables can be assigned as tractable
 families of distributions, which will be much more efficient than modeling
 the data distribution directly.
\end_layout

\begin_layout Standard
A typical example of latent variable models is when the joint distribution
 is Gaussian factorized as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Formula 
\begin{equation}
\begin{split}\pz & =\Norm(\mbz;0,\mbI_{r})\\
\decoding & =\Norm(\mbx;\mu_{\mbtheta}(\mbz),\sigma^{2}\mbI_{d})\\
p_{\mbtheta}(\mbx,\mbz) & =\decoding\pz
\end{split}
\label{eq:gaussian-factorized}
\end{equation}

\end_inset

In the above formulation
\change_inserted -1806609307 1589500274
,
\change_unchanged
 
\begin_inset Formula $\mbx\in\R^{d}$
\end_inset

 are observed samples,
\change_inserted -1806609307 1589500300
 and
\change_unchanged
 
\begin_inset Formula $\mbz\in\R^{r}$
\end_inset

 are latent variables while 
\begin_inset Formula $\mu_{\mbtheta}\colon\R^{r}\to\R^{d}$
\end_inset

 is a function parameterized by 
\begin_inset Formula $\mbtheta\in\Theta$
\end_inset

, 
\change_deleted -1806609307 1589500316
that
\change_inserted -1806609307 1589500316
which
\change_unchanged
 describes the relationship between the latent variables and the mean of
 the conditional distribution.
 The Gaussian prior 
\begin_inset Formula $\pz$
\end_inset

 is typically chosen to be 
\change_deleted -1806609307 1589500378
standard
\change_inserted -1806609307 1589500378
standard multivariate normal distribution
\change_unchanged
 to avoid degenerate solutions 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 307"
key "roweis1999unifying"
literal "false"

\end_inset

 and conditional covariance is typically assumed to be isotropic 
\begin_inset Formula $\sigma^{2}I_{d}$
\end_inset

 to avoid ill-defined problems.
 The aim is to approximate the true density 
\begin_inset Formula $p_{\mbtheta}(\mbx)\approx p(\mbx)$
\end_inset

 and this approximation can be obtained through marginalization: 
\begin_inset Formula 
\[
p_{\mbtheta}(\mbx)=\int p_{\mbtheta}(\mbx,\mbz)d\mbz
\]

\end_inset

Finally, in literature, there 
\change_deleted -1806609307 1589440386
has
\change_inserted -1806609307 1589440387
have
\change_unchanged
 been discussions about whether the independent latent structure assumption
 
\begin_inset Formula $\pz=\Norm(\mbz;0,\mbI_{r})$
\end_inset

 can lead to the discovery of the true disentangled variations, a task also
 known as disentangled representation learning 
\begin_inset CommandInset citation
LatexCommand citep
after "Sec. 3.5"
key "bengio2013representation"
literal "false"

\end_inset

.
 Disentangled representations are useful to represent variations in latent
 variations due to 
\change_deleted -1806609307 1589501038
its
\change_inserted -1806609307 1589501038
their
\change_unchanged
 ability to separate out the independent factors.
 
\change_inserted -1806609307 1589501060
However, there are also discussions on what disentangled representation
 implies for profile monitoring.
 
\change_unchanged
We are interested in whether and if so, how
\change_deleted -1806609307 1589501154
,
\change_unchanged
 such representations will be critical for profile monitoring.
\end_layout

\begin_layout Standard
A famous member of the family of models described above is the probabilistic
 principal component analysis (PPCA) 
\begin_inset CommandInset citation
LatexCommand citep
key "tipping1999probabilistic"
literal "false"

\end_inset

.
 The parameters are optimized via a maximum likelihood estimation framework
 and it can be solved analytically due to the fact that 
\begin_inset Formula $\mu_{\mbtheta}$
\end_inset

 is a simple linear transformation enabling the optimization to reuse results
 from original solutions to the PCA problem.

\change_inserted -1806609307 1589440464
 
\change_deleted -1806609307 1589440464

\end_layout

\begin_layout Standard
The assumption of PPCA that the latent and observed variables have a strictly
 linear relationship can be quite restricting.
 In real-world processes, it is likely that this relationship is highly
 nonlinear.
 Deep latent variable models are a marriage of deep neural networks and
 latent variable models that aim to solve this problem.
 Deep learning has enjoyed a tremendous resurgence in the last decade due
 to their superior performance that was unprecedented for many tasks such
 as image classification 
\begin_inset CommandInset citation
LatexCommand citep
key "krizhevsky2012imagenet"
literal "false"

\end_inset

, machine translation 
\begin_inset CommandInset citation
LatexCommand citep
key "bahdanau2014neural"
literal "false"

\end_inset

, and speech recognition 
\begin_inset CommandInset citation
LatexCommand citep
key "amodei2016deep"
literal "false"

\end_inset

.
 In theory, under sufficient conditions, a two
\change_deleted -1806609307 1589440484
 
\change_inserted -1806609307 1589440484
-
\change_unchanged
layer multilayer perceptron can approximate any function on a bounded region
 
\begin_inset CommandInset citation
LatexCommand citep
key "cybenko1989approximation,Hornik1991-li"
literal "false"

\end_inset

.
 Growing the width of shallow networks in an exponential fashion for arbitrarily
 complex tasks is not practical.
 It has been shown that deeper representations can often achieve 
\change_deleted -1806609307 1589440495
the
\change_unchanged
 better expressive power than shallow networks with 
\change_deleted -1806609307 1589440502
less
\change_inserted -1806609307 1589440503
fewer
\change_unchanged
 parameters due to the efficient reuse of the previous layers 
\begin_inset CommandInset citation
LatexCommand citep
key "eldan2016power"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
VAE is arguably the most foundational member of the deep latent variable
 model family.
 The main difference between PPCA and VAE is that 
\change_deleted -1806609307 1589441282
the latter
\change_inserted -1806609307 1589441285
VAE
\change_unchanged
 replace
\change_inserted -1806609307 1589441635
s
\change_unchanged
 the linear transformation with a high-capacity deep neural network (called
 
\shape italic
generative
\shape default
 or 
\shape italic
decoder
\shape default
).
 This is powerful in the sense that
\change_inserted -1806609307 1589441703
,
\change_unchanged
 along with a general
\change_deleted -1806609307 1589501186
 
\change_inserted -1806609307 1589501186
-
\change_unchanged
purpose prior 
\begin_inset Formula $\pz$
\end_inset

 
\change_inserted -1806609307 1589501286
(i.e., often set as independent normal distributions), deep neural networks
 can transfer such prior into 
\change_unchanged
a wide variety of densities 
\change_deleted -1806609307 1589441496
can be modeled
\change_unchanged
 
\begin_inset CommandInset citation
LatexCommand citep
key "kingma2019introduction"
literal "false"

\end_inset

.
 Unlike PPCA, these models will not have analytical solutions due to the
 complex nature of the neural network used.
 Like most other deep learning models, their parameters have to be optimized
 via gradient descent for maximum likelihood.
 The problem becomes even harder given the observation that the posterior
 
\begin_inset Formula $\decoding$
\end_inset

 takes meaningful values only for a small sub-region within 
\begin_inset Formula $\R^{r}$
\end_inset

.
 This makes sampling from the prior 
\begin_inset Formula $\pz$
\end_inset

 to estimate the likelihood prohibitively expensive.
 Both models work around this problem using the importance sampling framework
 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 532"
key "bishop2006pattern"
literal "false"

\end_inset

, where they introduce another network (called 
\shape italic
recognition
\shape default
 or 
\shape italic
encoder
\shape default
) to approximate a proposal distribution 
\begin_inset Formula $\encoding$
\end_inset

 â€”parametrized by 
\begin_inset Formula $\mbphi$
\end_inset

â€” which will hopefully sample latent variables from a much smaller region
 that is more likely to produce higher posterior densities for a given input
 
\begin_inset Formula $\mbx$
\end_inset

.

\change_inserted -1806609307 1589507484
 In fact, PPCA can be treated as a special case of VAE, where the decoder
 is modeled by linear transformation.
 
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted -1806609307 1589440560
The ultimate
\change_inserted -1806609307 1589440563
One important
\change_unchanged
 output of a trained VAE is the likelihood estimator.
 Once the two networks are trained, the log-likelihood 
\begin_inset Formula $\log\ptheta(\mbx)$
\end_inset

 can be approximated by a Monte Carlo sampling procedure with 
\begin_inset Formula $L$
\end_inset

 iterations 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 30"
key "kingma2019introduction"
literal "false"

\end_inset

: 
\begin_inset Formula 
\begin{equation}
\log\ptheta(\mbx)\approx\log\frac{1}{L}\sum_{l=1}^{L}\frac{\ptheta(\mbx,\mbz^{(l)})}{\qphizgivenx{\mbz^{(l)}}{\mbx}}\label{eqn:SummationLL}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
VAEs are trained to maximize the so-called evidence lower bound (ELBO),
 which is deemed a proxy to the likelihood: 
\begin_inset Formula 
\begin{equation}
\begin{split}\text{ELBO} & \triangleq\log\left(p(\mbx)\right)-\KL{\encoding}{q^{*}(\mbz|\mbx)}\\
 & =\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)}
\end{split}
\label{eqn:VAELoss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\KL{\cdot}{\cdot}$
\end_inset

 denotes the Kullback-Leibler divergence (KLD) between two distributions.
 The left-hand side is the quantity of interest
\change_inserted -1806609307 1589507508
,
\change_unchanged
 while the right-hand side is the tractable expression that guides the updating
 of parameters 
\begin_inset Formula $\mbtheta,\mbphi$
\end_inset

 in an end-to-end fashion.
\end_layout

\begin_layout Subsection

\change_deleted -1806609307 1589491989
Convolutional Layers
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted -1806609307 1589491986
In this work, we use convolutional encoders and decoders in our VAE architecture.
 Introduced in 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun1989backpropagation"
literal "false"

\end_inset

, convolutional layers have enabled tremendous performance increase in certain
 neural network applications where the data is of a certain spatial neighborhood
 structure such as images or audio waveform.
 They exploit an important observation of such data, where the learner should
 be equivariant to translations.
 This is an important injection of inductive bias into the network that
 largely reduce the number of parameters compared to the fully connected
 network by the use of parameter sharing.
 It eventually increase the statistical learning efficiency, especially
 for small samples.
 It must be noted however, convolutional layers are not equivariant to scale
 and rotation as they are to translation.
 Knowing what sort of inductive biases are injected into these layers is
 important for the understanding of disentanglement, which we will introduce
 later in this paper.
\change_unchanged

\end_layout

\begin_layout Subsection
Review of 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics in PCA
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:ReviewPCA"

\end_inset

 Process monitoring via PCA is typically undertook using the so-called 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics 
\begin_inset CommandInset citation
LatexCommand citep
key "Chen2004-px"
literal "false"

\end_inset

.
 The 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA is defined as the reconstruction error between the real
 sample 
\begin_inset Formula $\mbx$
\end_inset

 and the reconstructed sample 
\begin_inset Formula $\tilde{\mbx}$
\end_inset

.
 Its geometric representation is how far the sample is away from the learned
 subpsace of in-control (IC) samples.
 
\begin_inset Formula $\Tsq$
\end_inset

 represents how far the sample is away from the cluster of latent codes
 of the IC samples.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\Tsq$
\end_inset

 statistics and 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA are defined as follows: 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%TODO: give numbers to each line or not?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% No need
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Formula 
\begin{equation}
\begin{split}Q(\mbx) & =\gg\mbx-\tilde{\mbx}\gg^{2}\\
\Tsq_{PCA}(\mbx) & =\mbz^{\top}\mbSigma\inv_{r}\mbz=\mbx^{\top}\mbW_{r}\mbSigma\inv_{r}\mbW_{r}^{\top}\mbx,
\end{split}
\label{eqn: QTPCA}
\end{equation}

\end_inset

where matrix 
\begin_inset Formula $\mbW_{r}$
\end_inset

 is the loading matrix, and 
\begin_inset Formula $\mbSigma\inv_{r}$
\end_inset

 is the inverse of the covariance matrix when only the first 
\begin_inset Formula $r$
\end_inset

 principal components are kept.
 There are various methods to choose 
\begin_inset Formula $r$
\end_inset

 such as fixing the percentage of variation explained 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 41"
key "Chiang2001-nu"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
For processes with relatively small latent and residual dimensionality,
 the upper control limits of these statistics for the 
\begin_inset Formula $\alpha$
\end_inset

% Type-1 error tolerance is constructed by employing the normality assumptions
 of PPCA 
\begin_inset CommandInset citation
LatexCommand citep
after "p. 43-44"
key "Chiang2001-nu"
literal "false"

\end_inset

.
 However, using such measures for high-dimensional nonlinear profiles is
 prohibitively error-prone as both 
\begin_inset Formula $r$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 will be much higher than the assumptions on chi-square distribution can
 tolerate.
 As an alternative, non-parameteric methods to estimate upper percentiles
 are increasingly used for this purpose, such as simple sample percentile
 on a held-out set or fitting kernel density estimation to in-control statistics.
\end_layout

\begin_layout Subsection
Review and Critique of Previously Proposed Monitoring Statistics Proposed
 for VAE
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:critique"

\end_inset

 Three works have considered VAE for process monitoring, all of which propose
 different statistic formulations for monitoring.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang2019-lu"
literal "false"

\end_inset

 formulate what they call 
\begin_inset Formula $H^{2}$
\end_inset

 which is basically the Mahalanobis distance of the mean of the proposal
 distribution from standard Gaussian distribution.
 
\begin_inset Formula 
\begin{equation}
H^{2}=\mu_{\mbphi}(\mbx)^{\top}\mu_{\mbphi}(\mbx)
\end{equation}

\end_inset

The major drawback of using only this statistic is that it completely ignores
 the disturbances in residual distribution.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

 claim to extend 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $SPE$
\end_inset

 of PCA for VAE.
 For a given input 
\begin_inset Formula $\mbx$
\end_inset

 and a sing
\change_inserted -1806609307 1589508486
l
\change_unchanged
e sample drawn from 
\change_inserted -1806609307 1589508479
the 
\change_unchanged
proposal
\change_inserted -1806609307 1589508481
 distribution
\change_unchanged
 
\begin_inset Formula $\mbz^{(l)}\sim\encoding$
\end_inset

 and a reconstruction based on that sample 
\begin_inset Formula $\mbx^{(l)}\sim p_{\mbtheta}(\mbx\g\mbz^{(l)})$
\end_inset

, the proposed test statistics in this work are as follows: 
\begin_inset Formula 
\begin{equation}
\begin{aligned}T^{2} & =(\mbz^{(l)}-\bar{\mbz})^{\top}S_{\mbz}\inv(\mbz^{(l)}-\bar{\mbz})\\
SPE & =\gg\mbx^{(l)}-\mbx\gg_{2}^{2},
\end{aligned}
\end{equation}

\end_inset

where 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 are estimated over a single loop from the data.
 
\change_inserted -1806609307 1589509898
The proposed control charting methodology suggests that these two statistics
 work in combination and at least one vote of either statistics is enough
 to make a detection.
 However, the estimation of the 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 can be slow and unneccesary since
\change_unchanged
 
\change_deleted -1806609307 1589508942
It is unclear why they would use such an extra step since 
\change_unchanged
for a well trained VAE, 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 would be approximately equal to 
\change_deleted -1806609307 1589509318
the
\change_inserted -1806609307 1589509326
0 and 
\begin_inset Formula $I$
\end_inset


\change_deleted -1806609307 1589509847
 mean and covariance of the standard Gaussian distribution
\change_unchanged
.
 Instead, 
\change_deleted -1806609307 1589509355
they take on the additional risk associated with
\change_unchanged
 the estimation
\change_inserted -1806609307 1589509366
 can be unstable when data is limited
\change_unchanged
.
 
\change_deleted -1806609307 1589509896
The proposed control charting methodology suggests that these two statistics
 work in combination and at least one vote of either statistics is enough
 to make a detection.
 The authors do not mention how false alarm rate can be fixed given this
 methodology.
\change_unchanged

\end_layout

\begin_layout Standard
Finally, 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2019systematic"
literal "false"

\end_inset

 propose the 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 statistics by focusing on the two major components of the tractable part
 of the objective function of VAE shown as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The 
\begin_inset Formula $D$
\end_inset

 statistic is simply the KL divergence between the prior and proposal.
 For 
\begin_inset Formula $R$
\end_inset

 statistic, like 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

, they employ summary statistics over samples from proposal but also claim
 that sampling size can be fixed to one: 
\begin_inset Formula 
\begin{equation}
\begin{aligned}D & =\KL{\encoding}{p(\mbz)}\\
R & =\frac{1}{L}\sum_{l=1}^{L}-\log q_{\mbtheta}(\mbx\g\mbz^{(l)}),
\end{aligned}
\label{eq: DR}
\end{equation}

\end_inset


\begin_inset Formula $SPE$
\end_inset

 in and 
\begin_inset Formula $R$
\end_inset

 are essentially the same quantities up to a constant, which makes them
 identical in the context of monitoring statistic
\change_inserted -1806609307 1589509957
.

\change_unchanged
 
\change_deleted -1806609307 1589509958
because the rankings for the same testing samples given the same model will
 be the same for both.
\change_unchanged

\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:methodology"

\end_inset

 
\end_layout

\begin_layout Subsection
Proposed Monitoring Statistic
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:proposed-statistic"

\end_inset


\end_layout

\begin_layout Standard
Log-likelihood, 
\begin_inset Formula $\log\ptheta(\mbx)$
\end_inset

, arises as a natural candidate for monitoring statistic
\change_inserted -1806609307 1589510262
s
\change_unchanged
 in the context of VAEs.
 That is, given a well trained VAE, in-control samples should have relatively
 
\change_deleted -1806609307 1589516834
higher
\change_inserted -1806609307 1589516836
larger
\change_unchanged
 log-likelihood than out-of-control samples.
 However, the required number of Monte Carlo samplesâ€”
\begin_inset Formula $L$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:SummationLL"
plural "false"
caps "false"
noprefix "false"

\end_inset

â€” can be prohibitively large to get meaningful estimates of the likelihood
 
\change_inserted -1806609307 1589517898

\size small
sdf
\change_unchanged

\size default

\begin_inset CommandInset citation
LatexCommand citep
key "Kingma2013-dl"
literal "false"

\end_inset

, which do not satisfy the real-time 
\change_deleted -1806609307 1589516859
monitoring
\change_unchanged
 requirement 
\change_deleted -1806609307 1589516866
for
\change_inserted -1806609307 1589516912
to monitor
\change_unchanged
 high throughput systems.
 To address this issue, ELBO defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be used for a reasonable approximation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589526638
A natural choice of the monitoring statistics is to separate the ELBO into
 two terms 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 and 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

.
 We call 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 expected reconstruction error, which stands for expected reconstruction
 error in the residual space and 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 as latent KLD loss.

\change_unchanged
 To understand the role of 
\change_deleted -1806609307 1589523180
both
\change_inserted -1806609307 1589526394
these two
\change_unchanged
 terms in process monitoring, we revisit the assumptions of the model described
 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Let us formally represent an out-of-control distribution as 
\begin_inset Formula $p_{\delta}(\mbx)\neq p(\mbx)$
\end_inset

.
 Since 
\begin_inset Formula $p(\mbx)=\int p(\mbx\g\mbz)p(\mbz)d\mbz$
\end_inset

, we can observe two sources of out-of-control behaviours: disturbances
 in latent distribution 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

 and disturbances in observable distribution 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

.
 Note that various combinations of these two disturbances cover disturbances
 in the entire process.
 One can argue that 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 can detect the disturbances in the observable space 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

 and 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 represents the change in the latent space 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

.
 We know that for processes that can be accurately modeled by PCA, both
 terms play an important role 
\begin_inset CommandInset citation
LatexCommand citep
key "kim2003process"
literal "false"

\end_inset

 for process monitoring.
 We argue that this holds true for PPCA too.
 To prove this, we link the monitoring statistics of PCA (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:ReviewPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

) to PPCA using the ELBO framework.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop: T2Q"

\end_inset

 We know from the definition of PPCA 
\begin_inset CommandInset citation
LatexCommand citep
key "tipping1999probabilistic"
literal "false"

\end_inset

 that the prior, encoding and decoding functions are normally distributed
 as: 
\begin_inset Formula 
\[
\begin{split}p(\mbz) & =\Norm(0,\mbI)\\
\decoding & =\Norm(\mbW\mbz,\sigma^{2}\mbI)\label{eq:Gaussian}
\end{split}
\]

\end_inset

In this case, from PPCA, the encoder also follows the normal distribution
 as 
\begin_inset Formula $\encoding=\Norm(\mu_{\mbphi}(\mbx),\Sigma_{z})$
\end_inset

, where 
\begin_inset Formula $\mu_{\mbphi}(\mbx)=\mbM^{-1}\mbW^{\top}\mbx$
\end_inset


\change_inserted -1806609307 1589517549
,
\change_deleted -1806609307 1589517529
 and
\change_unchanged
 
\begin_inset Formula $\Sigma_{z}=\sigma^{2}\mbM^{-1}$
\end_inset

, 
\change_deleted -1806609307 1589517553
where
\change_inserted -1806609307 1589517553
and
\change_unchanged
 
\begin_inset Formula $\mbM=\mbW^{\top}\mbW+\sigma^{2}\mbI$
\end_inset

.
 Then, the two monitoring statistics can be 
\change_deleted -1806609307 1589523209
defined
\change_inserted -1806609307 1589523210
derived
\change_unchanged
 as: 
\begin_inset Formula 
\begin{equation}
\KL{\encoding}{p(\mbz)}=\frac{1}{2}\gg\mu_{\mbphi}(\mbx)\gg^{2}+C_{1}\label{eqn:KL_PPCA}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\E_{\mbz\sim q_{\mbphi}}\log\decoding\propto\gg\mbx-\mbW\mu_{\mbphi}(\mbx)\gg^{2}+C_{2}\label{eqn:E_PPCA}
\end{equation}

\end_inset

where 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{2}$
\end_inset

 are constants that doesn't depend on 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
The proof is given in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:PoofOfPropTQ"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that the constants do not affect
\change_inserted -1806609307 1589521223
 the
\change_unchanged
 
\change_deleted -1806609307 1589521138
out-of-control
\change_inserted -1806609307 1589521146
profile monitoring
\change_unchanged
 decision
\change_deleted -1806609307 1589521228
s
\change_unchanged
.
 Thus, the test statistics 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 is equivalent to the 
\begin_inset Formula $T^{2}$
\end_inset

 Statistics of PCA as defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn: QTPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

, and 
\begin_inset Formula $\E_{\mbz\sim q_{\mbphi}}\log\decoding$
\end_inset

 is equivalent the 
\begin_inset Formula $Q$
\end_inset

 statistic.
 Observe that previously proposed formulations mentioned in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

 relyâ€”directly or indirectlyâ€”on this framework.
 Statistics 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $SPE$
\end_inset

 are based on 
\begin_inset Formula $Q$
\end_inset

 or in the case of PPCA 
\begin_inset Formula $\E_{\mbz\sim q_{\mbphi}}\log\decoding$
\end_inset

.
 Let us call these 
\change_deleted -1806609307 1589523245

\emph on
observable variable
\change_inserted -1806609307 1589523245
residual
\change_unchanged
-based statistics
\emph default
.
 Likewise, 
\begin_inset Formula $H^{2},T^{2}$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 is based on 
\begin_inset Formula $T^{2}$
\end_inset

 of PCA or 
\begin_inset Formula $\KL{\encoding}{p(\mbz)}$
\end_inset

 of PPCA.
 We shall call these statistics, 
\emph on
latent-variable based statistics
\emph default
, as they rely exclusively on latent representations.

\change_inserted -1806609307 1589521150
 
\change_unchanged

\end_layout

\begin_layout Standard
Our first major claim is that latent variable-based statistics are not useful
 for profile monitoring when deep neural network based encoders are used
 to produce latent representations.
 We 
\change_deleted -1806609307 1589525946
cite
\change_inserted -1806609307 1589525948
 give
\change_unchanged
 
\change_inserted -1806609307 1589525954
the following 
\change_unchanged
two
\change_inserted -1806609307 1589525957
 major
\change_unchanged
 reasons
\change_inserted -1806609307 1589525965
:
\change_deleted -1806609307 1589525963
 for that.
 First,
\change_unchanged
 
\change_inserted -1806609307 1589525967
1) 
\change_unchanged
for such high-dimensional complex processes, most of the change or disturbances
 should be expected on residual distribution.
 According to 
\begin_inset CommandInset citation
LatexCommand citet
key "severson2016perspectives"
literal "false"

\end_inset

 faults in complex real-life processes tend to alter the existing relationship
 between latent sources of variation and what is observed, as opposed to
 pushing to most extreme cases in the latent variational sources.

\change_inserted -1806609307 1589526055
 2) A deep autoencoders typically require a much smaller latent dimension
 compared to PCA, which leaves a much smaller chance for change happening
 in the latent space.
 
\change_deleted -1806609307 1589525981
econd
\change_inserted -1806609307 1589526008
3)
\change_deleted -1806609307 1589525990
,
\change_unchanged
 the mean encoder 
\begin_inset Formula $\mu_{\mbphi}$
\end_inset

â€”which is a deep encoderâ€”is likely to lack two important features: disentangled
 representations and extrapolation capabilities.
 These two qualities are required so that extreme values of latent variables
 consistently converge out of in-control zone.
 We illustrate this in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:entang-extrap"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Observe how Point C is falsely identified as in-control when learned representa
tions are not disentangled or when they fail to extrapolate.

\change_inserted -1806609307 1589526008
 
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/Disentangled_Extrapolated.pdf
	lyxscale 50
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Different ways how an independent source of variation is learned.
 
\series bold
Top Left:
\series default
 A hypothetical in-control latent space (gray area) and a range of independent
 source of variation (line).
 Points A and B are extreme values of in-control and point C is an out-of-contro
l sample.
 
\series bold
Top Right: 
\series default
An ideal learned representation.
 
\series bold
Bottom Left:
\series default
 A learned representation fails to extrapolate.
 
\series bold
Bottom Right: 
\series default
A learned representation that is entangled.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:entang-extrap"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Theorem 1 in 
\begin_inset CommandInset citation
LatexCommand citet
key "locatello2018challenging"
literal "false"

\end_inset

 proves that without proper inductive biases injected into the model about
 sources of variation, it is impossible to find disentangled representations.
 Unfortunately, injecting such inductive biases requires detailed anticipation
 of variations among in-control samples as well as specialized neural network
 structures, both of which are extremely challenging tasks given the potential
 complexity of the processes that generates high-dimensional profiles.
 Moreover, even if the representations are disentangled, correct mappings
 of 
\begin_inset Formula $\encoding$
\end_inset

 may still not be obtained due to failure to extrapolate.
 Deep neural networks approximates well only at a bounded domain defined
 by where the training setâ€”in our case, the in-control setâ€”is densely sampled
 from.
 The behavior of the function is often unpredictable outside that domain.
 In other words, it may not extrapolate well beyond the domain of training
 samples.
 We refer interested readers to 
\begin_inset CommandInset ref
LatexCommand ref
reference "app:rosenbrock"
plural "false"
caps "false"
noprefix "false"

\end_inset

 where we replicated this phenomena on a toy example.
 To see why this is a problem, first note that the encoder 
\begin_inset Formula $\encoding$
\end_inset

 will only be trained with profiles densely sampled from the bounded region
 of in-control samples, for which 
\begin_inset Formula $\pz$
\end_inset

 are high.
 The behavior of the encoder is uncertain for profiles coming from dense
 regions of out-of-control latent structure 
\begin_inset Formula $p_{\delta}(\mbz)\neq\pz$
\end_inset

.
 We expect increased false negatives should the model falsely map these
 profiles onto high density regions of 
\begin_inset Formula $\pz$
\end_inset

 but not 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

.
\end_layout

\begin_layout Standard
Unlike latent variable-based statistics, extrapolation and disentanglement
 issues in 
\begin_inset Formula $\encoding$
\end_inset

 along with extrapolation issues in 
\begin_inset Formula $\decoding$
\end_inset

 help 
\change_deleted -1806609307 1589523238
observable variable
\change_inserted -1806609307 1589523238
residual
\change_unchanged
-based statistic detect faults better.
 More interestingly, this holds true even when the disturbance is purely
 on the latent structure.
 Incorrect mappings by 
\begin_inset Formula $\encoding$
\end_inset

 will lead to incorrect generations by 
\begin_inset Formula $\decoding$
\end_inset

 and thus larger statistic that will fall beyond control limits.
 Second, even if the mapping was correct, we might have extrapolation issues
 in the decoder 
\begin_inset Formula $\decoding$
\end_inset

 which is another possibility for this statistic to capture disturbances
 in latent variations.
 To see why this is the case, note that the encoder 
\begin_inset Formula $\encoding$
\end_inset

 is optimized to produce samples more from where 
\begin_inset Formula $\pz$
\end_inset

 is large.
 In turn, the decoder 
\begin_inset Formula $\decoding$
\end_inset

 will mostly be trained on samples from where 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

 is low.
 For disturbances in observable region 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)\neq p(\mbx\g\mbz)$
\end_inset

, it is trivial to see how 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 will be effective at capturing faults.
\end_layout

\begin_layout Standard
As of now, we have enough reasons to recommend the use of 
\change_deleted -1806609307 1589523240
observable variable
\change_inserted -1806609307 1589523240
residual
\change_unchanged
-based statistics as the only type of test statistic for a profile monitoring
 application that uses VAE.
 Indeed, variations of this type have been proposed previously: 
\begin_inset Formula $SPE$
\end_inset

 of 
\begin_inset CommandInset citation
LatexCommand citet
key "lee2019process"
literal "false"

\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "wang2019systematic"
literal "false"

\end_inset

.
 However, they both use random samples from the proposal distribution to
 estimate the expectation.
 This approach may require a large number of samples to be generated and
 thus a large number of forward passes on the decoder network, which is
 prohibitively expensive in terms of computation.
 We propose using first
\change_deleted -1806609307 1589528424
 
\change_inserted -1806609307 1589528424
-
\change_unchanged
order Taylor expansion instead
\change_inserted -1806609307 1589526664
 of 
\begin_inset Formula $ERE$
\end_inset


\change_unchanged
.
 
\change_deleted -1806609307 1589526445
We call this approximation 
\begin_inset Formula $ERE$
\end_inset

, which stands for expected reconstruction error due to the resemblance
 of this statistic to the reconstruction error in deterministic autoencoders.
 
\begin_inset Formula $ERE$
\end_inset

 can be derived as follows:
\change_inserted -1806609307 1589526299

\end_layout

\begin_layout Proposition

\change_inserted -1806609307 1589526870
The Tayler Expansion of the ERE can be derived as 
\begin_inset Formula 
\[
ERE=C_{3}+\gg\mbx-\mu_{\mbtheta}(\mu_{\mbphi}(\mbx))\gg_{2}^{2}+\frac{1}{2}tr(H_{z}\Sigma_{z})+o(\|\mathbf{z}-\mu(\mathbf{x})\|^{3})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589527346
The proof is given in 
\begin_inset CommandInset ref
LatexCommand ref
reference "app:ere"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard

\change_deleted -1806609307 1589526693
\begin_inset Formula 
\begin{align}
ERE & \triangleq\E_{\mbz\sim q_{\mbtheta}}\log\decoding\label{eqn:propstat:final}\\
 & =\E_{\mbz\sim p_{\mbtheta}}\left[\log q_{\mbtheta}(\mbx\g\mbz-(\mu_{\mbphi}-\mbz))\right]\\
 & \approx\log\ptheta(\mbx\g\mu_{\mbphi}(\mbx))\\
 & \propto\gg\mbx-\mu_{\mbtheta}(\mu_{\mbphi}(\mbx))\gg_{2}^{2}+C_{3}
\end{align}

\end_inset


\change_unchanged
The constant 
\begin_inset Formula $C_{3}$
\end_inset

 can be ignored given control charting logistics.
 
\change_inserted -1806609307 1589528880
We name the 1st-order approximation of 
\begin_inset Formula $ERE$
\end_inset

 as 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 2nd-order approximation of 
\begin_inset Formula $ERE$
\end_inset

 as 
\begin_inset Formula $ERE_{2}$
\end_inset

.
 
\change_unchanged
Given a trained VAE, 
\change_deleted -1806609307 1589527376
this quantity 
\change_inserted -1806609307 1589528876

\begin_inset Formula $ERE_{1}$
\end_inset

 
\change_unchanged
can be computed
\change_inserted -1806609307 1589528876
 efficiently
\change_unchanged
 by forward passing the new profile from the process 
\begin_inset Formula $\mbx$
\end_inset

 through 
\begin_inset Formula $\mu_{\mbphi}$
\end_inset

 and 
\begin_inset Formula $f_{\mbtheta}$
\end_inset

 successively and calculating the squared prediction error, without any
 reparameterization.

\change_inserted -1806609307 1589531740
 We will evaluate the use of 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 for process monitoring.
 
\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589531741
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589531798
Dorukhan, I am thinking of adding the performnace of ERE2 as well given
 this is not that hard to compute and may be quite likely to be asked by
 reviewer.
 I know that you mentioned the performance is similar but I felt that we
 can add this as well.
 
\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Subsection
Profile Monitoring Procedure
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:methodology:procedure"

\end_inset

 A typical profile monitoring follows two phases: Phase-I analysis, and
 Phase-II analysis.
 Phase-I analysis focuses on understanding the process variability by training
 an appropriate in-control mode, and selecting an appropriate control limit.
 In our case, Phase-I analysis results in a trained model (i.e.
 an encoder and a decoder) and an Upper Control Limit (UCL) to help set
\change_inserted -1806609307 1589528503
 
\change_unchanged
up the control chart for each of the monitoring statistics.
 In Phase-II, the system is exposed to new profiles generated by the process
\change_inserted -1806609307 1589528492
 in real time
\change_unchanged
 to decide whether these profiles are in-control or out-of-control.
 Our experimentation plan, outlined below, is formulated to emulate this
 scenario to effectively assess the performance of any combination of a
 model, a test statistic and a disturbance scenario
\change_inserted -1806609307 1589528531
 to generate the out-of-control samples
\change_unchanged
.
 
\end_layout

\begin_layout Itemize
Obtain IC dataset 
\begin_inset Formula $\dataset$
\end_inset

 and partition it into train, validation and test sets 
\begin_inset Formula $\dataset^{trn}$
\end_inset

,
\begin_inset Formula $\dataset^{val}$
\end_inset

,
\begin_inset Formula $\dataset^{tst}$
\end_inset


\change_inserted -1806609307 1589528542
.

\change_unchanged
 
\end_layout

\begin_layout Itemize
Train VAE using samples from 
\begin_inset Formula $\dataset^{trn}$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculate test statistic for all 
\begin_inset Formula $\mbx\in\dataset^{val}$
\end_inset

 and take it's 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

95
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 percentile as the UCL.
\end_layout

\begin_layout Itemize
Start admitting profiles from the process.
 Calculate test statistic using the trained VAE.
 If test statistic is over UCL, identify the sample as OC.
 
\end_layout

\begin_layout Standard
We train 10 different model instances with different seeds to account for
 inherent randomness due to
\change_inserted -1806609307 1589528562
 the
\change_unchanged
 weight initialization of deep neural networks.
\end_layout

\begin_layout Subsection
Neural Network Architectures
\change_inserted -1806609307 1589528594
 and Training
\change_unchanged
 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-Architectures"

\end_inset


\change_inserted -1806609307 1589491997

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589528740
In this work, we use convolutional encoders and decoders in our VAE architecture
 to represent the spatial neighborhood structures of the profiles.
 Introduced in 
\begin_inset CommandInset citation
LatexCommand citep
key "lecun1989backpropagation"
literal "false"

\end_inset

, convolutional layers have enabled tremendous performance increase in certain
 neural network applications where the data is of a certain spatial neighborhood
 structure such as images or audio waveform.
 They exploit an important observation of such data, where the learner should
 be equivariant to translations.
 This is an important injection of inductive bias into the network that
 largely reduces the number of parameters compared to the fully connected
 network by the use of parameter sharing.
 It eventually increases the statistical learning efficiency, especially
 for small samples.
 It must be noted, however, convolutional layers are not equivariant to
 scale and rotation as they are to translation.
 Knowing what sort of inductive biases is injected into these layers is
 important for the understanding of disentanglement, which we will introduce
 later in this paper.
\change_unchanged

\end_layout

\begin_layout Standard
We use the encoder-decoder structure outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The layers used that builds the model architectures used in this study
 are summarized as follows:
\end_layout

\begin_layout Itemize
C(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional layer with arguments referring to 
\change_inserted -1806609307 1589528754
the 
\change_unchanged
number of output channels 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

 and size of zero-padding 
\begin_inset Formula $P$
\end_inset

.
 
\end_layout

\begin_layout Itemize
CT(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional transpose layer with arguments referring to the number
 of output channels 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

, and size of zero-padding 
\begin_inset Formula $P$
\end_inset

.
 
\end_layout

\begin_layout Itemize
FC(
\begin_inset Formula $I,O$
\end_inset

): Fully connected layer with arguments referring to input dimension 
\begin_inset Formula $I$
\end_inset

 and output dimension 
\begin_inset Formula $O$
\end_inset

.
 
\end_layout

\begin_layout Itemize
A: Activation function.
 Leaky ReLU with a negative slope 
\change_inserted -1806609307 1589528766
of 
\change_unchanged

\begin_inset Formula $0.2$
\end_inset

.
\end_layout

\begin_layout Standard
Here, C(), CT(), and FC() are considered the linear transformation layers
 while R(), LR(), and S() are considered the nonlinear activation layers.
 Strided convolutions can be used to decrease the spatial dimensions in
 the encoders.
 Pooling layers are typically not recommended in autoencoder-like architectures
 
\begin_inset CommandInset citation
LatexCommand citep
key "radford2015unsupervised"
literal "false"

\end_inset

.
 Convolutional transpose layers are used to upscale latent codes back to
 observable dimensions.
\end_layout

\begin_layout Standard
The sequential order of the computational graphs used for this study 
\change_deleted -1806609307 1589528793
are
\change_inserted -1806609307 1589528794
is
\change_unchanged
 summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The encoder will output 
\begin_inset Formula $2r$
\end_inset

 nodes
\change_inserted -1806609307 1589528802
,
\change_unchanged
 which is a concatenation of the inferred posterior mean and variance, both
 are of length 
\begin_inset Formula $r$
\end_inset

.

\change_inserted -1806609307 1589528588
 
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float table
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\newcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture details of deep neural networks used in this study
\begin_inset CommandInset label
LatexCommand label
name "tab:model-architectures"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top" width="50text%">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Module 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Architecture
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Encoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
C(32, 4, 2, 1) - A - C(32, 4, 2, 1) - A - C(64, 4, 2, 1) - A - C(64, 4,
 2, 1) - A - C(64, 4, 1, 0) - FC(256, 
\begin_inset Formula $2r$
\end_inset

) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Decoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
FC(
\begin_inset Formula $r$
\end_inset

, 256) - A - CT(64, 4, 0, 0) - A - CT(64, 4, 2, 1) - A - C(32, 4, 2, 1)
 - CT(32, 4, 2, 1) - A - CT(1, 4, 2, 1) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\change_inserted -1806609307 1589528604

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589528938
The training of the neural network is given as follows: %TODO: 
\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589528637
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589528651
Can you give some details on the training of the Neural Network? 
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Section
Simulation Study Analysis and Results
\change_inserted -1806609307 1589499493
 
\begin_inset CommandInset label
LatexCommand label
name "sec:Simulation-Study-Analysis"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
In this section, we propose 
\change_deleted -1806609307 1589530904
the setup and results of our
\change_inserted -1806609307 1589530912
to evaluate the proposed methodology via the
\change_unchanged
 simulation study
\change_deleted -1806609307 1589530934
.
 The rationale behind conducting a simulation study is to have full control
 and information over the data generating process.
 This way, we can 
\change_inserted -1806609307 1589530938
 to 
\change_unchanged
test our claims we make in
\change_inserted -1806609307 1589527981
 
\change_unchanged

\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted -1806609307 1589530893
 in a controlled environment over the data generating process
\change_unchanged
.
 For every experiment mentioned in this section, we follow the procedure
 outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and we use VAE models with the architecture described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-Architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Simulation Setup
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:simsetting"

\end_inset

 We first evaluate the performance of the deep latent variable models in
 a simulation setting inspired by the work of 
\begin_inset CommandInset citation
LatexCommand citet
key "Shi2016-tg"
literal "false"

\end_inset

.
 The simulation procedure produces 2D point clouds that resemble the scanned
 topology of a gasket bead.
 Let each pixel on a 
\begin_inset Formula $64$
\end_inset

 by 
\begin_inset Formula $64$
\end_inset

 grid be denoted by a tuple 
\begin_inset Formula $\mbp=(p_{0},p_{1})$
\end_inset

.
 The values of the tuples stretch from 
\begin_inset Formula $0$
\end_inset

 to 
\begin_inset Formula $1$
\end_inset

, equally spaced, left to right and bottom-up.
 Each tuple takes a value based on its location through a function 
\begin_inset Formula $\mbp\mapsto f(\mbp;\czero,r)+\epsilon$
\end_inset

, where 
\begin_inset Formula $\epsilon\sim\Norm(0,1\times10^{-2})$
\end_inset

 is i.i.d Gaussian noise.
 The function 
\begin_inset Formula $f$
\end_inset

 is parametrized by the horizontal center location of the bead 
\begin_inset Formula $\czero$
\end_inset

, and the radius of the bead 
\begin_inset Formula $r$
\end_inset

.
 The vertical center of the bead is fixed to be at the center.
 Given any parameter set 
\begin_inset Formula $\{c_{0},r\}$
\end_inset

, each pixel 
\begin_inset Formula $\mbp$
\end_inset

 can be evaluated with the following logic: 
\begin_inset Formula 
\begin{equation}
\begin{split}g(\mbp;c_{0},r) & =1-\frac{(p_{0}-\czero)}{r}^{2}-\frac{(p_{1}-0.5)}{r}^{2}\\
f(\mbp;c_{0},r) & =\begin{cases}
\sqrt{g(\mbp;c_{0},r)} & \mbox{if }g(\mbp;c_{0},r)\geq0\\
0 & \mbox{if }g(\mbp;c_{0},r)<0
\end{cases}
\end{split}
\label{eq:gasketfun}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The samples are best visualized as grayscale images as shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gasketgrid"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Float figure
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:gasketgrid"

\end_inset

 
\begin_inset Graphics
	filename figs/gasket.pdf
	lyxscale 50
	width 90line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Gasket profiles depicted as grayscale images simulated with radius and center
 location they coincide with on the axes.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We define the sources of variation in IC gasket beads by two latent variables
 sampling from independent Gaussian distributions: 
\begin_inset Formula 
\begin{equation}
\begin{split}\czero\sim\Norm(0.5,1\times10^{-2})\\
r\sim\Norm(0.2,6.25\times10^{-4})
\end{split}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Finally, we will consider the following four types of OC variation patterns
 for the system:
\end_layout

\begin_layout Itemize

\series bold
Location shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $\czero$
\end_inset

 is altered by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $\czero\sim\Norm(0.5+\delta\times10^{-2},1\times10^{-2})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Width shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $a$
\end_inset

 is perturbed by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $r\sim\Norm(0.2+\delta\times10^{-4},6.25\times10^{-4})$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Mean shift
\series default
: all of the pixels are added an additive disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $f(\mbp;c_{0},r)\leftarrow f(\mbp;c_{0},r)+\delta$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Magnitude shift:
\series default
 all of the pixels are added a
\change_deleted -1806609307 1589531056
n
\change_unchanged
 multiplicative disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula $f(\mbp;c_{0},r)\leftarrow f(\mbp;c_{0},r)*\delta$
\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\delta$
\end_inset

 is the intensity of the change.
 Note that
\change_inserted -1806609307 1589531090
 the
\change_unchanged
 location shift and width shift represent disturbances in latent distribution
 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

.
 An important distinction between the two is that location equivariance
 is injected into convolutional networks but not scale equivariance, therefore
 we expect different reactions to these changes by deep convolutional latent
 variable models in terms of disentanglement.
 The other two cases, mean shift and magnitude shift, represent disturbances
 in the conditional distribution 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)$
\end_inset

.
 The training, validation, and testing IC or OC samples are generated of
 size 500 each.
\end_layout

\begin_layout Subsection
On the disentanglement and extrapolation performance of the encoder 
\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:recognition"

\end_inset


\end_layout

\begin_layout Standard
To investigate the disentanglement and extrapolation performance of the
 recognition network
\change_inserted -1806609307 1589531148
,
\change_unchanged
 we employ the following procedure.
 First, we train a VAE with 
\change_inserted -1806609307 1589531154
a 
\change_unchanged
2-dimensional latent code (for easier visualization) to convergence using
 in-control samples as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Then, we a set of profiles with varying center location 
\begin_inset Formula $c_{0}$
\end_inset

 and radius 
\begin_inset Formula $r$
\end_inset

 into the recognition network to obtain their respective proposal distributions.
 The points are picked inside and outside the tolerance region of the two
 quality characteristics to be compared against their mapping onto the represent
ation space.
 Finally, we sample 150 points from the proposals and plot them on the represent
ation space.
 The results are shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There are two important observations we would like to point out:
\end_layout

\begin_layout Itemize

\series bold
Representations are partially entangled
\series default
.
 For every fixed center location, the direction of the axis on which radius
 change differs.
 The direction would have been independent from the fixed value of center
 location should the representations were disentangled.
 We may claim partial disentanglement for location
\change_inserted -1806609307 1589531184
,
\change_unchanged
 but that would be quite unlikely if it wasn't for the translational equivarianc
e injected in convolutional layers 
\begin_inset CommandInset citation
LatexCommand citep
after "Thm. 1"
key "locatello2018challenging"
literal "false"

\end_inset

.
\end_layout

\begin_layout Itemize

\change_inserted -1806609307 1589531204

\series bold
The 
\change_unchanged
Encoder cannot extrapolate for extremely values of radius.
 
\series default
Observe how from point 7 to 1, from point 6 to 0 and from point 8 to 2 the
 proposal means are converging to a point in the in-control region.
 This signals that the representations will not be able to extrapolate for
 even smaller values of radii.
\end_layout

\begin_layout Standard
Given these two observations, we can conclude right away that extremely
 small gaskets (in terms of radius) will likely be gone undetected
\change_inserted -1806609307 1589531240
 if only the latent variable-based statistics is used
\change_unchanged
.
 On the other hand, samples coming from some tolerable radius rangesâ€”such
 as point 7â€” will be falsely identified as out-of-control.
 Observe for that point, how most sample latent codes fall outside the tolerable
 region in the inferred space.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/proposals_gray.pdf
	width 100line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Figure depicting the behavior of the recongition network of a VAE trained
 with in-control gasket sampled.
 
\series bold
Left: 
\series default
Nine gasket profiles whose latent variables are picked from in and out of
 the tolerable region of radius and center location.
 The tolerable region is represented by the gray dashed circle, a curve
 of isodistant points in terms of Mahalanobis distance to in control distributio
n.
 
\series bold
Right:
\series default
 150 latent codes sampled from each proposal (associated with the same id
 numbers on the top).
 Isodistant curve for standard Gaussian that is probability-wise equivalent
 to the one on the left is depicted as a gray dashed circle.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:proposals"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589531505
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589531553
Dorukhan, I think we can give figure with colors.
 We need to ensure that if they put into gray scale, it can still be readed.
 But there is no restriction of not using any colored images/figures in
 the paper so we should take advantage of that.
 
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Overall, 
\change_deleted -1806609307 1589531289
our are
\change_inserted -1806609307 1589531290
it is
\change_unchanged
 in agreement with our rationale behind the proposed statistic outlined
 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In other words, without extrapolation and disentanglement, we do not expect
 any monitoring statistic based purely on the output of the recognition
 networkâ€”such as 
\begin_inset Formula $H^{2}$
\end_inset

, 
\begin_inset Formula $T^{2}$
\end_inset

 or 
\begin_inset Formula $D$
\end_inset

 discussed in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

â€” to be robust enough to be safely employed in a process control mission.
\end_layout

\begin_layout Standard
We also want to make a remark that we tried the beta-VAE framework explained
 in 
\begin_inset CommandInset citation
LatexCommand citep
key "higgins2017beta"
literal "false"

\end_inset

 with 
\series bold

\begin_inset Formula $\beta=4$
\end_inset


\series default
 to remedy the problem of entanglement but obtained similar results.
\end_layout

\begin_layout Subsection
On the extrapolation performance of the decoder network
\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:generator"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:manifold_vae"
plural "false"
caps "false"
noprefix "false"

\end_inset

 hints us about the extrapolation performance of the decoder of the same
 VAE described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

 trained on in-control samples described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 It should be cross-examined with 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as the encoder and decoder are tightly coupled to each other.
 We observe two important behavior: the posterior gets distorted beyond
 two or three standard deviations and the representations are partially
 entangled in line with the behavior of its encoder depicted in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To see how this will help 
\change_deleted -1806609307 1589531352
detecting
\change_inserted -1806609307 1589531353
to detect
\change_unchanged
 disturbances in latent space, consider a gasket that is extremely large
 or at the very margins of the grid.
 Since the decoder cannot generate such a sample, there will be large pixel-to-p
ixel differences.
 This will in turn produce a larger monitoring statistic that will likely
 to fall outside the control limit.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/manifold_vae.pdf
	width 90line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Latent space traversal and the response of the decoder of a VAE with 2-dimension
al latent codes and trained with in-control gasket samples.
 Each row represents which latent dimension is traversed while the other
 dimension is fixed at zero.
 Each column represents what value is assigned to that latent dimension
 that is represented by the row label.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:manifold_vae"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
On the estimation of log-likelihood under importance sampling
\end_layout

\begin_layout Standard
Earlier, we claimed that it would take too many Monte Carlo sampling iterations
 to get a meaningful estimate of 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

.
 In this section
\change_inserted -1806609307 1589531389
,
\change_unchanged
 we test that claim on a random sample 
\begin_inset Formula $\mbx$
\end_inset

 using the proposal distribution 
\begin_inset Formula $\mbz\sim\encoding$
\end_inset

 which is obtained via the encoder of the same VAE model we have been using
 in this section.
 See 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Estimation-comparison-between"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for the results.
 The key observation is that it takes at least a few tens of Monte Carlo
 iterations to get a stable and accurate estimation.
 At that level, the single pass through encoder is negligible.
 This means using sampling will be more costly at least a factor of few
 tens to achieve the same accuracy as 
\change_inserted -1806609307 1589531404
the 
\change_unchanged
first-order approximation that we suggest.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/mc_vs_foa.pdf
	lyxscale 90
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimation comparison between Monte Carlo sampling (black line) and first
 order approximation (black dashed line).
 95% confidence interval band is shown in gray band and is based on simulations
 with ten different seeds.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Estimation-comparison-between"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\change_inserted -1806609307 1589531814

\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589531825
Dorukhan, Similar here, we can consider add 2nd order approx.
 
\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Subsection
Comparison of detection performance of proposed statistics
\end_layout

\begin_layout Standard
We now compare the proposed statistics based on how accurately they detect
 profiles from out-of-control processes outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that for all statistics that require sampling, we obtain a single
 sample and calculate the statistic based on that
\change_deleted -1806609307 1589531861
.

\change_unchanged
 
\change_deleted -1806609307 1589531863
This way we
\change_inserted -1806609307 1589531863
 to
\change_unchanged
 keep the computational demand 
\change_inserted -1806609307 1589531881
the 
\change_unchanged
same for all statistics and emulate the computational constraints of a real-life
 case.
 A preliminary result we must check is the robustness of the statistics
 by making sure all proposed statistics have roughly similar false alarm
 rate
\change_inserted -1806609307 1589531893
s
\change_unchanged
 on the held-out in-control test set, which should also be reasonably close
 to the desired rate 5%.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:far"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates that this is the case for all of them.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
False alarm rates on held-out dataset averaged over 10 replications per
 model and monitoring statistic.
 Standard deviations are in parentheses.
\begin_inset CommandInset label
LatexCommand label
name "tab:far"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="6">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Statistic 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SPE/R 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
D 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
H2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
T2 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.041(0.006) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.051(0.005) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.044(0.004) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.052(0.005) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.043(0.009) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Through 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we observe a clear superiority of 
\begin_inset Formula $ERE$
\end_inset

 over other methods when the disturbance is on the observable space (top
 row).
 Latent variable-based statistics 
\begin_inset Formula $D$
\end_inset

, 
\begin_inset Formula $H^{2}$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

 fail in this case since that they are purely computed using the proposal
 distribution latent variables.
 
\begin_inset Formula $ERE$
\end_inset

 also outperforms 
\begin_inset Formula $SPE/R$
\end_inset

, although by a smaller margin it has with the latent variable-statistics.
\end_layout

\begin_layout Standard
For the latter two disturbances occurring purely on latent dimensions, results
 are presented in the bottom row of 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The key observations can be listed as follows:
\end_layout

\begin_layout Itemize
We observe mixed results but generally 
\begin_inset Formula $ERE$
\end_inset

, 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $H^{2}$
\end_inset

 tend to perform better than 
\begin_inset Formula $SPE/R$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

.
 A commonality between the former three is that they don't rely on random
 samples, supporting our argument against this practice.
\end_layout

\begin_layout Itemize
Observe the radius shift type disturbance show in bottom left.
 Even though 
\begin_inset Formula $H^{2}$
\end_inset

 performs better on positive intensities (larger radii), it completely misses
 negative intensities (smaller radii).
 We foresaw this result in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To reiterate, disentanglement and failure to extrapolate in encoder is
 the reason behind this.
 Observe that this result extend to all the latent-variable based statistics.
\end_layout

\begin_layout Itemize
Unlike latent variable-based statistics, 
\begin_inset Formula $ERE$
\end_inset

 and 
\begin_inset Formula $SPE/R$
\end_inset

 behave more robustly against varying intensities.
 Among the two, we observe that 
\begin_inset Formula $ERE$
\end_inset

 consistently outperforms 
\begin_inset Formula $SPE/R$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement !t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/disturbance_on_pxz_vae_only.pdf
	lyxscale 70
	width 100line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Fault detection rates (y-axis) for varying intensities (x-axis) of different
 disturbance types (quadrants).
 Bands represent 95% confidence interval estimated around mean detection
 rates.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:disturbance_on_pxz"

\end_inset


\end_layout

\end_inset


\change_inserted -1806609307 1589531563

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589531563
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589531581
Similar here please use the color images.
 
\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
Note that in a real-life process, disturbances purely on either observable
 or latent space would be very unlikely.
 This is why overall, we conclude that 
\begin_inset Formula $ERE$
\end_inset

 is the best statistic among all when a combination of accuracy, robustness
 and computational demand is considered.
\end_layout

\begin_layout Section
Case Study Analysis & Results
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:case-study"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% TODO (@DS): how many anomaly samples do we have?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% TODO (@DS): Add a figure to illustrate both normal and abnormal samples,
 with one image in each class
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our dataset consists of defect image profiles from a hot-steel rolling process,
 which is shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There are 13 classes of surface defect types identified by the domain engineers.
 Four of these classesâ€”0,1,9 and 11â€”are considered minor defects and they
 constitute our in-control set.
 There are in total 338 images in these classes.
 The other nine classes make up the out of control cases and they have in
 combination 3351 images to report detection accuracy for.
 We randomly partition the IC corpus to fix train, validate and test sets
 with 60%-20%-20% relative sizes
\change_inserted -1806609307 1589532013
,
\change_unchanged
 respectively.
 The rest of the procedure followed is outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Same as in 
\change_inserted -1806609307 1589532024
the 
\change_unchanged
simulation study, to account for randomness in weight initialization, we
 replicate the experiment with 10 different seeds.
 The results are summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\change_inserted -1806609307 1589532059

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589532065
From 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\change_deleted -1806609307 1589532067
W
\change_inserted -1806609307 1589532067
w
\change_unchanged
e 
\change_inserted -1806609307 1589532068
can 
\change_unchanged
observe that 
\begin_inset Formula $ERE$
\end_inset

 consistently outperforms all other statistic formulations.
 
\change_inserted -1806609307 1589532095

\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589532149
Please help add more explanation.
 You can say 1) The latent-variable based monitoring statistics do not have
 any detection power at all.
 2) Our method work better than R/SPE in the residual space.
 
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Standard
For completeness, we also include a comparison of our proposed statistic
 against the baseline method PCA
\change_inserted -1806609307 1589532063
,
\change_unchanged
 where 90% of the variance is retained.
\change_inserted -1806609307 1589532160

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1589532160
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589532211
We also add some reasons for this such as 1) Linear layers requires much
 more number of parameters and 2) The use convolution layers can take advantage
 of the spatial neighborhood structure and inductive bias.
\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Summary of fault detection accuracies on out-of-control cases averaged over
 10 replications per model and monitoring statistic.
 Standard deviations are in parentheses.
 Bolded values represent the maximum average across different statistics.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:rolling_results"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="12" columns="6">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Statistic 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
D 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
H2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
T2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SPE/R 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERE
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fault ID 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.37(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.44
\series default
(0.06) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.17(0.06) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.23(0.04) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.03(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.84(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.85
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.62(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.75
\series default
(0.05) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.58(0.07) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.62(0.09) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15(0.08) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.05(0.05) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.79(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.13(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.17
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.64(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.70
\series default
(0.07) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.49(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.57
\series default
(0.05) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.79(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.02) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.71(0.04) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.77
\series default
(0.02) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Detection rate performance of ERE on hot steel rolling faulty cases compared
 against PCA with 90% variance retained and 
\begin_inset Formula $Q$
\end_inset

 statistic is used.
 Best performers bolded on each row.
 Standard deviation over 10 replication is in parentheses.
\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\arraystretch}{1.3}
\end_inset

 
\begin_inset Tabular
<lyxtabular version="3" rows="12" columns="3">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
PCA-Q
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ERE
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fault ID 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="none" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.44
\series default
(0.06) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.78(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.85
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.56(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.75
\series default
(0.05) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.99(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.52(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.80
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.17
\series default
(0.01) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.34(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.70
\series default
(0.07) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.29(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.57
\series default
(0.05) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.69(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.80(0.02) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.56(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.77
\series default
(0.02) 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "tab:rolling:vsPCA"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To support our claim of ineffectiveness of the statistics based on the latent
 space such as 
\begin_inset Formula $H^{2}$
\end_inset

 statistics, we refer the reader to 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:kde"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We observe how well separated the statistics are for 
\begin_inset Formula $ERE$
\end_inset

 and 
\begin_inset Formula $SPE/R$
\end_inset

 while for latent variable-based statistics the obtained values are mostly
 overlapping.

\change_inserted -1806609307 1589532226
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1589532237
Please add few sentences better explain this behavior.
 
\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/SPE.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$ERE$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:ERE}
\end_layout

\begin_layout Plain Layout

 
\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout

 
\backslash
hspace{0.1
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

 
\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/R.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$SPE/R$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:spe-r}
\end_layout

\begin_layout Plain Layout

 
\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout

 
\end_layout

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/H2.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$H^{2}$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:h2}
\end_layout

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout


\backslash
hfill
\end_layout

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/D.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$D$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:D}
\end_layout

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\begin_layout Plain Layout


\backslash
hfill
\end_layout

\begin_layout Plain Layout


\backslash
begin{subfigure}[b]{0.3
\backslash
textwidth}
\end_layout

\begin_layout Plain Layout

     
\backslash
centering
\end_layout

\begin_layout Plain Layout

     
\backslash
includegraphics[width=
\backslash
textwidth]{figs/kdes/T2.pdf}
\end_layout

\begin_layout Plain Layout

     
\backslash
caption{$T^{2}$}
\end_layout

\begin_layout Plain Layout

     
\backslash
label{fig:kde:T2}
\end_layout

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Kernel density estimation plots of statistics obtained for in-control and
 out-of-control steel defect profiles, per each proposed statistic type.
\begin_inset CommandInset label
LatexCommand label
name "fig:kde"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion 
\begin_inset CommandInset label
LatexCommand label
name "sec:conclusions"

\end_inset


\end_layout

\begin_layout Standard
In this paper, we criticize Phase-II monitoring statistics proposed so far
 in the literature for VAE that they were not performing optimally in terms
 of accuracy and/or computational feasibility.
 First, we point out two important issues related to the latent variables
 learned by the encoder of VAE, namely, entanglement and failure to extrapolate.
 Based on these issues, we argue that latent variable-based statistics should
 be discarded altogether because they will fail to reflect extremity in
 latent space.
 Then, we point out that statistics based on sampling will require too many
 samples to be computationally feasible.
 In 
\change_deleted -1806609307 1589532258
the
\change_unchanged
 light of these, we propose a novel formulation that addresses all of these
 issues.
\end_layout

\begin_layout Standard
We first run a simulation study to support our claims.
 Our simulation study demonstrates the existence of the issues we argue
 in the discussion of our methodology.
 It also shows that our formulation is more robust and for the most part
 more accurate than all the other statistics proposed so far.
 Finally, we validate the superiority of our formulation on a real-life
 case study where steel defect image profiles are used.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "./bibliography"

\end_inset


\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%dummy comment inserted by tex2lyx to ensure that this paragraph is not
 empty
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
refalias
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

section
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

appendix
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Proof of Proposition 3.1 
\begin_inset CommandInset label
LatexCommand label
name "sec:PoofOfPropTQ"

\end_inset


\end_layout

\begin_layout Standard
Kullback-Leibler divergence between two multivariate Gaussian distributions
 has a closed form solution.
 If we define these distributions as 
\begin_inset Formula $p_{0}=N(\mbz;\mbmu_{0},\mbSigma_{0})$
\end_inset

 and 
\begin_inset Formula $p_{1}=N(\mbz;\mbmu_{1},\mbSigma_{1})$
\end_inset

 where 
\begin_inset Formula $\mbmu$
\end_inset

 and 
\begin_inset Formula $\mbSigma$
\end_inset

 are respective mean vectors and covariance matrices, then according to
 
\begin_inset CommandInset citation
LatexCommand citep
key "hershey2007approximating"
literal "false"

\end_inset

 the closed form solution will be the following: 
\begin_inset Formula 
\begin{align*}
\KL{p_{0}}{p_{1}} & =\frac{1}{2}[\log\frac{\g\mbSigma_{1}\g}{\g\mbSigma_{0}\g}+Tr(\mbSigma_{1}\inv\mbSigma_{0})-r\\
 & \quad+(\mu_{0}-\mu{1})^{\top}\mbSigma_{1}\inv(\mu_{0}-\mu{1})]
\end{align*}

\end_inset

Since 
\begin_inset Formula $\encoding=\Norm(\mu(\mbx),\mbSigma_{z})$
\end_inset

 and 
\begin_inset Formula $p(\mbz)=\Norm(0,\mbI)$
\end_inset

, we can derive that 
\begin_inset Formula 
\begin{align*}
\KL{\encoding}{p(\mbz)} & =\frac{1}{2}\left[-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r\right]+\frac{1}{2}\mu(\mbx)^{\top}\mu(\mbx)\\
 & =\frac{1}{2}\mu(\mbx)^{\top}\mu(\mbx)+C,
\end{align*}

\end_inset

where 
\begin_inset Formula $C=-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r$
\end_inset

 is a constant, which doesn't depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Standard
To derive the SPE statistics, we will derive
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2}\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbx^{\top}\mbx-2\mbz^{\top}\mbW\mbx+\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mbx^{\top}\mbx-2\mu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\label{eq: spew}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Here, we know that 
\begin_inset Formula 
\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}tr(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & tr\left(\mbW^{\top}\mbW\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz\mbz^{\top})\right)\nonumber \\
= & tr\left(\mbW^{\top}\mbW(\mu(\mbx)\mu(\mbx)^{\top}+\Sigma_{z})\right)\nonumber \\
= & \mu(\mbx)^{\top}\mbW^{\top}\mbW\mu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\label{eq: tracezwwz}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Therefore, by plugging 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: tracezwwz"
plural "false"
caps "false"
noprefix "false"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: spew"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2} & =\mbx^{\top}\mbx-2\mu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\\
 & =\mbx^{\top}\mbx-2\mu(\mbx)^{\top}\mbW\mbx+\mu(\mbx)^{\top}\mbW^{\top}\mbW\mu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\\
 & =\|\mbx-\mbW\mu(\mbx)\|^{2}+C\\
\end{align*}

\end_inset

where 
\begin_inset Formula $C=tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)$
\end_inset

 that does not depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Section
A Toy Example to Demonstrate Out-of-distribution Behavior of Neural Networks
 
\begin_inset CommandInset label
LatexCommand label
name "app:rosenbrock"

\end_inset


\end_layout

\begin_layout Standard
Assume using a multilayer perceptron, we are trying to approximate the famous
 Rosenbrock function 
\begin_inset Formula $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$
\end_inset

 given 
\begin_inset Formula $(a,b)=(1,100)$
\end_inset

.
 In this small experiment, we sample tuples of two-dimensional points from
 a bounded region 
\begin_inset Formula $(x_{i},y_{i})\in[-1,3]\times[-2,3]$
\end_inset

.
 We use multilayer perceptron with six hidden layers and a hundred neurons
 in each layer.
 Half of the points are used in training and the other half is used as a
 validation set to optimize hyper-parameters.
 Using the trained network, we plot the actual Rosenbrock function along
 with the neural network approximation in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rosenbrock"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Notice how well the function is approximated for the region 
\begin_inset Formula $[-1,3]\times[-2,3]$
\end_inset

 but there is serious discrepancy between the approximated and the real
 outside of the region.
 This is a small yet to the point example of out-of-distribution issues
 with neural networks.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figs/rosenbrock.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Rosenbrock function (green surface) approximated by an MLP (red surface)
 given training (black crosses) and validation (black dots) samples form
 a bounded region 
\begin_inset Formula $(x_{i},y_{i})\in[-1,3]\times[-2,3]$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Rosenbrock"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Derived Testing Statistics for SPE
\change_inserted -1806609307 1589526886
 
\begin_inset CommandInset label
LatexCommand label
name "app:ere"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
Here, we define 
\begin_inset Formula $R(z)=\|y-g(z)\|^{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
E_{\mbz\sim q_{\theta}}R(\mbz) & =R(\mu_{z})+R'(\mbz)E[(\mbz-\mu(\mbx))]+R''(\mbz)\frac{1}{2}E[(\mbz-\mu(\mbx))^{\top}H_{z}(\mbz-\mu(\mbx))]\\
 & =R(\mu_{z})+R''(\mbz)\frac{1}{2}E[(\mbz-\mu(\mbx))^{\top}H_{z}(\mbz-\mu(\mbx))]\\
 & =R(\mu_{z})+\frac{1}{2}tr(H_{z}E[(\mbz-\mu_{z})(\mbz-\mu_{z})^{T}])\\
 & =R(\mu_{z})+\frac{1}{2}tr(H_{z}\Sigma_{z})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $\Sigma_{z}$
\end_inset

 is a diagonal matrix, 
\begin_inset Formula $tr(H_{z}S_{z})=tr(diag(H_{z})S_{z})=\sum_{i}(H_{z})_{ii}(S_{z})_{ii}$
\end_inset

, and only diagonal element of 
\begin_inset Formula $H_{z}$
\end_inset

 is needed, so it can be computed efficiently.
\end_layout

\end_body
\end_document
