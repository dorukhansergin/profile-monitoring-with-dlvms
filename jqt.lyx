#LyX file created by tex2lyx 2.3
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/hyan46/profile-monitoring-with-dlvms/
\textclass scrartcl
\begin_preamble
 



\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage{cleveref}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{stfloats}
\usepackage[super]{nth}
\usepackage{subcaption}
\usepackage{caption}
% for inkscape images
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgf}




\g@addto@macro\@floatboxreset\centering


% \usepackage{hyperref}
 
 \AtBeginDocument{% Overrides ref for Cref
 	\let\ref\Cref
 }

\crefalias{prop}{proposition}

% TIKZ
\usepackage{tikz}


% -- Arrows
\usetikzlibrary{arrows}

% --Bayesnet
\usetikzlibrary{bayesnet}
\usetikzlibrary{decorations.pathreplacing}

\tikzset{
	diagonal fill/.style 2 args={fill=#2, path picture={
			\fill[#1, sharp corners] (path picture bounding box.south west) -|
			(path picture bounding box.north east) -- cycle;}},
	reversed diagonal fill/.style 2 args={fill=#2, path picture={
			\fill[#1, sharp corners] (path picture bounding box.north west) |- 
			(path picture bounding box.south east) -- cycle;}}
}

\tikzstyle{partialobs} = [latent,diagonal fill={gray!25}{gray!0}]

%\@ifundefined{showcaptionsetup}{}{%
% \PassOptionsToPackage{caption=false}{subfig}}
%\usepackage{subfig}
%\makeatother

\DeclareFieldFormat[article,inbook,book,incollection,inproceedings,patent,thesis,unpublished]{citetitle}{#1}
\DeclareFieldFormat[article,inbook,incollection,inproceedings,patent,thesis,unpublished]{title}{#1} 

\providecommand{\propositionname}{Proposition}
\providecommand{\theoremname}{Theorem}



\end_preamble
\options headings=standardclasses
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding T1
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "mathptmx" "default"
\font_math "newtxmath" "auto"
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine biblatex
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\biblio_options isbn=false,url=false,eprint=false,minbibnames=10,maxbibnames=10,maxcitenames=3,firstinits=true
\biblatex_bibstyle chicago-authordate
\biblatex_citestyle chicago-authordate
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout
% INPUT PREAMBLES
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset CommandInset include
LatexCommand include
preview false
filename "preamble_glossary.lyx"

\end_inset

 
\end_layout

\begin_layout Title
Toward a Better Monitoring Statistic for Profile Monitoring with Variational Autoencoders
\end_layout

\begin_layout Abstract
Variational autoencoders have been recently proposed for the problem of process monitoring. While these works show impressive results over classical methods, the proposed monitoring statistics often ignore the inconsistencies in learned lower-dimensional representations and computational limitations in high-dimensional approximations. In this work, we first manifest these issues and then overcome them with a novel statistic formulation that increases out-of-control detection accuracy without compromising computational efficiency. We demonstrate our results on a simulation study with explicit control over latent variations, and a real-life example of image profiles obtained from a hot steel rolling process. 
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Abstract

\series bold
Keywords: 
\series default
 deep learning, high-dimensional nonlinear profile, latent variable model, profile monitoring, variational autoencoder 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:introduction"

\end_inset

 Profile monitoring has attracted a growing interest in the literature in the past decades for its ability to construct control charts with much better representations for certain types of process measurements 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "Woodall2004-bp,Woodall2007-xs,Maleki2018-uo"
literal "false"

\end_inset

. A profile can be defined as a functional relationship between the response variables and explanatory variables or spatiotemporal coordinates. In this work, we focus on the case where the profiles generated from the process are high-dimensional, 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
ie
\end_layout

\end_inset

, the number of such explanatory variables or spatiotemporal coordinates are large. Specifically, we focus on the case where profiles are observed in a high-dimensional space, but profile-to-profile variation lies on a nonlinear low-dimensional manifold. Our motivating example of such high-dimensional profiles is presented in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 below, in which we exhibit a sample of surface defect image profiles collected from a hot steel rolling process.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset Graphics 
	filename profile_examples.pdf
	width 50theight%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
A collection of 64 by 64 image profiles taken from a hot steel rolling process.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Rolling"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In literature, profile monitoring techniques can be categorized by their assumptions on the type of functional relationship. For example, linear profile monitoring techniques assumed that the profiles can be represented by a linear function. The idea is to extract the slope and the intercept from each profile and monitor its coefficients 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "zhu2009monitoring"
literal "false"

\end_inset

. Regularization techniques can also be used in linear profile estimation. For example, 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "zou2012lasso"
literal "false"

\end_inset

 utilizes a multivariate linear regression model for profiles with the LASSO penalty and use the regression coefficients for Phase-II monitoring. However, the linear assumption can be quite limiting. To address this challenge, nonlinear parametric models are normally proposed 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "Williams2007-ty,Jensen2009-tu,Noorossana2011-oj,Maleki2018-uo"
literal "false"

\end_inset

. These models assume an explicit family of parameterized functions and, their parameters are estimated via nonlinear regression. In both cases, the drawback of both linear and nonlinear parametric models is that they assume the parametric form is known beforehand, which might not always be the case in practice.
\end_layout

\begin_layout Standard
Another large body of profile monitoring research focuses on the type of profiles where the basis of the representation is assumed to be known, but the coefficients are unknown. For instance, to monitor smooth profiles, various non-parametric methods based on local kernel regression 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "zou2008monitoring,qiu2010nonparametric"
literal "false"

\end_inset

 and splines 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "chang2010statistical,Yan2018-ux"
literal "false"

\end_inset

 are developed. To monitor the non-smooth waveform signals, a wavelet-based mixed effect model is proposed 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "paynabar2011characterization"
literal "false"

\end_inset

. However, for all the aforementioned methods, it is assumed that the nonlinear variation pattern of the profile is well captured by a known basis or kernel. Usually, there is no guidance on selecting the right basis of the representation for the original data and it requires many trial and errors to find the right basis.
\end_layout

\begin_layout Standard
In the case that the basis of HD profiles are not known, dimensionality reduction techniques are widely used. Principal component analysis (PCA) is arguably the most popular method in this context for profile monitoring because of its simplicity, scalability, and good data compression capability. In 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "liu1995control"
literal "false"

\end_inset

, PCA is proposed to reduce the dimensionality of the streaming data where 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 charts are constructed to monitor the extracted representations and residuals, respectively. To generalize PCA methods to monitor the complex correlation among the channels of multi-channel profiles, 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "paynabar2015change"
literal "false"

\end_inset

 propose a multivariate functional PCA method and apply change point detection methods on the function coefficients. Along this line, tensor-based PCA methods are also proposed for multi-channel profiles, examples including uncorrelated multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "paynabar2013monitoring"
literal "false"

\end_inset

 and multi-linear PCA 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "grasso2014profile"
literal "false"

\end_inset

, and various tensor-based decomposition methods 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "yan2015image"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The main limitation of all the aforementioned PCA-related methods is that the expressive power of linear transformations is very limited. Furthermore, each principal component represents a global variation pattern of the original profiles, which is not efficient at capturing the local spatial correlation within a single profile. Therefore, PCA requires much larger latent-space dimensions than the dimension of the actual latent space, yielding a sub-optimal and overfitting-prone representation. This phenomenon hinders profile monitoring performance.
\end_layout

\begin_layout Standard
A systematic discussion of this issue is articulated in 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "Shi2016-tg"
literal "false"

\end_inset

. In that work, the authors identify the problems associated with assuming a closeness relationship in the subspace that is characterized by Euclidean metrics. They successfully observe that the intra-sample variation in complex high-dimensional corpora may lie on a nonlinear manifold as opposed to a linear manifold, which is assumed by PCA and related methods. However, the authors only focus on applying manifold learning for Phase-I analysis, while the Phase-II monitoring procedure is not touched upon.
\end_layout

\begin_layout Standard
In recent years, we observe a surge in deep learning-based solutions to the problem. For instance, deep autoencoders have been proposed for profile monitoring for Phase-I analysis in 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "Howard2018-op"
literal "false"

\end_inset

. In another work, 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "Yan2016-wa"
literal "false"

\end_inset

 compared the performance of contractive autoencoders and denoising autoencoders for Phase-II monitoring. 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "Zhang2018-js"
literal "false"

\end_inset

 proposed a denoising autoencoder for process monitoring. Aside from deterministic deep neural networks, only three works 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "wang2019systematic,Zhang2019-lu,lee2019process"
literal "false"

\end_inset

 proposed to use deep probabilistic latent variable models, specifically, variational autoencoders (VAE), for Phase-II monitoring. All the monitoring statistics in those works differ slightly, but they are all extensions of the classic 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

-charts of PCA. We argue that there is room for improvement for the monitoring statistic formulations in those works for several reasons, especially when high-dimensional profiles are considered. In this work, we propose a new monitoring statistic formulation to address this issue.
\end_layout

\begin_layout Standard
The contributions of this work are as follows: 
\end_layout

\begin_layout Itemize
We compare the existing monitoring statistics proposed by previous works on VAE-based monitoring and unify them into the latent-space and residual-space monitoring statistics. We also prove the mathematical equivalency of these statistics with the classical 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

-charts of PCA in the linear setting. 
\end_layout

\begin_layout Itemize
We highlight an important shortcoming of neural network-based encoders and how it negatively impacts the efficiency of statistics that are derived exclusively from learned latent representations. We demonstrate this on a carefully designed simulation study with explicit control over the actual latent variations. 
\end_layout

\begin_layout Itemize
We explain why residual-space monitoring statistics can cover most types of process drifts in both conceptual illustration and real simulation study. 
\end_layout

\begin_layout Itemize
We propose two approximations on the residual-space monitoring statistics leveraging on the first-order and second-order Taylor expansion that strikes a better balance between detection accuracy and computational feasibility than previously proposed similar statistics. 
\end_layout

\begin_layout Itemize
We support our claims on both simulation and real-life case study profile datasets. 
\end_layout

\begin_layout Standard
The rest of the paper is organized as follows: 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Background"
plural "false"
caps "false"
noprefix "false"

\end_inset

 first introduces variational autoencoders and reviews traditional 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 charts of PCA as well as the existing monitoring statistics proposed for VAE. 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology"
plural "false"
caps "false"
noprefix "false"

\end_inset

 introduces our proposed monitoring statistic formulation and the rationale behind how it tackles the shortcomings of existing formulations. 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Simulation-Study-Analysis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 introduces the simulation process used in this work as well as the manifestations of the aforementioned shortcomings. Finally, 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:case-study"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates the advantages of the proposed methodology on a real-life case study, using images from a hot-steel rolling process.
\end_layout

\begin_layout Section
Background 
\begin_inset CommandInset label
LatexCommand label
name "sec:Background"

\end_inset


\end_layout

\begin_layout Standard
In this section, we review the variational autoencoder (VAE) in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:lvms"
plural "false"
caps "false"
noprefix "false"

\end_inset

. We will then review the 
\begin_inset Formula $T^2$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics for PCA methods in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:ReviewPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Finally, we will briefly review the existing works profile monitoring works utilizing the VAE in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Variational Autoencoders 
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:lvms"

\end_inset


\end_layout

\begin_layout Standard
We will first review the variational autoencoder (VAE), which was first introduced by 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "Kingma2013-dl"
literal "false"

\end_inset

. VAE soon became one of the most prominent probabilistic models in the literature. The Gaussian factorized latent variable model perspective of VAEs is crucial to understand the role of this model in the context of profile monitoring. This is why we begin with an introduction to latent variable modeling.
\end_layout

\begin_layout Standard
Let us assume we observe samples 
\begin_inset Formula $\mbx\in\R^{d}$
\end_inset

 in a high-dimensional space, generated by a random multivariate process that can be described by the density function 
\begin_inset Formula $ p(\mbx) $
\end_inset

. We also believe that there is redundancy in this observation and sample-to-sample variation can be explained well by a latent representation 
\begin_inset Formula $\mbz\in\R^{r}$
\end_inset

, where the latent dimension 
\begin_inset Formula $ r \ll d $
\end_inset

. Latent variable models are powerful tools to model such complex distributions. The joint density 
\begin_inset Formula $ p(\mbx,\mbz) $
\end_inset

 is factorized into the distribution of the latent variables 
\begin_inset Formula $\pz$
\end_inset

 and the conditional distribution of observed variables given latent variables 
\begin_inset Formula $p(\mbx\g\mbz)$
\end_inset

. A typical example of latent variable models is when the joint distribution is Gaussian factorized as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

. 
\begin_inset Formula \begin{equation}
\begin{split}\pz & =\Norm(\mbz;0,\mbI_{r})\\
\decoding & =\Norm(\mbx;\mu_{\mbtheta}(\mbz),\sigma^{2}\mbI_{d})\\
p_{\mbtheta}(\mbx,\mbz) & =\decoding\pz
\end{split}
\label{eq:gaussian-factorized}
\end{equation}
\end_inset

In the above formulation, the function 
\begin_inset Formula $\mu_{\mbtheta}\colon\R^{r}\to\R^{d}$
\end_inset

 is a function parameterized by 
\begin_inset Formula $\mbtheta$
\end_inset

, which describes the relationship between the latent variables and the mean of the conditional distribution. The Gaussian prior 
\begin_inset Formula $\pz$
\end_inset

 is typically chosen to be standard multivariate Gaussian distribution to avoid degenerate solutions 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "roweis1999unifying"
literal "false"

\end_inset

 and conditional covariance is typically assumed to be isotropic 
\begin_inset Formula $\sigma^{2}I_{d}$
\end_inset

 to avoid ill-defined problems. The aim is to approximate the true density 
\begin_inset Formula $p_{\mbtheta}(\mbx)\approx p(\mbx)$
\end_inset

 and this approximation can be obtained through marginalization: 
\begin_inset Formula \[
p_{\mbtheta}(\mbx)=\int p_{\mbtheta}(\mbx,\mbz)d\mbz
\]
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
% Finally, in literature, there have been discussions about whether the independent latent structure assumption $pz=
\backslash
Norm(
\backslash
mbz;0,
\backslash
mbI_{r})$ can lead to the discovery of the true disentangled variations, a task also known as disentangled representation learning 
\backslash
parencite[Sec. 3.5]{bengio2013representation}. Disentangled representations are useful to represent variations in latent variations due to their ability to separate the independent factors. However, a discussion on what disentangled representation implies for profile monitoring is necessary. We are interested in whether and if so, how such representations will be critical for profile monitoring.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
% I think we delete all about disentanglement
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A famous member of the family of models described above is the probabilistic principal component analysis (PPCA) 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "tipping1999probabilistic"
literal "false"

\end_inset

. The parameters are optimized via a maximum likelihood estimation framework and it can be solved analytically since the function 
\begin_inset Formula $\mu_{\mbtheta}$
\end_inset

 is a simple linear transformation. This enables reusing analytical results from solutions to the classical PCA problem. The assumption of PPCA that the latent and observed variables have a strictly linear relationship is restrictive. In real-world processes, this relationship is likely highly nonlinear. Deep latent variable models are a marriage of deep neural networks and latent variable models that aim to solve this problem. Deep learning has enjoyed a tremendous resurgence in the last decade due to their superior performance that was unprecedented for many tasks such as image classification 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "krizhevsky2012imagenet"
literal "false"

\end_inset

, machine translation 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "bahdanau2014neural"
literal "false"

\end_inset

, and speech recognition 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "amodei2016deep"
literal "false"

\end_inset

. In theory, under sufficient conditions, a two-layer multilayer perceptron can approximate any function on a bounded region 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "cybenko1989approximation,Hornik1991-li"
literal "false"

\end_inset

. However, growing the width of shallow networks exponentially for arbitrarily complex tasks is not practical. It has been shown that deeper representations can often achieve better expressive power than shallow networks with fewer parameters due to the efficient reuse of the previous layers 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "eldan2016power"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
VAE is arguably the most foundational member of the deep latent variable model family. The main difference between PPCA and VAE is that VAE replaces the linear transformation with a high-capacity deep neural network (called 
\shape italic
generative
\shape default
 or 
\shape italic
decoder
\shape default
). This is powerful in the sense that, along with a general purpose prior 
\begin_inset Formula $\pz$
\end_inset

, deep neural networks can transform such prior to model a wide variety of densities to model the training data 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "kingma2019introduction"
literal "false"

\end_inset

. Unlike PPCA, these models will not have analytical solutions due to the complex nature of the neural network used. Like most other deep learning models, their parameters are often optimized via variants of stochastic gradient descent optimizers. The problem becomes even harder given that the posterior 
\begin_inset Formula $\decoding$
\end_inset

 takes meaningful values only for a small sub-region within the latent space 
\begin_inset Formula $\R^{r}$
\end_inset

. This makes sampling from the prior 
\begin_inset Formula $\pz$
\end_inset

 to estimate the likelihood prohibitively expensive. Both models work around this problem using the importance sampling framework 
\begin_inset CommandInset citation
LatexCommand citep
after "532"
before ""
key "bishop2006pattern"
literal "false"

\end_inset

, where they introduce another network (called 
\shape italic
recognition
\shape default
 or 
\shape italic
encoder
\shape default
) to approximate a proposal distribution 
\begin_inset Formula $\encoding$
\end_inset

 —parametrized by 
\begin_inset Formula $\mbphi$
\end_inset

— which aims to sample latent variables from a much smaller region that is more likely to produce higher posterior densities for a given input 
\begin_inset Formula $\mbx$
\end_inset

. The encoder is modeled as another Gaussian distribution 
\begin_inset Formula $\encoding = \Norm(\mbz;\mu_{\mbphi}(\mbx),\sigma_{\mbphi}(\mbx))$
\end_inset

 where the mean and standard deviation of the proposal distribution are inferred via high capacity neural networks 
\begin_inset Formula $\mu_{\mbphi}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\mbphi}$
\end_inset

, respectively.
\end_layout

\begin_layout Standard
One important output of a trained VAE is the likelihood estimator. Once the two networks are trained, the log-likelihood 
\begin_inset Formula $\log\ptheta(\mbx)$
\end_inset

 can be approximated by a Monte Carlo sampling procedure with 
\begin_inset Formula $L$
\end_inset

 iterations 
\begin_inset CommandInset citation
LatexCommand citep
after "30"
before ""
key "kingma2019introduction"
literal "false"

\end_inset

: 
\begin_inset Formula \begin{equation}
\log\ptheta(\mbx)\approx\log\frac{1}{L}\sum_{l=1}^{L}\frac{\ptheta(\mbx,\mbz^{(l)})}{\qphizgivenx{\mbz^{(l)}}{\mbx}}.\label{eqn:SummationLL}
\end{equation}
\end_inset


\end_layout

\begin_layout Standard
However, the Monte Carlo sampling procedure is shown to be computationally inefficient and the evidence lower bound (ELBO), which is deemed a proxy to the likelihood, is often used as the objective to be optimized. 
\begin_inset Formula \begin{equation}
\begin{split}\text{ELBO} & \triangleq\log\left(p(\mbx)\right)-\KL{\encoding}{q^{*}(\mbz|\mbx)}\\
 & =\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)},
\end{split}
\label{eqn:VAELoss}
\end{equation}
\end_inset

In the equation above, 
\begin_inset Formula $\KL{\cdot}{\cdot}$
\end_inset

 denotes the Kullback-Leibler divergence (KLD) between two distributions. The left-hand side is the quantity of interest, while the right-hand side is the tractable expression that guides the updating of parameters 
\begin_inset Formula $\mbtheta,\mbphi$
\end_inset

 in an end-to-end fashion.
\end_layout

\begin_layout Subsection
Review of 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 Statistics in PCA 
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:ReviewPCA"

\end_inset


\end_layout

\begin_layout Standard
We will then review the profile monitoring statistics in the principal component analysis (PCA). Profile monitoring via PCA is typically done using the 
\begin_inset Formula $\Tsq$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "Chen2004-px"
literal "false"

\end_inset

. The 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA is defined as the reconstruction error between the observed profile 
\begin_inset Formula $\mbx$
\end_inset

 and the reconstructed profile 
\begin_inset Formula $\tilde{\mbx}$
\end_inset

. The geometric interpretation of 
\begin_inset Formula $ Q $
\end_inset

 statistics is that it quantifies how far the sample is away from the learned subspace of in-control samples. 
\begin_inset Formula $\Tsq$
\end_inset

 statistics on the other hand, quantifies the shift along the directions of the most dominant principal components.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\Tsq$
\end_inset

 statistic and 
\begin_inset Formula $Q$
\end_inset

 statistic for PCA are defined formally as follows: 
\begin_inset Formula \begin{equation}
\begin{split}Q(\mbx) & =\gg\mbx-\tilde{\mbx}\gg^{2}\\
\Tsq(\mbx) & =\mbz^{\top}\mbSigma\inv_{r}\mbz=\mbx^{\top}\mbW_{r}\mbSigma\inv_{r}\mbW_{r}^{\top}\mbx,
\end{split}
\label{eqn: QTPCA}
\end{equation}
\end_inset

where matrix 
\begin_inset Formula $\mbW_{r}$
\end_inset

 is the loading matrix, and 
\begin_inset Formula $\mbSigma\inv_{r}$
\end_inset

 is the inverse of the covariance matrix when only the first 
\begin_inset Formula $r$
\end_inset

 principal components are kept. There are various methods to choose 
\begin_inset Formula $r$
\end_inset

 such as fixing the percentage of variation explained 
\begin_inset CommandInset citation
LatexCommand citep
after "41"
before ""
key "Chiang2001-nu"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
For processes with relatively small latent and residual dimensionality, the upper control limits of these statistics for the 
\begin_inset Formula $\alpha$
\end_inset

% Type-1 error tolerance is constructed by employing the normality assumptions of PPCA 
\begin_inset CommandInset citation
LatexCommand citep
after "43-44"
before ""
key "Chiang2001-nu"
literal "false"

\end_inset

. However, using such measures for high-dimensional nonlinear profiles is prohibitively error-prone as both 
\begin_inset Formula $r$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

 will be much higher than the assumptions of chi-square distribution can tolerate. As an alternative, non-parametric methods are typically used to estimate these limits, such as simple percentiles or kernel density estimators.
\end_layout

\begin_layout Subsection
Review of Previously Proposed Monitoring Statistics Proposed for VAE 
\begin_inset CommandInset label
LatexCommand label
name "sec:bckgrnd:critique"

\end_inset

 
\end_layout

\begin_layout Standard
In this section, we will briefly review several proposed monitoring statistics for variational autoencoders (VAE).s Three works have recently considered VAE for process monitoring, all of which propose different statistic formulations for monitoring. 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "Zhang2019-lu"
literal "false"

\end_inset

 propose 
\begin_inset Formula $H^{2}$
\end_inset

, which is basically the Mahalanobis distance of the mean of the proposal distribution from standard Gaussian distribution. 
\begin_inset Formula \begin{equation}
H^{2}=\mu_{\mbphi}(\mbx)^{\top}\mu_{\mbphi}(\mbx)
\end{equation}
\end_inset


\end_layout

\begin_layout Standard
In another work, 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "lee2019process"
literal "false"

\end_inset

 propose two statistics: 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $SPE$
\end_inset

. For a given input 
\begin_inset Formula $\mbx$
\end_inset

, a single sample is drawn from the proposal distribution 
\begin_inset Formula $\mbz^{(l)}\sim\encoding$
\end_inset

 which is used reconstruct the input using the generative model 
\begin_inset Formula $\mbx^{(l)}\sim p_{\mbtheta}(\mbx\g\mbz^{(l)})$
\end_inset

. The proposed test statistics in this work can be formalized as follows: 
\begin_inset Formula \begin{equation}
\begin{aligned}T^{2} & =(\mbz^{(l)}-\bar{\mbz})^{\top}S_{\mbz}\inv(\mbz^{(l)}-\bar{\mbz})\\
SPE & =\gg\mbx^{(l)}-\mbx\gg_{2}^{2},
\end{aligned}
\end{equation}
\end_inset

where 
\begin_inset Formula $\bar{\mbz}$
\end_inset

 and 
\begin_inset Formula $S_{\mbz}\inv$
\end_inset

 are estimated over a single pass of the entire set of in-control samples. In their methodology, these two statistics work in combination and at least one positive decision from either of the two statistics is enough to claim that the process is out-of-control.
\end_layout

\begin_layout Standard
Finally, 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "wang2019systematic"
literal "false"

\end_inset

 propose the 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 statistics by focusing on the two major components of the tractable part of the objective function of VAE shown as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The 
\begin_inset Formula $D$
\end_inset

 statistic is simply the KL divergence between the prior and proposal. For 
\begin_inset Formula $R$
\end_inset

 statistic, like 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "lee2019process"
literal "false"

\end_inset

, they employ summary statistics over samples from proposal but also claim that sampling size can be fixed to one: 
\begin_inset Formula \begin{equation}
\begin{aligned}D & =\KL{\encoding}{p(\mbz)}\\
R & =\frac{1}{L}\sum_{l=1}^{L}-\log q_{\mbtheta}(\mbx\g\mbz^{(l)}),
\end{aligned}
\label{eq: DR}
\end{equation}
\end_inset


\end_layout

\begin_layout Standard

\begin_inset Formula $SPE$
\end_inset

 in and 
\begin_inset Formula $R$
\end_inset

 are essentially the same quantities up to a constant, which makes them identical in the context of monitoring. This is why we will refer to them as 
\begin_inset Formula $SPE/R$
\end_inset

 throughout the rest of the paper.
\end_layout

\begin_layout Section
Methodology 
\begin_inset CommandInset label
LatexCommand label
name "sec:methodology"

\end_inset


\end_layout

\begin_layout Standard
In this section, we start by explaining how previously proposed statistics for VAE-based monitoring are modeled as extensions of their PCA-based monitoring counterparts, in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:residual-latent-vae-pca"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Then, we will reveal the pitfalls of this extension concerning the behaviors of neural networks in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Against the backdrop of these pitfalls, we will propose a novel monitoring statistic formulation. Lastly, we will outline the implementation details of profile monitoring procedures and neural network architectures we use in this study in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-Architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

, respectively.
\end_layout

\begin_layout Subsection
Relationship of the Monitoring Statistics for VAE and PCA
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:residual-latent-vae-pca"

\end_inset


\end_layout

\begin_layout Standard
A common approach in the literature to tackle process monitoring with VAE is to extend the definitions of 
\begin_inset Formula $ \Tsq $
\end_inset

 and 
\begin_inset Formula $ Q $
\end_inset

 statistics of the PCA-based monitoring to VAE. This is done by breaking the tractable portion of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eqn:VAELoss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 into two term as follows:
\end_layout

\begin_layout Standard

\begin_inset Formula \begin{equation}
Q_{VAE}  = \E_{\mbz\sim q_{\mbtheta}}\log\decoding, 
T^2_{VAE} = \KL{\encoding}{p(\mbz)}.  \label{eq: TQVAE}
\end{equation}
\end_inset


\end_layout

\begin_layout Standard
Either these formulations or some variant of them are typically used as the monitoring statistics. To understand the rationale behind this, we will revisit the assumptions of the model described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-factorized"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Let us formally represent an out-of-control distribution as a shift in 
\begin_inset Formula $p(\mbx)$
\end_inset

. Since 
\begin_inset Formula $p(\mbx)=\int p(\mbx\g\mbz)p(\mbz)d\mbz$
\end_inset

, we can anticipate two sources: a shift in the latent distribution 
\begin_inset Formula $\pz$
\end_inset

 or a shift in the residual distribution 
\begin_inset Formula $p(\mbx\g\mbz)$
\end_inset

. The two statistics are assumed to be connected to these two sources: 1) a shift in the conditional distribution 
\begin_inset Formula $ p(\mbx\g\mbz) $
\end_inset

 can be detected monitoring 
\begin_inset Formula $Q_{VAE}=\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

 and 2) a shift in the latent distribution 
\begin_inset Formula $p(\mbz)$
\end_inset

, can be detected monitoring 
\begin_inset Formula $T^2_{VAE}=\KL{\encoding}{p(\mbz)}$
\end_inset

.
\end_layout

\begin_layout Standard
This idea is similar to utilizing both 
\begin_inset Formula $T^2$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

-charts in the PCA-based method, where both terms play an important role in process monitoring 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "kim2003process"
literal "false"

\end_inset

. To make this similarity more obvious we prove that if the same ELBO framework for VAE used above is used for PPCA (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:ReviewPCA"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we prove the equivalency of 
\begin_inset Formula $T^{2}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 statistics of PPCA and 
\begin_inset Formula $T^2_{VAE}$
\end_inset

 and 
\begin_inset Formula $Q_{VAE}$
\end_inset

 of VAE in the linear settings.
\end_layout

\begin_layout Proposition

\begin_inset CommandInset label
LatexCommand label
name "prop: T2Q"

\end_inset

 If we use linear decoders for VAE, the models will become the Probabilistic PCA 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "tipping1999probabilistic"
literal "false"

\end_inset

 that the prior and decoding functions are normally distributed as: 
\begin_inset Formula \[
	\begin{split}p(\mbz) & =\Norm(0,\mbI),\\
		\decoding & =\Norm(\mbW\mbz,\sigma^{2}\mbI).\label{eq:Gaussian}
	\end{split}
	\]
\end_inset

In this case, the encoder can be solved analytically as another normal distribution as 
\begin_inset Formula $\encoding=\Norm(\mu_{\mbphi}(\mbx),\Sigma_{z})$
\end_inset

, where 
\begin_inset Formula $\mu_{\mbphi}(\mbx)=\mbM^{-1}\mbW^{\top}\mbx$
\end_inset

, 
\begin_inset Formula $\Sigma_{z}=\sigma^{2}\mbM^{-1}$
\end_inset

, and 
\begin_inset Formula $\mbM=\mbW^{\top}\mbW+\sigma^{2}\mbI$
\end_inset

. Then, the two monitoring statistics defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: TQVAE"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be derived as follows: 
\begin_inset Formula \begin{equation}
		\KL{\encoding}{p(\mbz)}=\frac{1}{2}\gg\mu_{\mbphi}(\mbx)\gg^{2}+C_{1},\label{eqn:KL_PPCA}
	\end{equation}
\end_inset


\begin_inset Formula \begin{equation}
		\E_{\mbz\sim q_{\mbphi}}\log\decoding\propto\gg\mbx-\mbW\mu_{\mbphi}(\mbx)\gg^{2}+C_{2},\label{eqn:E_PPCA}
	\end{equation}
\end_inset

where 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{2}$
\end_inset

 are constants that do not depend on 
\begin_inset Formula $x$
\end_inset

. The proof is given in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:PoofOfPropTQ"
plural "false"
caps "false"
noprefix "false"

\end_inset

. 
\end_layout

\begin_layout Standard
Note that the constants do not affect the profile monitoring decision as the control limits will be translated accordingly. Thus, the test statistic 
\begin_inset Formula $T^2_{VAE}$
\end_inset

 and 
\begin_inset Formula $Q_{VAE}$
\end_inset

 for linear decoders (i.e., PPCA) is equivalent to the 
\begin_inset Formula $T^{2}$
\end_inset

-statistic and 
\begin_inset Formula $Q$
\end_inset

-statistic of PCA, respectively. residual-space
\end_layout

\begin_layout Standard
Observe that previously proposed formulations mentioned in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

 draw inspiration—directly or indirectly—from this framework. Statistics 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $SPE$
\end_inset

 are based on the 
\begin_inset Formula $Q$
\end_inset

-statistic. Let us call these 
\emph on
residual-space statistics
\emph default
, as they rely on the sum of squared differences between the signal itself and its predicted value, 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
ie
\end_layout

\end_inset

, residuals. The statistics 
\begin_inset Formula $H^{2},T^{2}$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 are based on the 
\begin_inset Formula $T^{2}$
\end_inset

 of PCA. We call these 
\emph on
latent-space statistics
\emph default
, as they rely exclusively on latent representations.
\end_layout

\begin_layout Standard

\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pcaVSvae"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows a graphical illustration of this analogy of residual-space statistics and latent-space statistics for PCA and VAE. Residual-space statistics quantifies the distance of the observed data with respect to the learned linear or nonlinear manifold. The latent-space statistics monitors the distance within the learned manifold. In the linear case (i.e., PCA), this is the Euclidean distance. However, in the nonlinear case (i.e., VAE), this distance should be defined on the nonlinear manifold.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard
\align center

\begin_inset Graphics 
	filename PCAvsVAE.pdf
	width 90text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Illustration of the analogy between PCA and VAE. Closed regions describe the lower-dimensional manifold the in-control distribution lies in. The crosses represent the in-control samples observed in Phase-I and the gray region represents the subset of the lower-dimensional manifold where in-control samples are typically sampled from. The observation represented with a circle is typically detected with 
\begin_inset Formula $ Q $
\end_inset

-statistic and the observation represented with a triangle is typically detected with 
\begin_inset Formula $ \Tsq $
\end_inset

-statistic. 
\begin_inset CommandInset label
LatexCommand label
name "fig:pcaVSvae"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Proposed Monitoring Statistic
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:proposed-statistic"

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard
\align center

\begin_inset Graphics 
	filename Disentangled_Extrapolated.pdf
	width 90text%

\end_inset

 
\end_layout

\begin_layout Standard

\begin_inset Caption Standard

\begin_layout Plain Layout
Illustration of incorrect latent mappings phenomena and how process control fails in latent space. 
\series bold
Bottom left:
\series default
 The true latent variations of in-control samples are generated from the gray region, which is the probable region. Point A and Point B are extreme values along a dimension of variation. Point C is generated by an out-of-control process with a shift in latent distribution. Point D is generated by an out-of-control process with a shift in the residual distribution. The predicted counterpart of each point is denoted by an apostrophe (
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
eg
\end_layout

\end_inset

, A' for A). 
\series bold
Top Left:
\series default
 Observations of true latent variation in the high-dimensional space that lie close to a low-dimensional manifold. 
\series bold
Top Middle:
\series default
 The encoder and decoder of VAE trained exclusively with in-control samples (
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
ie
\end_layout

\end_inset

, the gray region in the observed space). 
\series bold
Bottom Middle:
\series default
 Incorrectly mapped variation in the predicted latent space where the gray region is the probable region. 
\series bold
Top Right:
\series default
 Reconstructions of the variation in high-dimensions, with a failure in extrapolation beyond the in-control region. 
\begin_inset CommandInset label
LatexCommand label
name "fig:entang-extrap"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this section, we will first reveal the shortcomings of the previously proposed VAE-based monitoring methodologies we present in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bckgrnd:critique"
plural "false"
caps "false"
noprefix "false"

\end_inset

. This will lead us to the rationale behind the design of our proposed statistic, which is also included in this section after the explanation of the shortcomings.
\end_layout

\begin_layout Standard
There are two major pitfalls of the previously proposed methodologies: 
\end_layout

\begin_layout Enumerate
Latent-space statistics 
\begin_inset Formula $H^{2},T^{2}$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 or any other formulation that relies exclusively on the latent representation 
\begin_inset Formula $ \mbz \sim \qphizgivenx{\mbz}{\mbx}$
\end_inset

 will be unreliable for process monitoring. Thus, they should be discarded altogether from the monitoring framework since they will likely increase false alarms without contributing to the detection power in any meaningful way. 
\end_layout

\begin_layout Enumerate
Residual-space statistic 
\begin_inset Formula $ SPE $
\end_inset

 and 
\begin_inset Formula $ R $
\end_inset

 rely on Monte Carlo sampling. These are not computationally feasible given how expensive calculations are on deep neural networks. An alternative approach is required to stay computationally feasible without sacrificing too much from the estimation quality of these statistics. 
\end_layout

\begin_layout Standard
We will address these two shortcomings in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:unreliable"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:efficiency"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Unreliability of Latent-space Statistics for Deep Autoencoders
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:unreliable"

\end_inset


\end_layout

\begin_layout Standard
First, we focus on the unreliability of latent-space statistics. Let us first start with the case when the shift occurs in the latent distribution (i.e., 
\begin_inset Formula $\pz$
\end_inset

). According to the PCA-VAE analogy illustrated in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pcaVSvae"
plural "false"
caps "false"
noprefix "false"

\end_inset

, latent-space statistics are supposed to catch such shifts, which are represented with triangular points in the same figure. While this may work for PCA-based monitoring, we claim that such an analogy cannot be straightforwardly made for VAE because neural network-based encoders in autoencoder architectures typically learn incorrect mappings of actual latent variations. We illustrate this phenomenon in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:entang-extrap"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The line segment ABC illustrates a traversal along a latent dimension. All the samples generated along the line segment AB are sampled from the typical region of the in-control process and their mappings are contained within the typical region of the predicted space. However, Point C is generated by an out-of-control process where there is a shift in the latent distribution but its mapping incorrectly falls within the probable region. This leads to false evidence which suggests that Point C is unlikely to be generated by an out-of-control process while in reality, it was.
\end_layout

\begin_layout Standard
The reasons as to why incorrect mappings are learned by deep autoencoders have been studied well in the deep learning literature. Interested readers are encouraged to refer to 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "AchilleS18"
literal "false"

\end_inset

 for a discussion of the properties of ideal latent representations and to 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "locatello2018challenging"
literal "false"

\end_inset

 for a discussion of the challenges around attaining one of these properties, namely, disentanglement. The key takeaway is that it is very likely that we end up with an imperfect mapping, especially with real-life datasets. Consequently, in Phase-II, samples generated by out-of-control processes that are characterized by a shift in the latent distribution will not be mapped consistently to the regions in the latent space, which we consider to be unusual. This will result in an increased type-II error.
\end_layout

\begin_layout Standard
A natural question to ask at this point is how should we expect to detect shifts in latent distribution if we cannot rely on latent representations. We argue that the residual-space statistics (i.e, an analogue of a 
\begin_inset Formula $ Q $
\end_inset

-chart) would catch such shifts too, even though its original purpose is to catch shifts in the residual space. Our argument is based on another 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

shortcoming
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

 of neural networks, namely, failure to extrapolate. Deep neural networks approximate well only at a bounded domain defined by where the training set is densely samples from. In the context of our problem, this refers to the high-density region of 
\begin_inset Formula $ p(\mbx) $
\end_inset

, which generated the set of in-control profiles we use in Phase-I. The behavior of the function is unpredictable and often erroneous outside the training domain. In other words, it does not extrapolate well beyond the domain of training samples, which are likely to be coming from out-of-control processes. We refer interested readers to 
\begin_inset CommandInset ref
LatexCommand ref
reference "app:rosenbrock"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where we replicate this phenomenon on a toy example.
\end_layout

\begin_layout Standard
A decoder that fails to extrapolate is counter-intuitively helpful for the residual-space statistics since it will struggle with generating profiles that are in the low-density region of the in-control data distribution 
\begin_inset Formula $ p(\mbx) $
\end_inset

. This means that the discrepancy between the true profile and its generated counterpart will be larger for out-of-control profiles than it is for in-control profiles, regardless of the source of the shift. Overall, we conclude that the residual-space monitoring statistic would be efficient at detecting changes in the residual space and latent space. We refer the readers to 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:entang-extrap"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for an illustration. Point C is generated from a shift in the latent space distribution. However, due to the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

incorrect
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

 mapping of the latent distribution, Point C' will still lie in the in-control region of the latent space. There is a significant discrepancy between the Point C and reconstruction C', which can be detected by the residual-space statistics. Point D is generated from a shift in the residual space and can be captured by the residual space statistics. In conclusion, the residual-space statistic should be able to catch both types of shifts.
\end_layout

\begin_layout Subsubsection
Improving the Computational Efficiency of the residual-space Statistics
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:efficiency"

\end_inset


\end_layout

\begin_layout Standard
Now that we established our rationale behind the first shortcoming we claim to reveal, we move onto the second and focus on the previously proposed residual-based statistics: 
\begin_inset Formula $ SPE $
\end_inset

 and 
\begin_inset Formula $ R $
\end_inset

. Both 
\begin_inset Formula $SPE$
\end_inset

 and 
\begin_inset Formula $ R $
\end_inset

 rely on samples from the proposal distribution for the estimation of the expectation. This approach requires a large number of samples to be generated, and thus a large number of the forward passes through the decoder network, which is prohibitively expensive in terms of computation when deployed in real-life systems. To overcome this problem, we propose a Taylor expansion based approximation. First, observe that 
\begin_inset Formula $\log\decoding\propto\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}+C$
\end_inset

 for all 
\begin_inset Formula $\mbx$
\end_inset

 and 
\begin_inset Formula $\mbz$
\end_inset

 because of the common isotropic covariance assumption. The constant 
\begin_inset Formula $C$
\end_inset

 can be discarded as noneffective in terms of control charting because it would only translate the limits and the statistics by the same amount for any given 
\begin_inset Formula $\mbx$
\end_inset

 and 
\begin_inset Formula $\mbz$
\end_inset

. We call the expression 
\begin_inset Formula $\E_{\mbz\sim q_{\phi}}\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset

 as the expected reconstruction error (ERE). The Taylor expansion for the first-order and second-order moment of ERE given the random variable 
\begin_inset Formula $\mbz\sim\encoding$
\end_inset

 can be derived analytically. 
\end_layout

\begin_layout Proposition

\begin_inset CommandInset label
LatexCommand label
name "prop:taylor-exp"

\end_inset

 Assume that a VAE is trained with in-control samples. The training results in the mean and diagonal covariance estimators of the proposal distribution as well as the mean estimator of the condition distribution which are denoted by 
\begin_inset Formula $ \mbmu_{\mbphi} $
\end_inset

,
\begin_inset Formula $ \mbsigma_{\mbphi} $
\end_inset

, and 
\begin_inset Formula $ \mbmu_{\mbtheta} $
\end_inset

, respectively. The first and second-order Taylor Expansion (denoted by 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 respectively) for the function 
\begin_inset Formula $\E_{\mbz\sim q_{\phi}}\gg \mbx-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset

 given the random variable 
\begin_inset Formula $\mbz\sim\encoding=\Norm(\mu_{\mbphi}(\mbx),\mbsigma_{\mbphi}(\mbz))$
\end_inset

 and where the conditional 
\begin_inset Formula $\decoding=\Norm(\mu_{\mbphi}(\mbx),\diag(\mbsigma_{\mbphi}(x)))$
\end_inset

 can be derived analytically as: 
\begin_inset Formula \begin{equation}
ERE_{1}=\gg\mbx-\mbmu_{\mbtheta}(\mbmu_{\mbphi}(\mbx))\gg_{2}^{2}\label{eq:ere-1}
\end{equation}
\end_inset


\begin_inset Formula \begin{equation}
ERE_{2}=\gg\mbx-\mbmu_{\mbtheta}(\mbmu_{\mbphi}(\mbx))\gg_{2}^{2}+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\diag(\mbsigma_{\mbphi}(x)))\label{eq:ere-2}
\end{equation}
\end_inset

where 
\begin_inset Formula $\mathbf{H}_{z}$
\end_inset

 is the Hessian of the function 
\begin_inset Formula $\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
\end_inset

 with respect to 
\begin_inset Formula $\mbz$
\end_inset

. The derivation is provided in 
\begin_inset CommandInset ref
LatexCommand ref
reference "app:ere"
plural "false"
caps "false"
noprefix "false"

\end_inset

. 
\end_layout

\begin_layout Standard
Given a trained VAE, 
\begin_inset Formula $ERE_{1}$
\end_inset

 can be computed efficiently by a single forward pass of the new profile from the pass 
\begin_inset Formula $\mbx$
\end_inset

 through 
\begin_inset Formula $\mbmu_{\mbphi}$
\end_inset

 and 
\begin_inset Formula $\mbmu_{\mbtheta}$
\end_inset

 successively and calculating the squared prediction error, without the need for any sampling. 
\begin_inset Formula $ERE_{2}$
\end_inset

 requires the additional computation of the diagonal of the Hessian 
\begin_inset Formula $\mathbf{H}_{z}$
\end_inset

 and a relatively less expensive trace operation since the covariance is diagonal. Both 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 are residual based statistics that are accurate and efficient to compute, which addresses the two shortcomings we mentioned at the beginning of this section. In our experiments, we will evaluate the effectiveness of both of these statistics in comparison to previously proposed monitoring statistics for VAE.
\end_layout

\begin_layout Subsection
Profile Monitoring Procedure
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:methodology:procedure"

\end_inset

 A typical profile monitoring follows two phases: Phase-I analysis and Phase-II analysis. Phase-I analysis focuses on understanding the process variability by training an appropriate in-control mode and selecting an appropriate control limit. In our case, Phase-I analysis results in a trained model (i.e., an encoder and a decoder) and an Upper Control Limit (UCL) to help set up the control chart for each of the monitoring statistics. In Phase-II, the system is exposed to new profiles generated by the process in real-time to decide whether these profiles are in-control or out-of-control. Our experimentation plan, outlined below, is formulated to emulate this scenario to effectively assess the performance of any combination of a model, a test statistic, and a disturbance scenario to generate the out-of-control samples. 
\end_layout

\begin_layout Itemize
Obtain in-control dataset 
\begin_inset Formula $\dataset$
\end_inset

 and partition it into train, validation and test sets 
\begin_inset Formula $\dataset^{trn}$
\end_inset

, 
\begin_inset Formula $\dataset^{val}$
\end_inset

, 
\begin_inset Formula $\dataset^{tst}$
\end_inset

. 
\end_layout

\begin_layout Itemize
Train VAE using samples from 
\begin_inset Formula $\dataset^{trn}$
\end_inset

. 
\end_layout

\begin_layout Itemize
Calculate test statistic for all 
\begin_inset Formula $\mbx\in\dataset^{val}$
\end_inset

 and take it's 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
nth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

95
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 percentile as the UCL. 
\end_layout

\begin_layout Itemize
Start admitting profiles online from the process. Calculate test statistic using the trained VAE. If the test statistic is over UCL, identify the sample as out-of-control. 
\end_layout

\begin_layout Standard
We train 10 different model instances with different seeds to account for inherent randomness due to the weight initialization of deep neural networks.
\end_layout

\begin_layout Subsection
Neural Network Architectures and Training 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-Architectures"

\end_inset


\end_layout

\begin_layout Standard
In this work, we use convolutional neural networks for the encoders and decoders in our VAE model to represent the spatial neighborhood structures of the profiles. Introduced in 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "lecun1989backpropagation"
literal "false"

\end_inset

, convolutional layers have enabled tremendous performance increase in certain neural network applications where the data is of a certain spatial neighborhood structure such as images or audio waveform. They exploit an important observation of such data, where the learner should be equivariant to translations. This is an important injection of inductive bias into the network that largely reduces the number of parameters compared to the fully connected network by the use of parameter sharing. It eventually increases the statistical learning efficiency, especially for small samples. It must be noted, however, convolutional layers are not equivariant to scale and rotation as they are to translation. Knowing what sort of inductive biases is injected into these layers is important for the understanding of disentanglement, which we will introduce later in this paper.
\end_layout

\begin_layout Standard
We use the encoder-decoder structure outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The layers used that builds the model architectures used in this study are summarized as follows: 
\end_layout

\begin_layout Itemize
C(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional layer with arguments referring to the number of output channels 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

 and size of zero-padding 
\begin_inset Formula $P$
\end_inset

. 
\end_layout

\begin_layout Itemize
CT(
\begin_inset Formula $O,K,S,P$
\end_inset

): Convolutional transpose layer with arguments referring to the number of output channels 
\begin_inset Formula $O$
\end_inset

, kernel size 
\begin_inset Formula $K$
\end_inset

, stride 
\begin_inset Formula $S$
\end_inset

, and size of zero-padding 
\begin_inset Formula $P$
\end_inset

. 
\end_layout

\begin_layout Itemize
FC(
\begin_inset Formula $I,O$
\end_inset

): Fully connected layer with arguments referring to input dimension 
\begin_inset Formula $I$
\end_inset

 and output dimension 
\begin_inset Formula $O$
\end_inset

. 
\end_layout

\begin_layout Itemize
A(): Activation function. Leaky ReLU with a negative slope of 
\begin_inset Formula $0.2$
\end_inset

. 
\end_layout

\begin_layout Standard
Here, C(), CT(), and FC() are considered the linear transformation layers while R(), LR(), and S() are considered the nonlinear activation layers. Strided convolutions can be used to decrease the spatial dimensions in the encoders. Pooling layers are typically not recommended in autoencoder-like architectures 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "radford2015unsupervised"
literal "false"

\end_inset

. Convolutional transpose layers are used to upscale latent codes back to ambient dimensions.
\end_layout

\begin_layout Standard
The sequential order of the computational graphs used for this study is summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The architecture choice is directly based on the encoder-decoder architecture that was used in 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "higgins2017beta"
literal "false"

\end_inset

, except that we use Leaky ReLU with a negative slope of 0.2 as the activation, which is advised in 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "radford2015unsupervised"
literal "false"

\end_inset

 for better gradient flow. The encoder outputs 
\begin_inset Formula $2r$
\end_inset

 nodes, which is a concatenation of the inferred posterior mean 
\begin_inset Formula $\mbmu_{\mbphi}(\mbx)$
\end_inset

 and variance 
\begin_inset Formula $\diag(\mbsigma(\mbx))$
\end_inset

, both are of length 
\begin_inset Formula $r$
\end_inset

. The number of epochs per training is fixed at 
\begin_inset Formula $1000$
\end_inset

, and the learning rate and batch size are fixed at 
\begin_inset Formula $0.001$
\end_inset

 and 
\begin_inset Formula $64$
\end_inset

, respectively, both are chosen empirically to guarantee a meaningful convergence. Adam algorithm is used for first-order gradient optimization with parameters 
\begin_inset Formula $(\beta_{1,}\beta_{2})=(0.9,0.999)$
\end_inset

 as advised in 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "KingmaB14"
literal "false"

\end_inset

. The model checkpoint is saved at every epoch where a better validation loss is observed. The latest checkpoint is used as the final model.
\end_layout

\begin_layout Standard
In our experiments, the architecture and the training conditions described above are optimized with respect to the convergence performance of the VAE objective on in-control dataset. This is because in real life, the practitioner will not have access to out-of-control samples. Consequently, the same setting worked well for both the simulation dataset and the case study dataset we consider in this paper. This gives us confidence that the selection is robust from one set to the other. However, a different dataset might benefit from adjustments to the above conditions. The adjustments should be based on monitoring the convergence of the VAE objective, as the procedure will benefit from a better approximated in-control distribution.
\end_layout

\begin_layout Standard
We would like to emphasize that even we focus only on the image profiles in our paper by the convolutional architectures, which will be introduced to the readers in the upcoming simulation and case study sections, the monitoring statistics we propose in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ere-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ere-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be applied to other profiles as well, which will be left as the future work.
\end_layout

\begin_layout Standard

\begin_inset Float table
placement !t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset FormulaMacro
\def\arraystretch {1.3}
\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture details of deep neural networks used in this study
\begin_inset CommandInset label
LatexCommand label
name "tab:model-architectures"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{}
\end_layout

\end_inset


\begin_inset Tabular 
<lyxtabular version="3" rows="3" columns="2">
<features rotate="0" booktabs="true" tabularvalignment="middle" tabularwidth="0pt">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top" width="80text%">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
Module 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
Architecture
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
Encoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
C(32, 4, 2, 1) - A() - C(32, 4, 2, 1) - A() - C(64, 4, 2, 1) - A() - C(64, 4, 2, 1) - A() - C(64, 4, 1, 0) - FC(256, 
\begin_inset Formula $2r$
\end_inset

)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
Decoder 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
FC(
\begin_inset Formula $r$
\end_inset

, 256) - A() - CT(64, 4, 0, 0) - A() - CT(64, 4, 2, 1) - A() - C(32, 4, 2, 1) - CT(32, 4, 2, 1) - A() - CT(1, 4, 2, 1)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Simulation Study Analysis and Results 
\begin_inset CommandInset label
LatexCommand label
name "sec:Simulation-Study-Analysis"

\end_inset


\end_layout

\begin_layout Standard
In this section, we will evaluate the proposed methodology via a simulation study. We will first test our claims we make in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:proposed-statistic"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in a controlled environment over the data generating process as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

. For every experiment mentioned in this section, we follow the procedure outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and we use VAE models with the architecture described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-Architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
We will then illustrate the incorrect mapping of the latent space and the extrapolation issue in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:generator"
plural "false"
caps "false"
noprefix "false"

\end_inset

 under this controlled experiment.
\end_layout

\begin_layout Subsection
Simulation Setup
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:simsetting"

\end_inset


\end_layout

\begin_layout Standard
We first evaluate the performance of the deep latent variable models in a simulation setting where we have explicit control over the latent variations. The simulation procedure produces 2D structured point clouds that resemble the scanned topology of a dome.
\end_layout

\begin_layout Standard
Let each pixel on a 
\begin_inset Formula $64$
\end_inset

 by 
\begin_inset Formula $64$
\end_inset

 grid be denoted by a tuple 
\begin_inset Formula $\mbp=(p_{0},p_{1})$
\end_inset

. The values of the tuples stretch from 
\begin_inset Formula $0$
\end_inset

 to 
\begin_inset Formula $1$
\end_inset

, equally spaced, left to right and bottom-up. Each tuple takes a value based on its location through a function 
\begin_inset Formula $\mbp\mapsto f(\mbp;c,r)+\epsilon$
\end_inset

, where 
\begin_inset Formula $\epsilon\sim\Norm(0,1\times10^{-2})$
\end_inset

 is i.i.d Gaussian noise. The function 
\begin_inset Formula $f$
\end_inset

 is parameterized by the horizontal location of the dome 
\begin_inset Formula $c$
\end_inset

, and the radius of the base of the dome 
\begin_inset Formula $r$
\end_inset

. The vertical location of the dome on the 2D surface is fixed at the vertical center of the surface. Given any parameter set 
\begin_inset Formula $\{c,r\}$
\end_inset

, each pixel 
\begin_inset Formula $\mbp$
\end_inset

 can be evaluated with the following logic: 
\begin_inset Formula \begin{equation}
\begin{split}g(\mbp;c,r) & =1-\frac{(p_{0}-c)}{r}^{2}-\frac{(p_{1}-0.5)}{r}^{2}\\
f(\mbp;c,r) & =\begin{cases}
\sqrt{g(\mbp;c,r)} & \mbox{if }g(\mbp;c,r)\geq0\\
0 & \mbox{if }g(\mbp;c,r)<0
\end{cases}
\end{split}
\label{eq:gasketfun}
\end{equation}
\end_inset


\end_layout

\begin_layout Standard
The samples are best visualized as grayscale images as shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gasketgrid"
plural "false"
caps "false"
noprefix "false"

\end_inset

 below.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{}
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename gasket.pdf
	width 50line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Dome profiles depicted as grayscale images simulated with radius and center location they coincide with on the axes.
\begin_inset CommandInset label
LatexCommand label
name "fig:gasketgrid"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The processes that generate the latent variations of in-control domes are defined as Gaussian distributions: 
\begin_inset Formula \begin{equation}
\begin{split}c\sim\Norm(0.5,1\times10^{-2})\\
r\sim\Norm(0.2,6.25\times10^{-4})
\end{split}
\end{equation}
\end_inset


\end_layout

\begin_layout Standard
As our out-of-control scenarios consider the following four distribution shifts in which 
\begin_inset Formula $\delta$
\end_inset

 denotes the intensity of the shift: 
\end_layout

\begin_layout Itemize

\series bold
Location shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $c$
\end_inset

 is altered by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula \[
c\sim\Norm(0.5+\delta\times10^{-2},1\times10^{-2})
\]
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Radius shift:
\series default
 the mean of the process that generates 
\begin_inset Formula $a$
\end_inset

 is perturbed by an amount 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula \[
r\sim\Norm(0.2+\delta\times10^{-4},6.25\times10^{-4})
\]
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Mean shift
\series default
: all the pixels are added an additive disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula \[
f(\mbp;c,r)\leftarrow f(\mbp;c,r)+\delta
\]
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Magnitude shift:
\series default
 all the pixels are added a multiplicative disturbance 
\begin_inset Formula $\delta$
\end_inset

 as in 
\begin_inset Formula \[
f(\mbp;c,r) \leftarrow f(\mbp;c,r)*\delta
\]
\end_inset


\end_layout

\begin_layout Standard
Note that the location shift and radius shift represent disturbances in latent distribution 
\begin_inset Formula $p_{\delta}(\mbz)$
\end_inset

. The other two cases, mean shift and magnitude shift, represent disturbances in the conditional distribution 
\begin_inset Formula $p_{\delta}(\mbx\g\mbz)$
\end_inset

.
\end_layout

\begin_layout Standard
We generate the training, validation, and testing sets for in-control domes as well as a set of each out-of-control scenario above. All sets have exactly 500 distinct samples. We generate these sets once, fix them, and use them for the analyses in the subsequent sections.
\end_layout

\begin_layout Subsection
On the Incorrect Mapping of Latent Representations by the Encoder 
\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:recognition"

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset Float figure
wide false
sideways false
status collapsed


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{}
\end_layout

\end_inset


\begin_inset Graphics 
	filename fixed_radii.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Standard
Fixed 
\begin_inset Formula $r$
\end_inset

, varying 
\begin_inset Formula $c$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Fixed-r-varying-c"

\end_inset



\end_layout

\end_inset


\end_layout
\end_inset


\end_layout

\begin_layout Standard

\begin_inset Float figure
wide false
sideways false
status collapsed


\begin_layout Standard
\align center

\begin_inset Graphics 
	filename fixed_center_locs.pdf
	scale 50

\end_inset

 
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Standard
Fixed 
\begin_inset Formula $c$
\end_inset

, varying 
\begin_inset Formula $r$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Fixed-c-varying-r"

\end_inset



\end_layout

\end_inset


\end_layout
\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Figure depicting the discrepancy between the true and predicted latent representations of the encoder of a VAE with two-dimensional latent code trained with in-control samples. For each subfigure, plots on the left show where real factors of variation are sampled from and the figure on the right is what the VAE encoder infers as the mean of the proposal distribution. In all figures, the regions that are considered to be in-control are represented with a dashed circle.
\series bold
Top:
\series default
 Real factors of variation are generated at three fixed levels of radius 
\begin_inset Formula $r$
\end_inset

 and varying values of center location 
\begin_inset Formula $c$
\end_inset

 on the left figure. Corresponding inferred means are plotted on the right graph. 
\series bold
Bottom:
\series default
 Similar to (b) but the center location 
\begin_inset Formula $c$
\end_inset

 fixed at three levels and varying 
\begin_inset Formula $r$
\end_inset

.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:proposals"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this section, we will investigate the latent representations produced by the encoder and whether it can be mapped back to the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

true
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

 latent space that generates the data in the context of our simulation study.
\end_layout

\begin_layout Standard
We first train a VAE with an architecture described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:model-architectures"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and fix the generating latent representation as 
\begin_inset Formula $ r=2 $
\end_inset

. The training samples are generated by the in-control dome generation process as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

. We will use the encoder of the trained VAE for the rest of the analysis.
\end_layout

\begin_layout Standard
We can generate samples from the trained encoder by fixing one of the true latent factors and traversing along the other. The plots on the left side of 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

 depicts the traversals of the true latent space we sample the domes from. We then push these generated examples through the encoder to obtain their respective proposal distributions. We will compare the mean of the respective proposal distributions and the true latent space. If the learned proposal distribution is mapped into a substantially different geometry by the encoders, we will describe the distribution as 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

incorrect
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the incorrectness in the mapping of latent representations. This incorrect mapping behavior is even worse when we are dealing with the extreme values in the true latent space. For example, from 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (b), we can conclude that domes with extremely small radii will likely go undetected if only the latent-space statistic is used.
\end_layout

\begin_layout Standard
Overall, the learned latent representations are typically 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

incorrect
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

 especially for the samples with extreme latent variables. This, in turn, will lead to an incorrect out-of-control assignment in Phase-II analysis, if only the latent-space monitoring statistic is used.
\end_layout

\begin_layout Subsection
On the Extrapolation Performance of the Decoder 
\begin_inset CommandInset label
LatexCommand label
name "sec:simstudy:generator"

\end_inset


\end_layout

\begin_layout Standard
In this subsection, we will evaluate the extrapolation performance of the decoder. To demonstrate this, we showed the generated images by the decoder in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:manifold_vae"
plural "false"
caps "false"
noprefix "false"

\end_inset

, when traveling along one axis of the latent dimension while keeping the other fixxed.
\end_layout

\begin_layout Standard
Here, the decoder is trained on in-control samples described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which is the same VAE described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Standard
It should be cross-examined with 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

 above as the encoder and decoder are tightly coupled to each other. We observe two important behaviors: the posterior gets distorted beyond two or three standard deviations, and the representations are partially entangled in line with the behavior of its encoder depicted in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:proposals"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
To see how this will help to detect disturbances in the latent space, we consider a dome that is extremely small in terms of the radius (i.e., small 
\begin_inset Formula $r$
\end_inset

) or at the very margins of the grid in terms of center location (i.e., center location 
\begin_inset Formula $c$
\end_inset

 far from 0.5). Looking at 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:manifold_vae"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can observe that the decoder simply cannot generate such a sample because it does not extrapolate well in either of the latent dimensions. This will, in turn, produce a larger reconstruction error and can be captured by the residual-space monitoring statistic.
\end_layout

\begin_layout Standard
Recall once again that the disturbance described is purely on the latent distribution 
\begin_inset Formula $p(\mbz)$
\end_inset

 and yet detected by the residual-space monitoring statistic only due to the extrapolation issue. 
\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset Graphics 
	filename manifold_vae.pdf
	width 90line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
latent-space traversal and the response of the decoder of a VAE with 2-dimensional latent codes and trained with in-control dome samples. Each row represents which latent dimension is traversed while the other dimension is fixed at zero. Each column represents what value is assigned to that latent dimension that is represented by the row label. Each image in each cell is generated by the decoder using that specific latent variable combination.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:manifold_vae"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
On the Estimation of Log-likelihood Under Importance Sampling
\end_layout

\begin_layout Standard
Earlier, we claimed that it would take too many Monte Carlo iterations to get a meaningful estimate of ERE defined as 
\begin_inset Formula $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$
\end_inset

. In this section, we test that claim on a random in-control sample 
\begin_inset Formula $\mbx$
\end_inset

 using the proposal distribution 
\begin_inset Formula $\mbz\sim\encoding$
\end_inset

, which is obtained via the encoder of the same VAE model we have been using in this section. The results of the sampling-based estimation of ERE, first-order approximation 
\begin_inset Formula $ERE_{1}$
\end_inset

, and second-order approximation 
\begin_inset Formula $ERE_{2}$
\end_inset

 are shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Estimation-comparison-between"
plural "false"
caps "false"
noprefix "false"

\end_inset

 below. The key observation is that it takes at least 60 Monte Carlo iterations to get a stable and accurate estimation. At that level, the single pass through the encoder is negligible. This means using sampling will be more costly at least 60 samples to achieve the same accuracy as the first-order approximation that we suggest and at least 80 samples to get the accuracy of the second-order approximation. Another important observation is that the second-order approximation is a bit more accurate than first-order approximation since it is closer to the sample average approximation, but their difference is quite insignificant. Furthermore, it requires much more computation for the second-order approximation given the second-order Hessian matrix needs to be evaluated. In the next subsection, we will evaluate the performance of 
\begin_inset Formula $ERE_{2}$
\end_inset

 and 
\begin_inset Formula $ERE_{1}$
\end_inset

 in Phase-II monitoring to evaluate whether the added computational complexity for 
\begin_inset Formula $ERE_{2}$
\end_inset

 is justifiable.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset Graphics 
	filename mc_vs_foa.pdf
	width 90text%

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Caption Standard

\begin_layout Plain Layout
Estimation comparison between Monte Carlo sampling, first-order approximation and second-order approximation. 95% confidence interval band is shown in the gray band and is based on simulations with ten different seeds. 
\begin_inset CommandInset label
LatexCommand label
name "fig:Estimation-comparison-between"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison of Detection Performance of Proposed Statistics
\end_layout

\begin_layout Standard
We now compare the proposed statistics based on the Phase II monitoring performance by how accurately they detect profiles from out-of-control processes outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simsetting"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Note that for all statistics that require sampling, we obtain a single sample and calculate the statistic based on that to keep the computational demand the same for all statistics and emulate the computational constraints of a real-life case. A preliminary result we must check is the robustness of the statistics by making sure all proposed statistics have false alarm rates on the held-out in-control test set, which should also be less than the desired rate 5%. 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:far"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates that this is the case for all of them.
\end_layout

\begin_layout Standard

\begin_inset Float table
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset FormulaMacro
\def\arraystretch {1.3}
\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
False alarm rates on the held-out dataset averaged over 10 replications per model and monitoring statistic. Standard deviations are in parentheses.
\begin_inset CommandInset label
LatexCommand label
name "tab:far"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Tabular 
<lyxtabular version="3" rows="2" columns="6">
<features rotate="0" booktabs="true" tabularvalignment="middle" tabularwidth="0pt">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
Statistic 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard

\begin_inset Formula $ERE_{1}$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
SPE/R 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
D 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard

\begin_inset Formula $H^2$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard

\begin_inset Formula $T^2$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.041(0.006) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.051(0.005) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.044(0.004) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.052(0.005) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.043(0.009)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Through 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we observe a clear superiority of 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 over other methods when the disturbance is on the observable space (top row). latent-space statistics 
\begin_inset Formula $D$
\end_inset

, 
\begin_inset Formula $H^{2}$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

 fail in this case since that they are purely computed using the proposal distribution latent variables. 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 also outperform 
\begin_inset Formula $SPE/R$
\end_inset

, although by a smaller margin it has with the latent variable-statistics. Between 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

, it's hard to claim which one works better since their mean performances are quite close to each other.
\end_layout

\begin_layout Standard
For the latter two disturbances occurring purely on latent dimensions, results are presented in the bottom row of 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:disturbance_on_pxz"
plural "false"
caps "false"
noprefix "false"

\end_inset

. The key observations can be listed as follows: 
\end_layout

\begin_layout Itemize
Generally 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

, 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $H^{2}$
\end_inset

 tend to perform better than 
\begin_inset Formula $SPE/R$
\end_inset

 and 
\begin_inset Formula $T^{2}$
\end_inset

. A commonality between the former three is that they do not rely on random samples, supporting our argument against this practice. 
\end_layout

\begin_layout Itemize
Observe the radius shift-type disturbance show in the bottom left figure. Even though 
\begin_inset Formula $H^{2}$
\end_inset

 performs better on positive intensities (larger radii), it completely misses negative intensities (smaller radii). We foresaw this result in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simstudy:recognition"
plural "false"
caps "false"
noprefix "false"

\end_inset

. To reiterate, the 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

incorrect
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
"
\end_layout

\end_inset

 mapping of the latent space and the lack of extrapolation in the encoder is the reason behind this. We would also suggest that this result can extend to all the latent-variable based statistics for deep autoencoder-based methods. 
\end_layout

\begin_layout Itemize
Unlike latent-space statistics, 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 and 
\begin_inset Formula $SPE/R$
\end_inset

 behave more robustly against varying intensities. In other words, the detection rate increase with increased intensities consistently. Among these, we observe that 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 consistently outperform 
\begin_inset Formula $SPE/R$
\end_inset

. 
\end_layout

\begin_layout Itemize

\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 perform very similarly. In this case, we conclude that the second-order information does not help too much for Phase-II monitoring. The reason behind this is that the second-order information also comes from the encoder. However, given that the encoders are trained on in-control samples and may provide inaccurate information in the out-of-control regions, the second-order information for out-of-control samples would be biased. Therefore, it does not provide additional gain for monitoring performance. 
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement !t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset Graphics 
	filename disturbance_on_pxz_vae_only.pdf
	width 100line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Fault detection rates (y-axis) for varying intensities (x-axis) of different disturbance types (quadrants). Bands represent a 95% confidence interval estimated around mean detection rates.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:disturbance_on_pxz"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As mentioned, in a real-life process, disturbances on the residual space is often more likely than the disturbance in the latent space. Therefore, we would recommend the use of residual-space monitoring statistics. Among all residual-space monitoring statistics, we conclude that 
\begin_inset Formula $ERE_{1}$
\end_inset

 perform the best considering the accuracy, robustness and, computational demand. This will be further validated through the case study analysis.
\end_layout

\begin_layout Section
Case Study Analysis & Results
\end_layout

\begin_layout Standard

\begin_inset CommandInset label
LatexCommand label
name "sec:case-study"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout
% TODO (@DS): how many anomaly samples do we have?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
% TODO (@DS): Add a figure to illustrate both normal and abnormal samples, with one image in each class
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

In this section, we will evaluate the performance of the proposed algorithm using real case study. Our dataset consists of defect image profiles from a hot-steel rolling process, which is shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rolling"
plural "false"
caps "false"
noprefix "false"

\end_inset

. There are 13 classes of surface defect types identified by the domain engineers. Four of these classes—0,1,9 and 11—are considered minor defects and they constitute our in-control set. There are 338 images in these classes. The other nine classes make up the out-of-control cases and they have in combination 3351 images to report detection accuracy for. We randomly partition the in-control corpus to fix train, validation, and test sets with 60%-20%-20% relative sizes, respectively. The rest of the procedure followed is outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:methodology:procedure"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Same as in the simulation study, to account for randomness in weight initialization, we replicate the experiment with 10 different seeds. For comparison, we also include the monitoring performance with the traditional PCA method with the same residual-space control chart, denoted as PCA-Q. The results are summarized in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

 below.
\end_layout

\begin_layout Standard

\begin_inset Float table
wide false
sideways true
status open


\begin_layout Standard

\begin_inset FormulaMacro
\def\arraystretch {1.3}
\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Summary of fault detection rates on out-of-control cases averaged over 10 replications per model and monitoring statistic. Standard deviations are in parentheses. Bolded values represent the maximum average across different statistics. 
\begin_inset CommandInset label
LatexCommand label
name "tab:rolling_results"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{}
\end_layout

\end_inset


\begin_inset Tabular 
<lyxtabular version="3" rows="13" columns="8">
<features rotate="0" booktabs="true" tabularvalignment="middle" tabularwidth="0pt">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<column alignment="left" valignment="top">
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
Model 
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="none" valignment="top" usebox="none" special="c">
\begin_inset Text

\begin_layout Standard
VAE
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell multicolumn="1" alignment="none" valignment="top" usebox="none" special="c">
\begin_inset Text

\begin_layout Standard
PCA
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
Statistic 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
D 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\begin_inset Formula $H^2$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\begin_inset Formula $T^2$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
SPE/R 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
ERE 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
ERE2 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
Q
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
Fault ID 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
<cell alignment="none" valignment="top" usebox="none">
\begin_inset Text

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
2 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.37(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.44(0.06) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.50
\series default
(0.06) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
3 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.17(0.06) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.23(0.04) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.03(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.84(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.85(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.86
\series default
(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.78(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
4 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.62(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.75
\series default
(0.05) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.71(0.05) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.56(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
5 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.58(0.07) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.62(0.09) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
1.00
\series default
(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.99(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
6 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.06(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.15(0.08) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.05(0.05) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.79(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.80
\series default
(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.80
\series default
(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.52(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
7 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.01(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.01(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.13(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.17
\series default
(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.15(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.11(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
8 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.64(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.70
\series default
(0.07) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.69(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.34(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
10 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.49(0.03) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.57
\series default
(0.05) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.57
\series default
(0.04) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.29(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
12 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.79(0.01) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.80
\series default
(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.80
\series default
(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Standard
0.69(0.00)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
13 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.00(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.01(0.00) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.71(0.04) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard

\series bold
0.77
\series default
(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.76(0.02) 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Standard
0.56(0.00)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can observe that 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 consistently outperforms all other monitoring statistic formulations. The divide between residual-space statistics and latent-space statistics observed in the simulation study is further validated here too. The inferiority of latent-space statistics are much more obvious here in the real case study, as we observe for most out-of-control classes the detection rate is simply zero. This observation further validates our claims that in practice, for deep autoencoders, the change happens in the residual space rather than the latent space. The advantage of VAE over PCA is mainly due to the better representative power and data compression ability of deep autoencoders compared to PCA. It is worth noting that the superiority of VAE over PCA for process monitoring was also demonstrated in the earlier works in various applications 
\begin_inset CommandInset citation
LatexCommand citep
after ""
before ""
key "Zhang2019-lu,wang2019systematic,lee2019process"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
To support our claim of the ineffectiveness of latent-space statistics, we refer the reader to 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:kde"
plural "false"
caps "false"
noprefix "false"

\end_inset

 below. We observe how well separated the statistics are for 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $SPE/R$
\end_inset

 while for latent-space statistics the obtained values are mostly overlapping. Note that we omitted 
\begin_inset Formula $ERE_{2}$
\end_inset

 because it was almost identical to 
\begin_inset Formula $ERE_{1}$
\end_inset

. To obtain a deeper understanding of the results, we point out in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Output-of-the"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for the original images and their reconstructions. The decoder is persistent on generating samples that look like in-control rolling samples with little fidelity to how the original defect sample looks like. When 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Original-profiles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Reconstructions-via-VAE"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are cross-examined, it is apparent why reconstruction error would be high. On the contrary, 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Inferred-means"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows that most latent representations fall into the region that would be considered in-control from a profile monitoring perspective. We observe instances of classes 3,5,6 and 7 generate the latent variables in the out-of-control regions. However, even for these classes, 
\begin_inset Formula $SPE/R$
\end_inset

, 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

 yields much better detection power than 
\begin_inset Formula $D$
\end_inset

, 
\begin_inset Formula $H^{2}$
\end_inset

, and 
\begin_inset Formula $T^{2}$
\end_inset

, as it can be seen in 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:rolling_results"
plural "false"
caps "false"
noprefix "false"

\end_inset

. In conclusion, we would like to suggest the use of 
\begin_inset Formula $ERE_{1}$
\end_inset

 for deep autoencoders, which is consistent with our findings in the simulation study.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.3
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename SPE.pdf
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\begin_inset Formula $ERE_{1}$
\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:kde:ERE"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset space \hspace{}
\length 10text%
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.3
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename R.pdf
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\begin_inset Formula $SPE/R$
\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:kde:spe-r"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.3
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename H2.pdf
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\begin_inset Formula $H^{2}$
\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:kde:h2"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset space \hfill{}

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.3
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename D.pdf
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\begin_inset Formula $D$
\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:kde:D"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset space \hfill{}

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
begin{subfigure}[b]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
{
\end_layout

\end_inset

0.3
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
textwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics 
	filename T2.pdf
	width 100text%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\begin_inset Formula $T^{2}$
\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:kde:T2"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\backslash
end{subfigure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Caption Standard

\begin_layout Plain Layout
Kernel density estimation plots of statistics obtained for in-control and out-of-control steel defect profiles, per each proposed statistic type.
\begin_inset CommandInset label
LatexCommand label
name "fig:kde"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard

\begin_inset Box Frameless
position "t"
hor_pos "l"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "25text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open


\begin_layout Plain Layout

\begin_inset Float figure
wide false
sideways false
status collapsed


\begin_layout Standard

\begin_inset Graphics 
	filename casestudy_orig.pdf
	width 99text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Standard
Original profiles
\begin_inset CommandInset label
LatexCommand label
name "fig:Original-profiles"

\end_inset



\end_layout

\end_inset


\end_layout
\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "l"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "25text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open


\begin_layout Plain Layout

\begin_inset Float figure
wide false
sideways false
status collapsed


\begin_layout Standard

\begin_inset Graphics 
	filename casestudy_recons.pdf
	width 82col%

\end_inset

 
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Standard
Reconstructions via VAE
\begin_inset CommandInset label
LatexCommand label
name "fig:Reconstructions-via-VAE"

\end_inset



\end_layout

\end_inset


\end_layout
\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "l"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open


\begin_layout Plain Layout

\begin_inset Float figure
wide false
sideways false
status collapsed


\begin_layout Standard

\begin_inset Graphics 
	filename casestudy_scat.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Standard
Inferred means
\begin_inset CommandInset label
LatexCommand label
name "fig:Inferred-means"

\end_inset



\end_layout

\end_inset


\end_layout
\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Output of the VAE decoder and the encoder for randomly select rolling profiles. 
\series bold
Left: 
\series default
Original profiles visualized. Each row is a class of defect profile and each column is randomly selected from that class. 
\series bold
Middle: 
\series default
Reconstructions of the samples with one-to-one correspondence to the samples on the image to the left. 
\series bold
Right: 
\series default
Inferred mean locations of each of the defects visualized on left. Points are annotated by their class IDs. 
\begin_inset CommandInset label
LatexCommand label
name "fig:Output-of-the"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally, we report execution time details for our proposed statistic, 
\begin_inset Formula $ ERE_{1}] $
\end_inset

. For this study, we utilized a workstation with 6-core Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz CPUs and 4 GeForce GTX 1080 Ti GPUs. Neural network computations are executed on a single GPU and a single CPU core is used for image input/output and preprocessing steps such as resizing to 64-by-64 and grayscale conversion wherever needed. A single GPU has 12GB memory and the model parameters take up about 730MBs. GPUs can leverage parallel computation of multiple images, therefore the remaining memory can be used to stock up images so their execution becomes parallel. An example of a batch of 128 images takes up only 63MBs more space in the GPU's memory and the per image execution time is roughly 0.8 milliseconds. On the extreme case of using a single image per batch, per image execution time is around 2 milliseconds on average.
\end_layout

\begin_layout Section
Conclusion 
\begin_inset CommandInset label
LatexCommand label
name "sec:conclusions"

\end_inset


\end_layout

\begin_layout Standard
In this paper, we focused on evaluating Phase-II monitoring statistics proposed so far in the literature for VAE and demonstrate that they were not performing optimally in terms of accuracy and/or computational feasibility. First, we classified these statistics into two groups and showed how they are designed as an extension to the classical statistics used for PCA. Then we pointed out that such an extension is not as straightforward as it seems due to the incorrectness of learned latent representations by VAEs and also due to the failure to extrapolate behavior. This led us to the conclusion that only residual-space statistics should be monitoring, regardless of the anticipated source of the shift in the process. We also pointed out that the residual-space statistics based on sampling will require too many samples to be computationally feasible. Finally, we proposed a novel formulation by deriving the Taylor expansion of the expected reconstruction error that addresses the computational efficiency issue in residual-space statistics.
\end_layout

\begin_layout Standard
We put our claims to test with a carefully designed simulation study. This study demonstrated the discrepancy between the true latent variations and its learned counterparts, and its implications to the process monitoring performance of latent-space statistics. We also reinforced our claim that the derived statistics based on the residual space is overall more robust and accurate than all the other statistics proposed so far. Finally, we validated the superiority of our formulation on a real-life case study, where steel defect image profiles are used.
\end_layout

\begin_layout Standard
For future work, we hope to extend the proposed method for other types of data format. For example, for sequential profiles (e.g. time series), one-dimensional convolutional layers or a recurrent neural network for encoder and decoder structures as outlined in 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "ChungKDGCB15"
literal "false"

\end_inset

 can be used. We are also curious to see how new developments in deep learning research will affect profile monitoring in high dimensions in the future. Specifically, developments in deep latent variable models and representation learning may have important implications.
\end_layout

\begin_layout Standard

\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"
options ""

\end_inset


\end_layout

\begin_layout Standard
\start_of_appendix

\begin_inset ERT
status collapsed

\begin_layout Plain Layout
%dummy comment inserted by tex2lyx to ensure that this paragraph is not empty
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout
%dummy  inserted by tex2lyx to ensure that this paragraph is not empty
\backslash
refalias{section}{appendix}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Proof of Proposition 3.1 
\begin_inset CommandInset label
LatexCommand label
name "sec:PoofOfPropTQ"

\end_inset


\end_layout

\begin_layout Standard
The Kullback-Leibler divergence between two multivariate Gaussian distributions has a closed-form solution. If we define these distributions as 
\begin_inset Formula $p_{0}=N(\mbz;\mbmu_{0},\mbSigma_{0})$
\end_inset

 and 
\begin_inset Formula $p_{1}=N(\mbz;\mbmu_{1},\mbSigma_{1})$
\end_inset

 where 
\begin_inset Formula $\mbmu$
\end_inset

 and 
\begin_inset Formula $\mbSigma$
\end_inset

 are respective mean vectors and covariance matrices, then according to 
\begin_inset CommandInset citation
LatexCommand citet
after ""
before ""
key "hershey2007approximating"
literal "false"

\end_inset

 the closed-form solution will be the following: 
\begin_inset Formula \begin{align}
\KL{p_{0}}{p_{1}} & =\frac{1}{2}[\log\frac{\g\mbSigma_{1}\g}{\g\mbSigma_{0}\g}+Tr(\mbSigma_{1}\inv\mbSigma_{0})-r+(\mbmu_{0}-\mbmu_{1})^{\top}\mbSigma_{1}\inv(\mbmu_{0}-\mbmu_{1})]\label{eq:kld-closed-form}
\end{align}
\end_inset

Since 
\begin_inset Formula $\encoding=\Norm(\mbmu(\mbx),\mbSigma_{z})$
\end_inset

 and 
\begin_inset Formula $p(\mbz)=\Norm(0,\mbI)$
\end_inset

, we can derive that 
\begin_inset Formula \begin{align}
\KL{\encoding}{p(\mbz)} & =\frac{1}{2}\left[-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r\right]+\frac{1}{2}\mbmu(\mbx)^{\top}\mbmu(\mbx)\nonumber \\
 & =\frac{1}{2}\mbmu(\mbx)^{\top}\mbmu(\mbx)+C,\label{eq:kld-prior}
\end{align}
\end_inset

where 
\begin_inset Formula $C=-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r$
\end_inset

 is a constant, which doesn't depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Standard
To derive the SPE statistics, we will derive
\end_layout

\begin_layout Standard

\begin_inset Formula \begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2}\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbx^{\top}\mbx-2\mbz^{\top}\mbW\mbx+\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\label{eq: spew}
\end{align}
\end_inset


\end_layout

\begin_layout Standard
Here, we know that 
\begin_inset Formula \begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}tr(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & tr\left(\mbW^{\top}\mbW\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz\mbz^{\top})\right)\nonumber \\
= & tr\left(\mbW^{\top}\mbW(\mbmu(\mbx)\mbmu(\mbx)^{\top}+\Sigma_{z})\right)\nonumber \\
= & \mbmu(\mbx)^{\top}\mbW^{\top}\mbW\mbmu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\label{eq: tracezwwz}
\end{align}
\end_inset


\end_layout

\begin_layout Standard
Therefore, by plugging 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: tracezwwz"
plural "false"
caps "false"
noprefix "false"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: spew"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we have 
\begin_inset Formula \begin{align}
\mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2} & =\mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
 & =\mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mbmu(\mbx)^{\top}\mbW^{\top}\mbW\mbmu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\nonumber \\
 & =\|\mbx-\mbW\mbmu(\mbx)\|^{2}+C\label{eq:q-to-ere}
\end{align}
\end_inset

where 
\begin_inset Formula $C=tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)$
\end_inset

 that does not depend on 
\begin_inset Formula $\mbx$
\end_inset

.
\end_layout

\begin_layout Section
A Toy Example to Demonstrate Out-of-distribution Behavior of Neural Networks 
\begin_inset CommandInset label
LatexCommand label
name "app:rosenbrock"

\end_inset


\end_layout

\begin_layout Standard
Assume using a multilayer perceptron, we are trying to approximate the famous Rosenbrock function 
\begin_inset Formula $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$
\end_inset

 given 
\begin_inset Formula $(a,b)=(1,100)$
\end_inset

. In this small experiment, we sample tuples of two-dimensional points from a bounded region 
\begin_inset Formula $(x_{i},y_{i})\in[-1,3]\times[-2,3]$
\end_inset

. We use a multilayer perceptron with six hidden layers and a hundred neurons in each layer. Half of the points are used in training, and the other half is used as a validation set to optimize hyper-parameters. Using the trained network, we plot the actual Rosenbrock function along with the neural network approximation in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Rosenbrock"
plural "false"
caps "false"
noprefix "false"

\end_inset

. Notice how well the function is approximated for the region 
\begin_inset Formula $[-1,3]\times[-2,3]$
\end_inset

, but there is a serious discrepancy between the approximated and the real outside of the region. This is a small yet to the point example of out-of-distribution issues with neural networks.
\end_layout

\begin_layout Standard

\begin_inset Float figure
placement t
wide false
sideways false
status open


\begin_layout Standard
\align center

\begin_inset Graphics 
	filename rosenbrock.pdf
	width 100text%

\end_inset

 
\end_layout

\begin_layout Standard

\begin_inset Caption Standard

\begin_layout Plain Layout
Rosenbrock function (green surface) approximated by an multilayer perceptron(red surface) given training (black crosses) and validation (black dots) samples form a bounded region 
\begin_inset Formula $(x_{i},y_{i})\in[-1,3]\times[-2,3]$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Rosenbrock"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
ERE Testing Statistic Derivation 
\begin_inset CommandInset label
LatexCommand label
name "app:ere"

\end_inset


\end_layout

\begin_layout Standard
To derive the 
\begin_inset Formula $ERE_{1}$
\end_inset

 and 
\begin_inset Formula $ERE_{2}$
\end_inset

, we first define 
\begin_inset Formula $R(z)=\|y-\mbmu_{\mbtheta}(z)\|^{2}$
\end_inset

 as the reconstruction error (RE). The quantity we would like approximate is 
\begin_inset Formula $E_{\mbz\sim q_{\phi}}R(\mbz)$
\end_inset

 where 
\begin_inset Formula $\encoding=\Norm(\mbmu_{\mbphi}(\mbx),\Sigma_{z})$
\end_inset

. We are looking for the Taylor expansion of the expected RE (ERE) around 
\begin_inset Formula $z_{0}=\mbmu_{\mbphi}(\mbx)$
\end_inset

, i.e., the first moment. For notational simplicity, we use 
\begin_inset Formula $H_{z}$
\end_inset

 to denote the Hessian 
\begin_inset Formula $R''(\mbmu_{\mbphi}(\mbx))$
\end_inset

. The derivation is formalized as follows:
\end_layout

\begin_layout Standard

\begin_inset Formula \begin{align}
E_{\mbz\sim q_{\phi}}R(\mbz) & =R(\mbmu_{\mbphi}(\mbx))+R'(\mbmu_{\mbphi}(\mbx))E_{\mbz\sim q_{\phi}}[\mbz-\mbmu_{\mbphi}(\mbx)])\nonumber \\
 & \quad+\frac{1}{2}E_{\mbz\sim q_{\phi}}[(\mbz-\mbmu_{\mbphi}(\mbx))^{\top}\mathbf{H}_{z}(\mbz-\mbmu_{\mbphi}(\mbx))]+O(\|(\mbz-\mbmu_{\mbphi}(\mbx)\|^{3}\nonumber \\
 & \simeq R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}E_{\mbz\sim q_{\phi}}[(\mbz-\mbmu_{\mbphi}(\mbx))^{\top}\mathbf{H}_{z}(\mbz-\mbmu_{\mbphi}(\mbx))]\nonumber \\
 & =R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}E[(\mbz-\mbmu_{z})(\mbz-\mbmu_{z})^{T}])\nonumber \\
 & =R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\Sigma_{z})\label{eq:derived-ere2}
\end{align}
\end_inset


\end_layout

\begin_layout Standard
Note for 
\begin_inset Formula $ERE_{1}$
\end_inset

, the second term 
\begin_inset Formula $\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\Sigma_{z})$
\end_inset

 is droped and we are left with 
\begin_inset Formula $R(\mbmu_{\mbphi}(\mbx))$
\end_inset

 only. For 
\begin_inset Formula $ERE_{2}$
\end_inset

, since 
\begin_inset Formula $\Sigma_{z}$
\end_inset

 is a diagonal matrix, 
\begin_inset Formula $\mathrm{tr}(\mathbf{H}_{z}S_{z})=\mathrm{tr}(diag(\mathbf{H}_{z})S_{z})=\sum_{i}(\mathbf{H}_{z})_{ii}(S_{z})_{ii}$
\end_inset

 holds. We can utilize this result to compute 
\begin_inset Formula $ERE_{2}$
\end_inset

, in a more computationally efficient manner. 
\begin_inset Note Note
status open
\begin_layout Plain Layout

\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"

\end_inset

\end_layout

\end_inset


\end_layout

\end_body
\end_document
