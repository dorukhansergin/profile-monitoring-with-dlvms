#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrartcl
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding default
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype true
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type authoryear
\biblatex_bibstyle chicago-authordate
\biblatex_citestyle chicago-authordate
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section*
Story of the Paper
\end_layout

\begin_layout Itemize
Previous attempts at formulating Phase-II monitoring statistics for VAE:
\end_layout

\begin_deeper
\begin_layout Itemize
sub-optimal in terms of computation and detection performance when it comes
 to high-dimensional profiles.
 because:
\end_layout

\begin_deeper
\begin_layout Enumerate
some based on latent variable statistics
\end_layout

\begin_layout Enumerate
the ones based on observable variable statistics rely on sampling to estimate
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Statistics based on latent variables are sub-optimal because:
\end_layout

\begin_deeper
\begin_layout Itemize
deep encoders are prone to two problems, both of which required to get reliable
 monitoring statistics.
 these are:
\end_layout

\begin_deeper
\begin_layout Itemize
at extrapolating beyond training set
\end_layout

\begin_layout Itemize
at finding disentangled representations
\end_layout

\end_deeper
\begin_layout Itemize
for high-dimensional profiles, most disturbance should be expected on the
 observable space
\end_layout

\end_deeper
\begin_layout Itemize
Statistics based on observable space are sub-optimal because:
\end_layout

\begin_deeper
\begin_layout Itemize
They rely on sampling which could be quite unstable and require way too
 many samples -> prohibitively expensive computations
\end_layout

\end_deeper
\begin_layout Section*
Paper Outline
\end_layout

\begin_layout Subsection*
Introduction
\end_layout

\begin_layout Enumerate
Profile monitoring is an important problem
\end_layout

\begin_layout Enumerate
Categorize PM methods by the assumptions they make on the functional relationshi
p
\end_layout

\begin_deeper
\begin_layout Itemize
Parametric:
\end_layout

\begin_deeper
\begin_layout Itemize
Functional relationship is assumed to be linear or nonlinear
\end_layout

\begin_layout Itemize
Downside: what if we don't the exact parametric form of the function
\end_layout

\end_deeper
\begin_layout Itemize
Nonparametric, known basis
\end_layout

\begin_layout Itemize
Nonparametric, unknown basis
\end_layout

\begin_deeper
\begin_layout Itemize
PCA and related linear dimensionality reduction techniques
\end_layout

\begin_layout Itemize
Deep learning as a nonlinear alternative
\end_layout

\begin_layout Itemize
Some deep models
\end_layout

\begin_layout Itemize
Deep latent variable models, specifically, VAE works
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
What is missing in the VAE works?
\end_layout

\begin_deeper
\begin_layout Itemize
They propose latent variable-based statistics assuming that the model will
 successfully recover the independent variation structure.
\end_layout

\begin_deeper
\begin_layout Itemize
That rarely happens.
 This is because (1) learnt representations are likely to be disentangled,
 (2) they won't extrapolate of out-of-distribution inputs.
\end_layout

\end_deeper
\begin_layout Itemize
They propose approximate observable variable-statistics by Monte Carlo sampling.
 For HD profiles this might require a very large number of Monte Carlo iteration
s.
 This will, in turn, explode computational requirements.
\end_layout

\end_deeper
\begin_layout Enumerate
What do we propose and what is our contribution?
\end_layout

\begin_deeper
\begin_layout Itemize
We propose a monitoring statistic that remedies both of the problems mentioned
 with earlier statistic proposals.
\end_layout

\begin_layout Itemize
We demonstrate the issues related to entangled representations and failure
 to extrapolate in inferred latent representations on a carefully curated
 simulation study.
 We show under what conditions they arise and how they hinder detection
 performance.
\end_layout

\begin_layout Itemize
We manage to reproduce the same behavior on both adversarial autoencoders
 and variational autoencoders to demonstrate that our results extend to
 common deep latent variable models.
\end_layout

\begin_layout Itemize
We validate our results on a real-life case study of HD image profiles collected
 from a hot steel rolling process.
\end_layout

\end_deeper
\begin_layout Subsection*
Background
\end_layout

\begin_layout Standard
The aim of this section is to introduce the reader to all the relevant backgroun
d material required to understand the research question we're trying to
 address.
\end_layout

\begin_layout Enumerate
Deep Latent Variable Models
\end_layout

\begin_deeper
\begin_layout Itemize
Latent Variable Models
\end_layout

\begin_layout Itemize
Deep Learning
\end_layout

\begin_layout Itemize
Deep Latent Variable Models
\end_layout

\end_deeper
\begin_layout Enumerate
Review of T^2 and Q statistics in PCA
\end_layout

\begin_deeper
\begin_layout Itemize
Why do we do this? Because previous works get inspired from these two statistics
 while forming their own.
\end_layout

\end_deeper
\begin_layout Enumerate
Review of previous monitoring statistics proposed for VAE
\end_layout

\begin_deeper
\begin_layout Itemize
H2, T2, R, D, SPE
\end_layout

\end_deeper
\begin_layout Subsection*
Methodology
\end_layout

\begin_layout Enumerate
From 
\begin_inset Formula $\log(p(x))$
\end_inset

 to ELBO to SPE and KLD and how they relate to T2 and Q of PCA shown by
 Proposition 1
\end_layout

\begin_layout Enumerate
Why latent based statistics won't work
\end_layout

\begin_deeper
\begin_layout Itemize
entangled representations
\end_layout

\begin_layout Itemize
failure to extraplote
\end_layout

\begin_layout Itemize
Verdict: don't use the KLD part
\end_layout

\end_deeper
\begin_layout Enumerate
Why we don't want to estimate SPE with Monte Carlo sampling
\end_layout

\begin_deeper
\begin_layout Itemize
in high-dimensions this is very costly
\end_layout

\begin_layout Itemize
a first-order approximation would get a decent estimation with only a single
 pass through the network
\end_layout

\begin_layout Itemize
Verdict: don't use sampling, use first-order approximation
\end_layout

\end_deeper
\begin_layout Enumerate
Formally present our statistic (I'd suggest calling this FOA, named after
 first-order approximation but I'm unsure(?))
\end_layout

\begin_layout Enumerate
Phase-2 monitoring steps
\end_layout

\begin_layout Subsection*
Simulation Study
\end_layout

\begin_layout Enumerate
Why we do simulation study? 3 reasons:
\end_layout

\begin_deeper
\begin_layout Itemize
show entangled representations exists
\end_layout

\begin_layout Itemize
show extrapolation fails
\end_layout

\begin_layout Itemize
show first-order approximation is good enough
\end_layout

\end_deeper
\begin_layout Enumerate
Gasket simulations
\end_layout

\begin_deeper
\begin_layout Enumerate
Motivation behind the design of Gasket simulation.
\end_layout

\begin_deeper
\begin_layout Itemize
It's in line with the model assumptions
\end_layout

\begin_layout Itemize
We have full access to easily discernible latent values that generated process
\end_layout

\begin_layout Itemize
It's a good, simple emulation of a high-dimensional profile
\end_layout

\end_deeper
\begin_layout Enumerate
Simulation Setup
\end_layout

\begin_deeper
\begin_layout Itemize
details omitted...
 already in paper
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Results
\end_layout

\begin_deeper
\begin_layout Enumerate
Representations are partially entangled
\end_layout

\begin_deeper
\begin_layout Enumerate
The only reason why center location is disentangled is that convolutional
 layers have the right inductive bias.
\end_layout

\begin_layout Enumerate
Radius couldn't have been captured in disentangled way.
 Smaller radius yields smaller statistic, which is undesirable profiles
 generated with small radi are actually potentially out-of-control cases
\end_layout

\end_deeper
\begin_layout Enumerate
Representations cannot extrapolate
\end_layout

\begin_layout Enumerate
Monte Carlo sampling can achieve first-order approximation level estimation
 quality only after a few dozens of iterations.
 The computational complexity implication is huge.
\end_layout

\end_deeper
\begin_layout Subsection*
Case Study
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
TODO:
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Conclusion
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
TODO:
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Sanity Check
\end_layout

\begin_layout Section*
Possible Improvements
\end_layout

\begin_layout Paragraph*
Should we get rid of AAE?
\end_layout

\begin_layout Standard
It's results are weird because the estimated sigmas for 
\begin_inset Formula $z$
\end_inset

 are really high (i.e.
 the approximate posterior 
\begin_inset Formula $q(z|x)$
\end_inset

 is extremely dispersed).
 It's adding unnecessary complication to the analysis and our contribution
 that this extends to all DLVMs is (1) non-essential (2) questionable.
\end_layout

\begin_layout Paragraph*
Should we explicitly defend first-order approximation against second-order
 approximation?
\end_layout

\begin_layout Standard
I tried second-order approximation.
 For Rolling, it improves in some classes but by a very small margin.
 For Gasket, it improves here and there but it also occasionally performs
 worse than first-order.
 All in all, the pros of having it is very limited to the cons of increasing
 per-sample computation time by a factor of 10 (I tried this too).
\end_layout

\begin_layout Paragraph*
Should we only have simulation study?
\end_layout

\begin_layout Standard
I'm not sure what we can present more about case study other than crude
 results.
 It seems like it becomes a very small chapter.
 What we can do is to maybe have Results and Discussion as two different
 chapters as it is in the canonical IMRaD format.
 We can have both simulation and case study results in Results section.
 We do joint discussion in Discussion section.
\end_layout

\end_body
\end_document
