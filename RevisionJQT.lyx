#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author -1806609307 "Hao yan"
\author -767166615 "Dorukhan Sergin"
\end_header

\begin_body

\begin_layout Section
Overall Comments
\end_layout

\begin_layout Standard
Your article has been reviewed by two reviewers and I, as guest-editor of
 the JQT Special Issue.
 Reports from the two reviewers are below.
 The decision for this article is Major Revision.
 Specifically, Reviewer 1 viewed the paper positively, but expressed concerns
 about the generalizability of your method.
 This reviewer asks you to consider other architectures besides convolutional
 neural networks to evaluate the generality of your method.
 Reviewer 2 was less positive.
 This reviewer expressed concerns about the ability for the JQT audience
 to understand your paper, and also points out several important technical
 issues that should be addressed.
 The guest editor did not provide a report, but agreed that the paper addresses
 an important topic, and is a good fit with the direction of the JQT Special
 Issue on Artificial Intelligence.
 The guest editor concurs with the technical and editorial suggestions of
 both reviewers.
 I encourage you to submit a revision of this article, and when doing so,
 please include a point-by-point response to each of the reviewers’ concerns.
 
\end_layout

\begin_layout Standard

\series bold
\bar under
Ans: 
\series default
\bar default
 Thank you for the great suggestions.
 According to the reviewers' comments, we have made the following revision
 to the paper
\end_layout

\begin_layout Itemize
Revision 1
\end_layout

\begin_layout Itemize
Revision 2.
 
\end_layout

\begin_layout Itemize
List the overview of your revisions after finishing the paper.
\end_layout

\begin_layout Section
Reviewer: 1
\end_layout

\begin_layout Subsection
Major Comments
\end_layout

\begin_layout Standard
This paper is well written and the technical details describing variational
 autoencoders, classical SPC monitoring, deep learning architectures and
 proposed statistics are well founded and nicely laid out.
 It is my belief that the paper, as currently written, deserves acceptance
 and publication in this journal.
 There are a few minor concerns that might enhance the applicability of
 the proposed methodology and are laid out below:
\end_layout

\begin_layout Enumerate
Neural Network Architectures and Training: The authors propose convolutional
 neural networks.
 (CNNs) for both the encoders and decoders.
 The specific proposed architectures include Leaky ReLU activation functions
 with a negative slope of 0.2.
 The authors also use stochastic gradient decent with a batch size of 64
 trained for 1000 epochs and a learning rate of 0.001.
 My main concern here is that this specific architecture might work well
 for certain problems but may not generalize well in all situations.
 I consider the authors should explain whether the proposed methodology
 is either robust to these hyperparameters, or if hyperparameter optimization
 might be required for other problems.
 
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for th
\change_inserted -767166615 1606709977
is important comment.
 We made the following changes to Section3.3:
\end_layout

\begin_layout Itemize

\change_inserted -767166615 1606710006
We added the reference on which we base our CNN architecture (Higgins et
 al.
 2017)
\end_layout

\begin_layout Itemize

\change_inserted -767166615 1606710421
We added the reference for why we chose Leaky ReLU with a negative slope
 of 0.2 for activation (Radford, Metz and Chintala 2016).
\end_layout

\begin_layout Itemize

\change_inserted -767166615 1606710398
We added the reference on which we base the parameters for Adam optimizer
 (Kingma and Ba 2015).
\end_layout

\begin_layout Itemize

\change_inserted -767166615 1606710294
We added the extra paragraph that starts with 
\begin_inset Quotes eld
\end_inset

In our experiments, the architecture...
\begin_inset Quotes erd
\end_inset

 in which we explain the rationale behind our choice of how hyperparameters
 are chosen.
 We claim that the selection is robust since the same setting worked well
 for two datasets that are drastically different: the simulation set and
 the case study.
 Nevertheless, we leave an open door.
 We state that our choices are based on the VAE convergence and we provide
 advise to the practitioners that if they need make adjustments, they should
 do it with respect to this indicator.
\change_deleted -767166615 1606709047
e comments about the hyper parameter tuning
\change_unchanged
.
 
\change_deleted -767166615 1606709150
The reason that we choose Leakly ReLU is because xxx papers use it in xxx.
\end_layout

\begin_layout Standard

\change_deleted -767166615 1606709150
Furthermore, we do have perform a hyperparameter tuning through cross-validation
 and below is the guidelines for hyperparamter optimization.
\end_layout

\begin_layout Standard

\change_deleted -767166615 1606709150
We do agree that advanced hyperparamter optimization algorithm including
 Bayesian optimization can be used and combined with the techniques provided
 in the paper and can be discussed.
 
\change_unchanged

\end_layout

\end_deeper
\begin_layout Enumerate
For sequence-based data other architectures could provide advantages, such
 as a Long-Short Term Memory (LSTM) autoencoders.
 Variational LSTM autoencoders have been proposed in the literature.
 I think the authors should mention whether the proposed statistic could
 be obtained using other architectures (besides CNNs) for generality of
 the methodology.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for the suggestion.
 Indeed, we agree that sequence-based profiles are commonly found and so
 the possibility of a recurrent architecture should be mentioned for interested
 readers.
 We added a related note for our readers in the last three paragraphs of
 Section 3.3.
\end_layout

\end_deeper
\begin_layout Enumerate
The authors mention that the accuracy and computational feasibility of the
 proposed method, however for the sake of completeness briefly explain how
 computationally demanding is the proposed method and the computational
 limitations.
 With this comment I am not necessarily referring to the theoretical computation
al complexity of VAEs, or of the profile monitoring procedure, but, for
 example, under a specific machine configuration and computational power
 the methodology certain computational time, and if there are limits in
 either image size, sample size, batch size, learning rate, etc.
\end_layout

\begin_layout Enumerate
Minor comments
\end_layout

\begin_deeper
\begin_layout Itemize
Introduction, Page 3, Line 39… it normally requires many trial and error
 CHANGE TO it normally requires trial and error OR rephrase
\end_layout

\begin_layout Itemize
Methodology, Proposition 3.1, Page 12, Line 26 ….
 Where C1 and C2 and constants that doesn’t depend on x CHANGE TO where
 C1 and C2 are constants that do not depend on x
\end_layout

\begin_layout Itemize
Comparison of Detection Performance of Proposed Statistics, Page 25, Line
 47 … the former three is that they don’t rely on random samples CHANGE
 TO the former three is that they do not rely on random samples.
 
\end_layout

\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for the detailed revisions of the paper.
 We have made the changes accordingly and carefully proofread the rest of
 the paper again.
\end_layout

\end_deeper
\begin_layout Section
Reviewer: 2
\end_layout

\begin_layout Enumerate
I like the intuition that is conveyed regarding why (9) by itself is not
 a good monitoring statistic and how (10) overcomes this.
 My primary concern is that the paper is not written with the JQT audience
 in mind and assumes readers are more familiar with VAE and deep learning
 concepts than typical JQT readers are.
 I hope that the authors can improve upon this aspect, since I think it
 could make a nice contribution to JQT.
 Many of my comments below relate to this.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 We have revised the paper accordingly, with the consideration of the JQT
 audience in mind.
 Please see the detailed comments below.
 
\end_layout

\end_deeper
\begin_layout Enumerate
The writing is very sloppy.
 Even the very first sentence of the abstract is incomplete and clearly
 was not proofread.
 There are also a lot of typos and other careless mistakes, e.g., inconsistencies
 in the list of references, like some names being replaced by et al.
 The writing must be improved to be suitable for JQT.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Sorry for the typos.
 We have proofread the papers again and all the revision of the papers are
 highlighted.
 
\end_layout

\end_deeper
\begin_layout Enumerate
Even more critical than sloppy writing is that it is not written with the
 JQT audience in mind.
 For example, the three lines before EQ.
 (6) need elaboration.
 I am pretty familiar with VAEs and also with multivariate SPC, but I am
 still having trouble following what the authors mean.
 I think many JQT readers, especially those less familiar with "machine
 leaning" will not be able to follow.
 The same goes for much of the rest of the paper, e.g., the arguments in Section
 3.1, which is really the main contributions of the paper.
 I was able to follow, but I had to read it a few times, and I don't think
 most JQT readers will do that.
 The writing should be more tailored to the JQT audience.
 
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for the suggestions.
 We have added more details and visualization to exaplain our Section 3.1.
 
\end_layout

\end_deeper
\begin_layout Enumerate
On page 12, I recommend rewording the statement "Our first major claim is
 that latent variable-based statistics are not useful for profile monitoring
 ..." I think you mean that using only (9) (the T^2-like component) by itself
 is not useful.
 But since (10) can also be viewed as a latent variable-based statistic,
 the current wording sounds like you are saying that using (9) and (10)
 together are not useful.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for this important suggestion.
 Given that the models we are using belong to the latent-variable models,
 it is quite understandable that the word 
\begin_inset Quotes eld
\end_inset

latent variable-based
\begin_inset Quotes erd
\end_inset

 may cause confusions on which statistics it actually refer to, either (9)
 or (10).
 To make it more clear, we will use the space in which these two monitoring
 statistics were computed to refer to these two statistics.
 Thus, we referred the statistics (10) as the 
\begin_inset Quotes eld
\end_inset

residual space statistics
\begin_inset Quotes erd
\end_inset

 and the statistics (9) as the 
\begin_inset Quotes eld
\end_inset

latent space statistics
\begin_inset Quotes erd
\end_inset

.
 
\change_inserted -1806609307 1606630566

\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1606630594
Have we defined the latent space and the residual space before in the paper?
 
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, the point we try to make in this paper is 
\change_deleted -1806609307 1606628809
slightly different.
 We claim
\change_unchanged
 that 
\change_inserted -1806609307 1606628828
the latent space statistics 
\change_unchanged
(9) (the T^2-like component) should not be used 
\emph on
at all
\emph default
.
 They are unreliable and that has to do with the typical incorrectness of
 the latent representations learned by deep encoders.
 We agree with your comment that we haven't made that clear enough in the
 paper.

\change_deleted -767166615 1606667954
 
\change_inserted -1806609307 1606630491

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1606630510
To reflect the this, 
\change_deleted -1806609307 1606630504
This is why
\change_unchanged
 we did major revision to the related parts of Section 3.1 and also to Section
 4.2
\change_inserted -1806609307 1606634174
 by changing the name of 
\begin_inset Quotes eld
\end_inset

latent variable-based statistic
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Quotes eld
\end_inset

latent space statistics
\begin_inset Quotes erd
\end_inset


\change_unchanged
.

\change_inserted -1806609307 1606634188
 We also made the followng adjustment in the paper: 
\begin_inset Quotes eld
\end_inset


\begin_inset Quotes erd
\end_inset

 
\change_unchanged

\end_layout

\end_deeper
\begin_layout Enumerate
Why are you considering a VAE, as opposed to just an AE, i.e., why do we need
 the variational part? By this, I mean why shouldn't we omit any assumed
 distribution p(z) and just fit a regular autoencoder? With a regular AE,
 we still have the encoded variables 
\begin_inset Formula $z=\mu_{\phi}(x)$
\end_inset

, the decoded approximation 
\begin_inset Formula $\mu_{\theta}(z)$
\end_inset

, and the "residuals" 
\begin_inset Formula $x-\mu_{\theta}(z)$
\end_inset

, so we could still consider the same two 
\begin_inset Formula $T^{2}$
\end_inset

-like monitoring statistics for z and for the residuals.
 As I mention in a comment below, I suspect part of the reason for the failure-t
o-extrapolate is that the assumed p(z) acts like a regularizer and prevents
 some of the z's from falling outside the normal region, even when the x
 falls outside its region.
 Maybe you would not have this failure-to-extrapolate problem if you use
 a regular AE instead of a VAE.

\change_deleted -767166615 1606681276
 
\change_unchanged

\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for this comment.
 This is indeed an interesting point.
 The major issue of using a regular AE is the irregular latent space mapping
 and the challenge of setting the decision boundary around this prior-free
 cloud of points related to the latent-space statistics.

\change_deleted -767166615 1606681188
 
\change_unchanged

\end_layout

\begin_layout Standard
For example, one can think of a number of alternatives: (1) fitting a multivaria
te Gaussian, (2) using a minimum enclosing hyperrectangle (whether axis-parallel
 or arbitrarily oriented), (3) using average distance to K-nearest neighbors
 or (4) fitting a Kernel density estimation model.
 All these alternatives come with an implicit assumption around the distribution
 of the in-control samples.
 More often than not, it is very unlikely to guess which one will work best
 for any given dataset, especially when deep learning based encoder-decoder
 structures are used against very complicated datasets such as images.

\change_deleted -767166615 1606681188
 
\change_unchanged

\end_layout

\begin_layout Standard
To illustrate the ineffectiveness of the AE method, we ran another set of
 experiments.
 For fairness, we kept the same convolutional encoder-decoder architecture
 as in VAE for the real-case study.
 The experiments were run on the same train-validation-test split of the
 rolling image dataset.
 We plot the encoded variables 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $z=\mu_{\phi}(x)$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 below for when the compressed length is 2, similar to the way we did it
 in the paper.
 We also provide some decision boundary alternatives, in line with our discussio
n above.
 We can observe that using regular AE does not necessarily mitigate the
 issue in question.
 Other than classes 3, 5 and 6, none of the class encodings (shown in blue)
 really deviate too much from the center of gravity of the validation points
 (shown in red).
 Classes 3, 5 and 6 are not surprising either.
 They are the ones that are picked up the most by latent space statistics
 (please see column 
\begin_inset Formula $H^{2}$
\end_inset

 in Table 3 in page 28).
 This suggests that AE and VAE behave quite similar regardless of the regulariza
tion brought up by VAE.
 Overall, we like to conclude that the lack of the extrapolation issue is
 an inherent problem of deep neural network including both VAE and AE, which
 has been discussed in Appendix.
 B.
 As a result, we kindly reject the suspicion that VAE regularization is
 the culprit.
\change_inserted -1806609307 1606631836

\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename scatter.pdf
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Encoded variables with a regular AE.
 Validation samples are shown in red.
 Out-of-control class samples are shown in blue.
 Black lines indicate the contours of the fit wherever applicable.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Regarding point 1 on page 12: For the standard linear PCA-based monitoring,
 it is well known that only monitoring the dominant PC space misses shifts
 that occur outside of this space.
 I recommend including an older reference on this, and relating your point
 to the analogous point for linear PCA.
 This will help JQT readers (who are familiar with the linear situation)
 to better relate to the nonlinear situation.
 
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for this 
\change_deleted -1806609307 1606633598
crucial
\change_inserted -1806609307 1606633598
great
\change_unchanged
 point
\change_inserted -1806609307 1606633600
s
\change_unchanged
.
 
\change_inserted -1806609307 1606633696
We agree that the previous 
\begin_inset Quotes eld
\end_inset


\change_unchanged
Point 1
\change_inserted -1806609307 1606633699

\begin_inset Quotes erd
\end_inset

 
\change_deleted -1806609307 1606633703
 can be
\change_inserted -1806609307 1606633703
is
\change_unchanged
 misleading for our readers
\change_inserted -1806609307 1606634110
.

\change_deleted -1806609307 1606633731
 given the conclusion we want to lead them to in this paper so this comment
 is quite relevant
\change_unchanged
.
 
\change_deleted -1806609307 1606633900
This is also related to the 4th comment you made so we hope to provide a
 more thorough answer to that too.
\change_unchanged

\end_layout

\begin_layout Standard
Our explanation of the sources of disturbances in Section 3.1 is at the core
 of our argument.
 There, we make the distinction of the disturbances in the latent distribution
 
\begin_inset Formula $p_{\delta}(z)\neq p(z)$
\end_inset

 and the residual distribution 
\begin_inset Formula $p_{\delta}(x|z)\neq p(x|z)$
\end_inset

, given the overall probability of observing any sample when the process
 is in-control can be described as 
\begin_inset Formula $p(x)=\int p(x|z)p(z)dz$
\end_inset

.
 The purpose of a 
\begin_inset Formula $T^{2}$
\end_inset

-chart in PCA is to monitor the dominant PC space or in the probabilistic
 paradigm, 
\change_inserted -1806609307 1606633951
i.e., 
\change_unchanged
the disturbance in the latent distribution.
 
\change_deleted -1806609307 1606633964
This is a valid concern when PCA is used.
 
\change_unchanged
The point we are trying to make in this paper is that, unlike PCA-based
 profile monitoring, latent space statistics, such as 
\begin_inset Formula $T^{2}$
\end_inset

, should not be considered 
\emph on
at all
\emph default
 if VAE is being used.
 In other words, given a limited budget for false positives, 
\begin_inset Formula $T^{2}$
\end_inset

-chart is not worth having alongside with 
\begin_inset Formula $Q$
\end_inset

-chart when VAE is used, due to the incorrectness of the latent mapping
\change_deleted -1806609307 1606633994
 semantics
\change_unchanged
.
 
\change_deleted -1806609307 1606634012
We tested this in the simulation study, by creating two such disturbances,
 radius shift and location shift.
\change_inserted -1806609307 1606634378
The ineffectiveness of the latent space statistics is further illustrated
 in the simulation and case study.
 
\end_layout

\begin_layout Standard

\change_inserted -1806609307 1606634564
To help the readers to understand why Q-chart will only be used.
 We have added the following explanation in the revised paper: 
\begin_inset Quotes eld
\end_inset

Similar behaviors have been observed in PCA that using the 
\begin_inset Formula $T^{2}$
\end_inset

 statistics on a few dominant PC may miss the shifts that happended in the
 residual space [ref].
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard

\change_inserted -1806609307 1606634213
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1606634590
I felt that the reviewers just ask can we explain why only Q-chart should
 be used for VAE from the PCA-point of view.
 Therefore, I think it is good for us just to add this sentence in the response.
 
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted -1806609307 1606634201
Here, we made the following adjustment in the revised manuscript as follows:
\begin_inset Quotes eld
\end_inset


\begin_inset Quotes erd
\end_inset

 
\change_unchanged

\end_layout

\end_deeper
\begin_layout Enumerate
Figure 2 and Point 2 on page 13 needs a lot more elaboration.
 It is making an important argument, but I don't think readers without strong
 machine learning background will follow with the current level of detail.
 
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for this suggestion.
 We certainly agree with it.
 Figure 2 is updated in a way that it represents the entire story of the
 paper.
 In line with your comments about the paper being accessible by JQT audience,
 we also skimmed the details about the properties of latent representations,
 such as disentanglement or invariance to nuisance factors.
 The main idea that we want to convey to the readers in that section is
 the unreliability of the semantics of deep encoder learned latent representatio
ns when it comes to profile monitoring.
 
\change_deleted -1806609307 1606634621
Please visit the updated Section 3.1.
\change_inserted -1806609307 1606634632
Here are the major changes that we made in Section 3.1: 
\end_layout

\begin_layout Itemize

\change_inserted -1806609307 1606634714
We removed the entire concepts of the disentanglement.
 For more discussion about the disentanglement, please visit the Comment
 8.
 
\end_layout

\begin_layout Itemize

\change_inserted -1806609307 1606634715
xxx
\change_unchanged

\end_layout

\end_deeper
\begin_layout Enumerate
Further regarding Point 2 on page 13: I understand the failure-to-extrapolate
 argument and how it results in missed detections.
 But the entangled latent space argument is less clear.
 It seems to me that the entangled latent space situation depicted in the
 bottom-right of Figure 2 results in a missed signal because it also fails
 to extrapolate.
 So the real reason in both the bottom-left and bottom-right is failure
 to extrapolate.
 Am I missing something?
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for this valuable comment.

\change_inserted -1806609307 1606634908
 We totally agree the 
\begin_inset Quotes eld
\end_inset

failure-to-extrapolate
\begin_inset Quotes erd
\end_inset

 is the major reason for missed detections.
 
\change_deleted -1806609307 1606634882
 
\change_inserted -1806609307 1606634893

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1606634913
We also agree with your comment 
\change_unchanged
that
\change_inserted -1806609307 1606634768
 under some circumstances, the
\change_unchanged
 entanglement causes failure to extrapolate which in turn results in Type-II
 error
\change_deleted -1806609307 1606634775
, may be true under certain conditions
\change_unchanged
.
 
\change_inserted -1806609307 1606635010
However, we do not have enough practical evidence or theory to support that
 the entanglement may not lead to the missed detections or failure to extropalat
e.
 
\change_deleted -1806609307 1606635023
However, entanglement is not a necessary condition to failure to extrapolate
 nor the conditions are caused by the same dynamics related to deep learning.
 We do not have enough evidence to support that or can provide theoretical
 guarantees to it.

\change_unchanged
 If you check Figure 2, the representation may have been able to successfully
 extrapolate beyond Point A and result in a correctly identified as an evidence
 to process being out-of-control, even when we had 
\change_inserted -1806609307 1606635038
a 
\change_unchanged
clear entanglement.
 See top right subfigure in Figure 4 for an example.
 When radius is fixed at the mean (
\begin_inset Formula $r=0.15$
\end_inset

), all the in-control samples fall within the region and all the out-of-control
 samples fall outside.
 The representation is clearly entangled.
 Yet the decisions are right and one might say the extrapolation is successful.
 Your comment is valuable in the sense that we did not clearly define what
 it means to fail to extrapolate and what is entanglement to the reader.
 This would confuse our reader
\change_inserted -1806609307 1606634983
s
\change_unchanged
.
 
\change_inserted -1806609307 1606634983

\end_layout

\begin_layout Standard
In the end, 
\change_inserted -1806609307 1606635146
we decide to make the following changes in the revised manuscript:
\change_deleted -767166615 1606709016
 
\change_inserted -1806609307 1606635146

\end_layout

\begin_layout Itemize

\change_inserted -1806609307 1606635206
We remove the concepts of disentangment, given it is even not clearly defined
 in the deep learning community and the causal relationship of the disentangment
 and anomaly detection is not supported by the experiments.
 
\end_layout

\begin_layout Itemize

\change_inserted -1806609307 1606635251
We use the word 
\begin_inset Quotes eld
\end_inset

incorrect mapping of latent space
\begin_inset Quotes erd
\end_inset

 
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1606635232
Is it true? What word do you use?
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\change_inserted -1806609307 1606635257
xxx? 
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1606635261
Can you explain more?
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted -1806609307 1606635237
we decided to keep it simple for the audience and call any mapping that's
 semantically wrong as 
\emph on
incorrect
\emph default
, which we claim to be the culprit of the inaccurate decision made by the
 latent space statistics.
\change_unchanged

\end_layout

\end_deeper
\begin_layout Enumerate
Further regarding Point 2: The crux of the failure-to-extrapolate issue
 is that unusual x values might get mapped to typical z values, and therefore
 cause false negatives.
 Is this because of the structure of the decoder part (which might suggest
 a different structure could be more suitable for this type of VAE-based
 process monitoring), or is it because of the implicit regularization of
 z when fitting the VAE via the assumed p(z), or both.
 I suspect the latter is just as important if not more important, but Appendix
 B focuses on the former.
 Could this problem be solved by using a regular AE instead of a VAE (per
 my earlier comment)? Or could it be solved by replacing the Gaussian p(z)
 that is used during the fitting stage with a heavier tailed distribution
 (e.g.
 multivariate t, or mixture of Gaussian and uniform over a much larger support)
 during the monitoring stage? If the same Gaussian p(z) is used during monitorin
g as during fitting the VAE, this is like telling the control chart that
 you expect the data during monitoring to behave the same as the data during
 fitting, i.e., that process shifts are not likely to happen.
 The Gaussian/uniform mixture p(z) would allow you to specify "prior" probabilit
ies on how likely a shift is to occur.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for this fruitful discussion.
 
\change_inserted -1806609307 1606636159
First, 
\change_deleted -1806609307 1606636160
W
\change_inserted -1806609307 1606636160
w
\change_unchanged
e 
\change_inserted -1806609307 1606635277
have 
\change_unchanged
partially answer
\change_inserted -1806609307 1606635280
ed
\change_unchanged
 the your comments related to AE in the response of your 5th comment.
 Even when we use a regular AE, the latent representations are biased towards
 the center of gravity of the training representations
\change_inserted -1806609307 1606636053
 and the 
\begin_inset Quotes eld
\end_inset

failure to extropalation issue still occur
\begin_inset Quotes erd
\end_inset


\change_unchanged
.
 
\change_inserted -1806609307 1606636161

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1606635324
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1606635540
I think we can still replace the prior distribution by others during the
 training stage.
 So I remove your comments of asking how to do that.
\end_layout

\end_inset


\change_deleted -1806609307 1606635689
For your other comment, we don't quite understand how the prior can be replaced
 by some other distribution during the monitoring stage.
 The model only infers a single mean vector and the diagonals of a single
 covariance.
 The multivariate-t would need an additional degrees of freedom or a mixture
 of Gaussians would need multiple means and covariances to be inferred.
 Once the architecture and the weights are fixed, we don't think it is feasible
 to do that.
 Besides, theoretically, we don't see anything wrong about using the same
 VAE model during monitoring stage.
 
\change_inserted -1806609307 1606636141

\end_layout

\begin_layout Standard

\change_inserted -1806609307 1606635739
We would like to emphasize that the major reason that the latent statistics
 doesn't work well is shown in 
\change_deleted -1806609307 1606635735
The job of VAE is to model the pdf of the distribution that generates the
 in-control samples 
\begin_inset Formula $p(x)$
\end_inset

 and this is very neatly explained in Section 1.3 of 
\begin_inset Quotes eld
\end_inset

An Introduction to Variational Autoencoder
\begin_inset Quotes erd
\end_inset

 by Kingma and Welling (2019) which is a follow-up paper by the same authors
 who invented the VAE model.
 In theory, having an accurate pdf of in-control samples is exactly what
 we need.
 We don't need to make any assumption about the data stream, before or after
 monitoring, as the domain of the pdf is the entire space of all possible
 inputs.
 Using the pdf, we can simply monitor a new sample 
\begin_inset Formula $x_{i}$
\end_inset

 by checking 
\begin_inset Formula $p(x_{i})<\tau$
\end_inset

.
 The importance of 
\change_unchanged
Appendix.B
\change_inserted -1806609307 1606635746
, which
\change_unchanged
 
\change_deleted -1806609307 1606635616
in this study is that it 
\change_unchanged
reveals that we cannot trust the output of 
\begin_inset Formula $p(x_{i})$
\end_inset

 when 
\begin_inset Formula $x_{i}$
\end_inset

 is not coming from high density regions of in-control distribution
\change_inserted -1806609307 1606635676
s
\change_unchanged
.
 
\change_inserted -1806609307 1606635668

\end_layout

\begin_layout Standard

\change_deleted -1806609307 1606635639
In practice, there is another issue.
 Computing 
\begin_inset Formula $p(x_{i})$
\end_inset

 is not tractable so one is d
\change_inserted -1806609307 1606635644
 
\change_deleted -1806609307 1606635639
oomed to rely on approximations.
 These two problems are what we are revealing and addressing in this paper,
 which previous papers did not.

\change_inserted -1806609307 1606635645
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted -1806609307 1606635665
I felt we don't have any proof on this sentence, even it might be true so
 I like to remove it.
\end_layout

\end_inset


\change_unchanged

\end_layout

\end_deeper
\begin_layout Enumerate
Section 4.1: I don't recognize this as the gasket bead example from Shi,
 Apley and Runger (2016).
 It might represent some other type of bead that changes position/shape,
 but it doesn't seem to have much connection to gaskets.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for this comment.
 We actually did not claim it is the same but it is only inspired, especially
 in the sense that one variation is affecting the location of the peak of
 the profile while the other affect the shape.
 On the other hand, it is our mistake that we kept the wording gasket bead
 because just as your comment suggests, a 2D point cloud with a circular
 shape does not have much connection to the gasket bead component anymore.
 We made sure all the related wordings are changed to dome.
\end_layout

\end_deeper
\begin_layout Enumerate
Additional comments that are minor by themselves but related to the bigger
 problem of poor writing in general:
\end_layout

\begin_deeper
\begin_layout Itemize
First paragraph of intro: In "...
 intra-sample variation lies on a nonlinear low-dimensional manifold" it
 is not clear what "sample" means.
 Does "intra-sample variation" just mean variation across a sample of parts,
 items, images, etc? Please clarify.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for the comments.
\end_layout

\end_deeper
\begin_layout Itemize
Page 5, 3rd contribution bullet: It sounds like you are saying latent variable-b
ased monitoring should not be used, but it seems the entire paper is about
 latent variable-based monitoring.
 VAEs are latent variable models.
 Please clarify.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for the comments.
\end_layout

\end_deeper
\begin_layout Itemize
In Eq.
 (5), I can't find where 
\backslash
mu_{
\backslash
phi}(x) has been defined.
 I can guess what it is, but readers shouldn't have to guess.
 There are quite a few other undefined or unclearly defined notations.
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for the comments.
\end_layout

\end_deeper
\begin_layout Itemize
On page 11, does "this is actually true for linear latent variable models"
 mean "this holds exactly for linear latent variable models"?
\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\bar under
Ans:
\series default
\bar default
 Thank you for the comments.
\end_layout

\end_deeper
\end_deeper
\end_body
\end_document
