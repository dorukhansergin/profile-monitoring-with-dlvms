 %% LyX 2.3.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,headings=standardclasses]{scrartcl}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathptmx}
\usepackage{newtxmath}
\renewcommand{\familydefault}{\rmdefault}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{color}
\usepackage{array}
\usepackage{verbatim}
\usepackage{float}
\usepackage{rotfloat}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.




\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage{cleveref}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{stfloats}
\usepackage[super]{nth}
\usepackage{subcaption}
\usepackage{caption}
% for inkscape images
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgf}



\g@addto@macro\@floatboxreset\centering


% \usepackage{hyperref}
 
 \AtBeginDocument{% Overrides ref for Cref
 	\let\ref\Cref
 }

\crefalias{prop}{proposition}

% TIKZ
\usepackage{tikz}

% -- Arrows
\usetikzlibrary{arrows}

% --Bayesnet
\usetikzlibrary{bayesnet}
\usetikzlibrary{decorations.pathreplacing}

\tikzset{
	diagonal fill/.style 2 args={fill=#2, path picture={
			\fill[#1, sharp corners] (path picture bounding box.south west) -|
			(path picture bounding box.north east) -- cycle;}},
	reversed diagonal fill/.style 2 args={fill=#2, path picture={
			\fill[#1, sharp corners] (path picture bounding box.north west) |- 
			(path picture bounding box.south east) -- cycle;}}
}

\tikzstyle{partialobs} = [latent,diagonal fill={gray!25}{gray!0}]

%\@ifundefined{showcaptionsetup}{}{%
% \PassOptionsToPackage{caption=false}{subfig}}
%\usepackage{subfig}
%\makeatother

\usepackage[style=chicago-authordate,isbn=false,url=false,eprint=false,minbibnames=10, maxbibnames=10,maxcitenames=3,firstinits=true]{biblatex}
\DeclareFieldFormat[article,inbook,book,incollection,inproceedings,patent,thesis,unpublished]{citetitle}{#1}
\DeclareFieldFormat[article,inbook,incollection,inproceedings,patent,thesis,unpublished]{title}{#1} 

\providecommand{\propositionname}{Proposition}
\providecommand{\theoremname}{Theorem}

\addbibresource{bibliography.bib}
\begin{document}
% INPUT PREAMBLES
\include{preamble_glossary}
\title{Toward a Better Monitoring Statistic for Profile Monitoring with Variational
Autoencoders}
\maketitle
\begin{abstract}
Due to the recent success of deep learning models, nonlinear profile
monitoring schemes based on deep generative models specifically for
variational autoencoders. While these works show impressive results
over classical methods, the proposed monitoring statistic often ignore
shortcomings of learned lower-dimensional representations and computational
limitations of real-world high-dimensional systems. In this work,
we first manifest these issues and then overcome them with a novel
statistic formulation that both increases out-of-control detection
accuracy and computational efficiency. We demonstrate our results
both on a carefully designed simulation study and a real-life example
of image profiles obtained from a hot steel rolling process.

\medskip{}

\textbf{Keywords: }  deep learning, high-dimensional nonlinear profile, latent variable model, profile monitoring, variational autoencoder,
\end{abstract}

\section{Introduction}

\label{sec:introduction} Profile monitoring has attracted a growing
interest in the literature in the past decades for its ability to
construct control charts with much better representations for certain
types of process measurements \parencite{Woodall2004-bp,Woodall2007-xs,Maleki2018-uo}.
A profile can be defined as a functional relationship between the
response variables and explanatory variables or spatiotemporal coordinates.
In this work, we focus on the case where the profiles generated from
the process are high-dimensional (HD)\textit{\emph{, i.e.}}, the number
of such explanatory variables or spatiotemporal coordinates are large.
Specifically, we focus on sets of HD profiles for which intra-sample
variation lies on a nonlinear low-dimensional manifold \parencite{Shi2016-tg}.
Our motivating example of such HD profiles is presented in \ref{fig:Rolling}
below, in which we exhibit a sample of surface defect image profiles
collected from a hot steel rolling process.

\begin{figure}[H]
\includegraphics[width=0.5\textheight]{profile_examples.pdf} \caption{A collection of 64 by 64 image profiles taken from a hot steel rolling
process.}
\label{fig:Rolling}
\end{figure}

In literature, profile monitoring techniques can be categorized by
their assumptions on the type of functional relationship. Linear profile
monitoring can be considered the most basic profile monitoring technique,
in which it is assumed that the profile can be represented by a linear
function. The idea is to extract the slope and the intercept from
each profile and monitor its coefficients \parencite{zhu2009monitoring}.
Regularization techniques can also be used in linear profile estimation.
For example, \textcite{zou2012lasso} utilizes a multivariate linear
regression model for profiles with the LASSO penalty and use the regression
coefficients for Phase-II monitoring. However, the linearity assumption
can be quite limiting. To address this challenge, nonlinear parametric
models are proposed \parencite{Williams2007-ty,Jensen2009-tu,Noorossana2011-oj,Maleki2018-uo}.
These models assume an explicit family of parameterized functions
and, their parameters are estimated via nonlinear regression. In both
cases, the drawback of both linear and nonlinear parametric models
is that they assume the parametric form is known beforehand, which
might not always be the case.

Another large body of profile monitoring research focuses on the type
of profiles where the basis of the representation is assumed to be
known, but the coefficients are unknown. For instance, to monitor
smooth profiles, various non-parametric methods based on local kernel
regression \parencite{zou2008monitoring,qiu2010nonparametric} and
splines \parencite{chang2010statistical} are developed. To monitor
the non-smooth waveform signals, a wavelet-based mixed effect model
is proposed \parencite{paynabar2011characterization}. However, for
all the aforementioned methods, it is assumed that the nonlinear variation
pattern of the profile is well captured by a known basis or kernel.
Usually, there is no guidance on selecting the right basis of the
representation for the original data and it requires
trial and error to find the right basis.

In the case that the basis of HD profiles are not known, dimensionality
reduction techniques are widely used. Principal component analysis
(PCA) is arguably the most popular method in this context for profile
data monitoring because of its simplicity, scalability, and good data
compression capability. In \textcite{liu1995control}, PCA is proposed
to reduce the dimensionality of the streaming data and, $T^{2}$ and
$Q$ charts are constructed to monitor the extracted representations
and residuals, respectively. To generalize PCA methods to monitor
the complex correlation among the channels of multi-channel profiles,
\textcite{paynabar2015change} propose a multivariate functional PCA
method and apply change point detection methods on the function coefficients.
Along this line, tensor-based PCA methods are also proposed for multi-channel
profiles, examples including uncorrelated multi-linear PCA \parencite{paynabar2013monitoring}
and multi-linear PCA \parencite{grasso2014profile}. Finally, various
tensor-based PCA methods \parencite{yan2015image} are compared and
different test statistics are developed for tensor-based process monitoring.

The main limitation of all the aforementioned PCA-related methods
is that the expressive power of linear transformations is very limited.
Furthermore, each principal component represents a global variation
pattern of the original profiles, which is not efficient at capturing
the local spatial correlation within a single profile. Therefore,
PCA requires much larger latent space dimensions than the dimension
of the actual latent space, yielding a sub-optimal and overfitting-prone
representation. This phenomenon hinders profile monitoring performance.

A systematic discussion of this issue is articulated in \parencite{Shi2016-tg}.
In that work, the authors identify the problems associated with assuming
a closeness relationship in the subspace that is characterized by
Euclidean metrics. They successfully observe that the intra-sample
variation in complex high-dimensional corpora may lie on a nonlinear
manifold as opposed to a linear manifold, which is assumed by PCA
and related methods. However, the authors only focus on applying manifold
learning for Phase-I analysis, while the Phase-II monitoring procedure
is not touched upon.

Deep dimensionality reduction models have been proposed as an alternative
to classical dimensionality reduction techniques in a handful. Deep
autoencoders have been proposed for profile monitoring for Phase-I
analysis in \parencite{Howard2018-op}. \textcite{Yan2016-wa} compared
the performance of contractive autoencoders and denoising autoencoders
for Phase-II monitoring. \textcite{Zhang2018-js} proposed a denoising
autoencoder for process monitoring. Aside from deterministic deep
neural networks, only three works \parencite{wang2019systematic,Zhang2019-lu,lee2019process}
proposed to use deep probabilistic latent variable models, specifically,
variational autoencoders (VAE), for Phase-II monitoring. All the monitoring statistics in those works differ slightly, but
they are all extensions of the classic $T^{2}$ and $Q$-charts of
PCA. We argue that there is room for improvement for the monitoring
statistic formulations in those works for several reasons, especially
when high-dimensional profiles are considered. In this work, we propose
a new monitoring statistic formulation to address this issue.

The contributions of this work are as follows:
\begin{itemize}
\item We compare all the existing monitoring statistics for deep latent
variable models with the focus on the VAE model and propose a better
monitoring statistic that responds robustly and, in most cases, more
accurately than previous formulations requiring only a single pass
through the autoencoder.
\item We give insight into the existing monitoring statistics for latent-variable
models with a focus on variational autoencoders and classify them
based on the latent-variable based and residual space monitoring statistics.
We also show how these statistics are natural extensions to the monitoring
statistics for traditional dimension reduction methods such as probabilistic
PCA.
\item Unlike traditional monitoring statistics for PCA,
we demonstrate through extensive experiments and give insight on why latent space monitoring
statistics should not be used due to the "incorrect" mapping of the latent variable and the encoder will likely fail to extrapolate well beyond the region of in-control profiles.
\item We derive two approximations on the residual space monitoring statistics
leveraging on the first-order and second-order Taylor expansion and
compare with the sampling-based monitoring statistics. We have demonstrated
that the first-order approximation of the residual space monitoring
statistics gives the best overall detection accuracy and is computationally
efficient.
\item We demonstrate the effectiveness of this new formulation on both simulation
and real-life case studies and conclude that residual space monitoring
statistics with deep learning architecture as encoders and decoders
outperform other monitoring statistics and the traditional non-deep-learning-based
methods.
\end{itemize}
%
The rest of the paper is organized as follows: \ref{sec:Background}
first introduces variational autoencoders and reviews traditional
$T^{2}$ and $Q$ charts of PCA as well as the existing monitoring
statistics for VAE. \ref{sec:methodology} introduces the proposed
monitoring statistic and gives insights on its meaning and mathematical
relationship with the traditional PCA methods. \ref{sec:Simulation-Study-Analysis}
shows the simulation study to demonstrate the performance of the proposed
method and give insight into why only the residual space statistics
is recommended and how to make sure it is computationally efficient.
Finally, \ref{sec:case-study} demonstrates the advantages of the
proposed methodology on a real-life case study,
using images from hot-steel rolling processes.

\section{Background \label{sec:Background}}

In this section, we will first briefly review the methodology of Variational Autoencoder (VAE) \parencite{Kingma2013-dl} in \ref{sec:bckgrnd:lvms}. Furthermore, we will review the existing monitoring statistics in the literature based on latent variables models including the PCA methods and VAE methods in \ref{sec:bckgrnd:ReviewPCA} and \ref{sec:bckgrnd:critique}, respectively. 

\subsection{Variational Autoencoders \label{sec:bckgrnd:lvms}}

In this section, we will first introduce Variational Autoencoder (VAE) \parencite{Kingma2013-dl},
which is the primary modeling framework in this work. The Gaussian factorized
latent variable model perspective of VAEs is crucial to understand
the role of this model in the context of the profile monitoring. This
is why we begin with an introduction to latent variable modeling.

Latent variables models are powerful tools to model complex distributions
over high-dimen\-sional spaces. The underlying assumption is that there
exists a low-dimensional latent structure that explains well the variations
in the high-dimensional observed space. Typically, the density over
observed variables can be decoupled into the distribution on the latent
variables $\pz$ and the conditional distribution of observed variables
given latent variables $p(\mbx\g\mbz)$. Then, these distributions
can be assigned to tractable families of distributions, such as Gaussian.
This enables a more efficient modeling of the data distribution $p(\mbx)$.

A typical example of latent variable models is when the joint distribution
is Gaussian factorized as in \ref{eq:gaussian-factorized}. 
\begin{equation}
\begin{split}\pz & =\Norm(\mbz;0,\mbI_{r})\\
\decoding & =\Norm(\mbx;\mu_{\mbtheta}(\mbz),\sigma^{2}\mbI_{d})\\
p_{\mbtheta}(\mbx,\mbz) & =\decoding\pz
\end{split}
\label{eq:gaussian-factorized}
\end{equation}
In the above formulation, $\mbx\in\R^{d}$ are observed samples, and
$\mbz\in\R^{r}$ are latent variables while $\mu_{\mbtheta}\colon\R^{r}\to\R^{d}$
is a function parameterized by $\mbtheta\in\Theta$, which describes
the relationship between the latent variables and the mean of the
conditional distribution. The Gaussian prior $\pz$ is typically chosen
to be standard multivariate Gaussian distribution to avoid degenerate
solutions \parencite{roweis1999unifying} and conditional covariance
is typically assumed to be isotropic $\sigma^{2}I_{d}$ to avoid ill-defined
problems. The aim is to approximate the true density $p_{\mbtheta}(\mbx)\approx p(\mbx)$
and this approximation can be obtained through marginalization: 
\[
p_{\mbtheta}(\mbx)=\int p_{\mbtheta}(\mbx,\mbz)d\mbz
\]
% Finally, in literature, there have been discussions about whether the independent latent structure assumption $pz=\Norm(\mbz;0,\mbI_{r})$ can lead to the discovery of the true disentangled variations, a task also known as disentangled representation learning \parencite[Sec. 3.5]{bengio2013representation}. Disentangled representations are useful to represent variations in latent variations due to their ability to separate the independent factors. However, a discussion on what disentangled representation implies for profile monitoring is necessary. We are interested in whether and if so, how such representations will be critical for profile monitoring.
% I think we delete all about disentanglement

A famous member of the family of models described above is the probabilistic
principal component analysis (PPCA) \parencite{tipping1999probabilistic}.
The parameters are optimized via a maximum likelihood estimation framework
and it can be solved analytically since $\mu_{\mbtheta}$ is a simple
linear transformation. This enables reusing analytical results from
solutions to the classical PCA problem. The assumption of PPCA that
the latent and observed variables have a strictly linear relationship
is restrictive. In real-world processes, it is likely that this relationship
is highly nonlinear. Deep latent variable models are a marriage of
deep neural networks and latent variable models that aim to solve
this problem. Deep learning has enjoyed a tremendous resurgence in
the last decade due to their superior performance that was unprecedented
for many tasks such as image classification \parencite{krizhevsky2012imagenet},
machine translation \parencite{bahdanau2014neural}, and speech recognition
\parencite{amodei2016deep}. In theory, under sufficient conditions,
a two-layer multilayer perceptron can approximate any function on
a bounded region \parencite{cybenko1989approximation,Hornik1991-li}.
However, growing the width of shallow networks exponentially
for arbitrarily complex tasks is not practical. It has been shown
that deeper representations can often achieve better expressive power
than shallow networks with fewer parameters due to the efficient reuse
of the previous layers \parencite{eldan2016power}.

VAE is arguably the most foundational member of the deep latent variable
model family. The main difference between PPCA and VAE is that VAE
replaces the linear transformation with a high-capacity deep neural
network (called \textit{generative} or \textit{decoder}). This is
powerful in the sense that, along with a general-purpose prior $\pz$,
deep neural networks can transfer such prior to model a wide variety
of densities to model the training data \parencite{kingma2019introduction}.
Unlike PPCA, these models will not have analytical solutions due to
the complex nature of the neural network used. Like most other deep
learning models, their parameters are often optimized via various variants of stochastic gradient 
descent optimizers. The problem becomes even harder given
the observation that the posterior $\decoding$ takes meaningful values
only for a small sub-region within $\R^{r}$. This makes sampling
from the prior $\pz$ to estimate the likelihood prohibitively expensive.
Both models work around this problem using the importance sampling
framework \parencite[532]{bishop2006pattern}, where they introduce
another network (called \textit{recognition} or \textit{encoder})
to approximate a proposal distribution $\encoding$ ---parametrized
by $\mbphi$--- which aims to sample latent variables from
a much smaller region that is more likely to produce higher posterior
densities for a given input $\mbx$. In fact, PPCA can be treated
as a special case of VAE, where the decoder is modeled by linear transformation.

One important output of a trained VAE is the likelihood estimator.
Once the two networks are trained, the log-likelihood $\log\ptheta(\mbx)$
can be approximated by a Monte Carlo sampling procedure with $L$
iterations \parencite[30]{kingma2019introduction}: 
\begin{equation}
\log\ptheta(\mbx)\approx\log\frac{1}{L}\sum_{l=1}^{L}\frac{\ptheta(\mbx,\mbz^{(l)})}{\qphizgivenx{\mbz^{(l)}}{\mbx}}.\label{eqn:SummationLL}
\end{equation}

However, the Monte Carlo sampling procedure is shown to be computational inefficient and the evidence lower bound (ELBO),
which is deemed a proxy to the likelihood, is often used: 
\begin{equation}
\begin{split}\text{ELBO} & \triangleq\log\left(p(\mbx)\right)-\KL{\encoding}{q^{*}(\mbz|\mbx)}\\
 & =\E_{\mbz\sim q_{\mbtheta}}\log\decoding+\KL{\encoding}{p(\mbz)},
\end{split}
\label{eqn:VAELoss}
\end{equation}
where $\KL{\cdot}{\cdot}$ denotes the Kullback-Leibler divergence
(KLD) between two distributions. The left-hand side is the quantity
of interest, while the right-hand side is the tractable expression
that guides the updating of parameters $\mbtheta,\mbphi$ in an end-to-end
fashion.

\subsection{Review of $\protect\Tsq$ and $Q$ Statistics in PCA \label{sec:bckgrnd:ReviewPCA}}

Process monitoring via PCA is typically
undertaken using the $\Tsq$ and $Q$ statistics \parencite{Chen2004-px}.
The $Q$ statistic for PCA is defined as the reconstruction error
between the real sample $\mbx$ and the reconstructed sample $\tilde{\mbx}$.
The geometric interpretation of $ Q $ is that it quantifies how far the sample is away from the
learned subspace of in-control samples. $\Tsq$, on the other hand, quantifies the shift along the directions of the most dominant principal components.

The $\Tsq$ statistics and $Q$ statistic for PCA are defined formally as follows:
\begin{equation}
\begin{split}Q(\mbx) & =\gg\mbx-\tilde{\mbx}\gg^{2}\\
\Tsq(\mbx) & =\mbz^{\top}\mbSigma\inv_{r}\mbz=\mbx^{\top}\mbW_{r}\mbSigma\inv_{r}\mbW_{r}^{\top}\mbx,
\end{split}
\label{eqn: QTPCA}
\end{equation}
where matrix $\mbW_{r}$ is the loading matrix, and $\mbSigma\inv_{r}$
is the inverse of the covariance matrix when only the first $r$ principal
components are kept. There are various methods to choose $r$ such
as fixing the percentage of variation explained \parencite[41]{Chiang2001-nu}.

For processes with relatively small latent and residual dimensionality,
the upper control limits of these statistics for the $\alpha$\% Type-1
error tolerance is constructed by employing the normality assumptions
of PPCA \parencite[43-44]{Chiang2001-nu}. However, using such measures
for high-dimensional nonlinear profiles is prohibitively error-prone
as both $r$ and $d$ will be much higher than the assumptions of
chi-square distribution can tolerate. As an alternative, non-parametric
methods are typically used to estimate these limits, such as simple percentiles or kernel density estimators.

\subsection{Review of Previously Proposed Monitoring Statistics
Proposed for VAE \label{sec:bckgrnd:critique} }

Three works have recently considered
VAE for process monitoring, all of which propose different statistic
formulations for monitoring. \textcite{Zhang2019-lu} propose $H^{2}$, which is basically the
Mahalanobis distance of the mean of the proposal distribution from standard Gaussian distribution. 
\begin{equation}
H^{2}=\mu_{\mbphi}(\mbx)^{\top}\mu_{\mbphi}(\mbx)
\end{equation}

In another work, \textcite{lee2019process} propose two statistics: $T^{2}$ and $SPE$. For a given input $\mbx$, a single sample is drawn from the proposal distribution $\mbz^{(l)}\sim\encoding$ which is used reconstruct the input using the generative model $\mbx^{(l)}\sim p_{\mbtheta}(\mbx\g\mbz^{(l)})$. The proposed test statistics in this work can be formalized follows:
\begin{equation}
\begin{aligned}T^{2} & =(\mbz^{(l)}-\bar{\mbz})^{\top}S_{\mbz}\inv(\mbz^{(l)}-\bar{\mbz})\\
SPE & =\gg\mbx^{(l)}-\mbx\gg_{2}^{2},
\end{aligned}
\end{equation}
where $\bar{\mbz}$ and $S_{\mbz}\inv$ are estimated over a single pass of the entire set of in-control samples. In their methodology, these two statistics work in combination and at least one positive decision from either of the two statistics is enough to claim that the process is out-of-control. 

Finally, \textcite{wang2019systematic} propose the $R$ and $D$ statistics by focusing on the two major components of the tractable part of the objective function of VAE shown as in \ref{eqn:VAELoss}. The $D$ statistic is simply the KL divergence between the prior and proposal. For $R$ statistic, like \textcite{lee2019process}, they employ summary statistics over samples from proposal but also claim that sampling size can be fixed to one: 
\begin{equation}
\begin{aligned}D & =\KL{\encoding}{p(\mbz)}\\
R & =\frac{1}{L}\sum_{l=1}^{L}-\log q_{\mbtheta}(\mbx\g\mbz^{(l)}),
\end{aligned}
\label{eq: DR}
\end{equation}

$SPE$ in and $R$ are essentially the same quantities up to a constant, which makes them identical in the context of monitoring. This is why we will refer to them as $SPE/R$ throughout the rest of the paper.

\section{Methodology \label{sec:methodology}} 

In Section 3, we will start how the concepts of residual space statistics and latent space statistics can be used for VAE and PCA in Section \ref{sec:residual-latent-vae-pca}. Furthermore, we will discuss our refined monitoring statistics designed specific for VAE methods in Section \ref{sec:proposed-statistic}. The implementation details of profile monitoring procedures and neural network architectures are discussed in \ref{sec:methodology:procedure} and \ref{subsec:Model-Architectures}. 



\subsection{Relationship of the Monitoring Statistics for VAE and PCA} 
\label{sec:residual-latent-vae-pca}

In this section, we would like to generalize the definition of the $ \Tsq $ and $ Q $ statistics of the PCA-based monitoring for VAE. This can be achieved by breaking the tractable formulation of \ref{eqn:VAELoss} into two term  as follows:
\[
Q_{VAE}  = \E_{\mbz\sim q_{\mbtheta}}\log\decoding, 
T^2_{VAE} = \KL{\encoding}{p(\mbz)}. 
\]. 
$Q_{VAE}$ and $T^2_{VAE}$ can be used as the monitoring statistics for VAE. where, the ELBO is simply the summation of this two terms as $ELBO = Q_{VAE}+T^2_{VAE}$. 

To understand the rationale behind this, we will revisit the assumptions of the model described in \ref{eq:gaussian-factorized}.
Let us formally represent an out-of-control distribution as the change of $p(\mbx)$. 
Since $p(\mbx)=\int p(\mbx\g\mbz)p(\mbz)d\mbz$, we can anticipate two sources of out-of-control behaviors: a shift in latent distribution $\pz$ or a shift in residual distribution $p(\mbx\g\mbz)$ as follows: 1) For the shift in the residual space $p(\mbx\g\mbz)$, any changes in the residual space $p(\mbx\g\mbz)$ can be reflected by the change of $Q_{VAE}=\E_{\mbz\sim q_{\mbtheta}}\log\decoding$. 
2) Similarly, for the shift in the latent space $p(\mbz)$, any changes of in the latent distribution $\encoding$ can be represented by the $T^2_{VAE}=\KL{\encoding}{p(\mbz)}$. 

This idea is similar to the $T^2$ and $Q$ chart in the PCA analysis, where both terms actually play an important role for process monitoring \parencite{kim2003process}. This also holds for Probabilistic PCA (PPCA) and will become the $T^{2}$ and $Q$ statistics in the traditional monitoring framework. To prove this, we link the $T^{2}$ and $Q$ statistics of PPCA (see \ref{sec:bckgrnd:ReviewPCA}) using the ELBO framework.

\begin{prop}
	\label{prop: T2Q} We know from the definition of PPCA \parencite{tipping1999probabilistic}
	that the prior, encoding and decoding functions are normally distributed
	as: 
	\[
	\begin{split}p(\mbz) & =\Norm(0,\mbI),\\
		\decoding & =\Norm(\mbW\mbz,\sigma^{2}\mbI).\label{eq:Gaussian}
	\end{split}
	\]
	In this case, from PPCA, the encoder can be solved analytically as
	another normal distribution as $\encoding=\Norm(\mu_{\mbphi}(\mbx),\Sigma_{z})$,
	where $\mu_{\mbphi}(\mbx)=\mbM^{-1}\mbW^{\top}\mbx$, $\Sigma_{z}=\sigma^{2}\mbM^{-1}$,
	and $\mbM=\mbW^{\top}\mbW+\sigma^{2}\mbI$. Then, the two monitoring statistics can be derived as follows:
	\begin{equation}
		T^2_{VAE} = \KL{\encoding}{p(\mbz)}=\frac{1}{2}\gg\mu_{\mbphi}(\mbx)\gg^{2}+C_{1},\label{eqn:KL_PPCA}
	\end{equation}
	\begin{equation}
		Q_{VAE} = \E_{\mbz\sim q_{\mbphi}}\log\decoding\propto\gg\mbx-\mbW\mu_{\mbphi}(\mbx)\gg^{2}+C_{2},\label{eqn:E_PPCA}
	\end{equation}
	where $C_{1}$ and $C_{2}$ are constants that do not depend on $x$. The proof is given in \ref{sec:PoofOfPropTQ}
\end{prop}

Note that the constants do not affect the profile monitoring decision as the control limits will be translated accordingly. Thus, the test statistic $T^2_{VAE}$ is equivalent to the $T^{2}$-statistic of PCA and $Q_{VAE}$ is equivalent the $Q$-statistic of PCA. 

Observe that previously proposed formulations mentioned in \ref{sec:bckgrnd:critique} draw inspiration---directly or indirectly---from this framework. Statistics $R$ and $SPE$ are based on the $Q$-statistic. Let us call these \emph{residual space statistics}, as they rely on the sum of squared differences between the signal itself and its predicted value, \ie, residuals. The statistics $H^{2},T^{2}$ and $D$ are based on the $T^{2}$ of PCA. We call these \emph{latent space statistics}, as they rely exclusively on latent representations. 

\ref{fig:pcaVSvae} shows a graphical illustration of this analogy of residual-space statistics and latent space statistics for PCA and VAE. Residual space statistics models the distance of the data with respect to the linear or nonlinear manifolds. The latent space statistics would monitor the distance within the latent space. In linear subspace modeled by the PCA methods, this is the Euclidean distance. However, in the VAE model, this distance should be defined on the nonlinear manifold. 


\begin{figure}[t]
	\begin{centering}
		\includegraphics[width=0.9\textwidth]{PCAvsVAE.pdf}
		\caption{Illustration of the analogy between PCA and VAE. Closed regions describe the lower-dimensional manifold the in-control distribution lies in. The crosses represent the in-control samples observed in Phase-I and the gray region represents the subset of the lower-dimensional manifold where in-control samples are typically sampled from. The observation represented with a circle is typically detected with $ Q $-statistic and the observation represented with a triangle is typically detected with $ \Tsq $-statistic. \label{fig:pcaVSvae}}	
	\end{centering}
\end{figure}


\subsection{Proposed Monitoring Statistic}

\label{sec:proposed-statistic}

\begin{figure}[t]
	\begin{centering}
		\includegraphics[width=0.9\textwidth]{Disentangled_Extrapolated.pdf}
		\par\end{centering}
	\caption{Illustration of incorrect latent mappings phenomena and how process control fails in latent space. \textbf{Bottom left:} The true latent variations of in-control samples are generated from the gray region, which is the probable region. Point A and Point B are extreme values along a dimension of variation. Point C is generated by an out-of-control process with a shift in latent distribution. Point D is generated by an out-of-control process with a shift in the residual distribution. The predicted counterpart of each point is denoted by an apostrophe (\eg, A' for A). \textbf{Top Left:} Observations of true latent variation in the high-dimensional space that lie close to a low-dimensional manifold. \textbf{Top Middle:} The encoder and decoder of VAE trained exclusively with in-control samples (\ie, the gray region in the observed space). \textbf{Bottom Middle:} Incorrectly mapped variation in the predicted latent space where gray region is the probable region. \textbf{Top Right:} Reconstructions of the variation in high-dimensions, with a failure in extrapolation beyond the in-control region. \label{fig:entang-extrap}}
\end{figure}

In this section, we will first reveal the shortcomings of the previously proposed VAE-based monitoring methodologies we present in \ref{sec:bckgrnd:critique}. Finally, we propose our methodology to improve the monitoring statistics proposed in \ref{sec:bckgrnd:critique}.

There are two major shortcomings of the previously proposed methodologies:
\begin{enumerate}
	\item Latent space statistics $H^{2},T^{2}$ and $D$ or any other formulation that relies exclusively on the latent representations $ \mbz \sim \qphizgivenx{\mbz}{\mbx}$ are unreliable for process monitoring. Thus, they should be discarded altogether from the monitoring framework since they will likely increase false alarms without contributing to the detection power in any meaningful way.
	\item Approximating the residual space statistic via Monte Carlo sampling is not computationally feasible if a certain level of detection accuracy is desired. 
\end{enumerate}
We will address these two shortcomings in \ref{sec:unreliable} and \ref{sec:efficiency}. 

\subsubsection{Unreliable Latent Space Statistics for Deep Autoencoders} \label{sec:unreliable}

First, we focus on the unreliability of latent space statistics. Let us first start with the case when the shift occurs in the latent distribution (i.e., $\pz$). According to the PCA-VAE analogy illustrated in \ref{fig:pcaVSvae}, latent space statistics are supposed catch such shifts, which are represented with triangular points in the same figure. While this may work for PCA-based monitoring, we claim that such an analogy cannot be straightforwardly made for VAE because neural network-based encoders in autoencoder architectures typically learn complex and incorrect mappings of latent variations. We illustrate this phenomenon in \ref{fig:entang-extrap}. The line segment ABC illustrates a traversal along a dimension of a latent variation. All the samples generated along the line segment AB are sampled from the probable region of the in-control process and their mappings are contained within the probable region of the predicted space. However, Point C is generated by an out-of-control process where there is a shift in latent distribution but its mapping incorrectly falls within the probable region. This leads to a false evidence that suggests that Point C is unlikely to be generated by an out-of-control process.

The reasons as to why incorrect mappings are learned by deep autoencoders have been studied in the deep learning literature. Interested readers are encouraged to refer to \textcite{AchilleS18} for a discussion of the properties of ideal latent representations and to \textcite{locatello2018challenging} for a discussion of the challenges around attaining one of these properties, namely, disentanglement. 
%For the sake of profile monitoring, we only need to acknowledge the fact that we cannot take perfect mapping for granted for deep autoencoder architectures. 
On the contrary, it is very likely that we end up with an imperfect mapping, especially with real-life datasets. Consequently, in Phase-II, samples generated by out-of-control processes that are characterized with a shift in the latent distribution, which will not be mapped consistently to the regions in the latent space which we consider to be unusual.

A natural question to ask at this point is how should we expect to detect shifts in latent distribution if we cannot rely on latent representations. Earlier, we argued that an analogue of a $ Q $-chart would catch such shifts. Our argument is based on another shortcoming of neural networks, namely, failure to extrapolate. Deep neural networks approximate well only at a bounded domain defined by where the training set---in our case, the set of in-control samples we use in Phase-I---is densely sampled from. The behavior of the function is unpredictable and often erroneous outside the training domain. In other words, it does not extrapolate well beyond the domain of training samples, which are the out-of-control samples in the context of our problem. We refer interested readers to \ref{app:rosenbrock}, where we replicate this phenomenon on a toy example. 

A decoder that fails to extrapolate is helpful since the it will struggle with generating profiles that are in the low density region of the in-control data distribution $ p(\mbx) $. In our context, this means the discrepancy between the true input and its generated counterpart will be larger for out-of-control samples than it is for in-control samples, regardless of the source of the shift. Thus, a monitoring statistic that is based on the residuals only would be efficient at covering both cases. We refer the reader back to \ref{fig:entang-extrap} for an illustration. There is a significant discrepancy between the observations and reconstructions of Point C and D, even though they are generated by different sources of shifts in the process. Thus, an extension of a residual space statistic should be able to catch both type of shifts.

\subsubsection{Improve the Computational Efficiency of the Residual Space Statistics} \label{sec:efficiency}

Now that we established our rationale behind the first shortcoming we claim to reveal, we move onto the second and focus on the previously proposed residual based statistics: $ SPE $ and $ R $. Both $SPE$ and $ R $ rely on samples from the proposal distribution for the estimation of the expectation. This approach requires a large number of samples to be generated, and thus a large number of the forward passes through the decoder network, which is prohibitively expensive in terms of computation when deployed in real-life systems. To overcome this problem, we propose a Taylor expansion based approximation. First, observe that $\log\decoding\propto\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}+C$ for all $\mbx$ and $\mbz$ because of the common isotropic covariance assumption. The constant $C$ can be discarded as noneffective in terms of control charting because it would only translate the limits and the statistics by the same amount for any given $\mbx$ and $\mbz$. We call the expression $\E_{\mbz\sim q_{\phi}}\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$ as the expected reconstruction error (ERE). The Taylor expansion for the first-order and second-order moment of ERE given the random variable $\mbz\sim\encoding$ can be derived analytically.
\begin{prop}
\label{prop:taylor-exp}
The first and second-order Taylor Expansion (denoted by $ERE_{1}$
and $ERE_{2}$ respectively) for the function $\E_{\mbz\sim q_{\phi}}\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
given the random variable $\mbz\sim\encoding$ where $\decoding=\Norm(\mu_{\mbphi}(\mbx),\diag(\mbsigma_{\mbphi}(x)))$
can be derived analytically as:
\begin{equation}
ERE_{1}=\gg\mbx-\mbmu_{\mbtheta}(\mbmu_{\mbphi}(\mbx))\gg_{2}^{2}\label{eq:ere-1}
\end{equation}
\begin{equation}
ERE_{2}=\gg\mbx-\mbmu_{\mbtheta}(\mbmu_{\mbphi}(\mbx))\gg_{2}^{2}+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\diag(\mbsigma_{\mbphi}(x)))\label{eq:ere-2}
\end{equation}
where $\mathbf{H}_{z}$ is the Hessian of the function $\gg x-\mbmu_{\mbtheta}(z)\gg_{2}^{2}$
with respect to $\mbz$. The derivation is provided in \ref{app:ere}.
\end{prop}

Given a trained VAE, $ERE_{1}$ can be computed efficiently by a single forward pass of the new profile from the process $\mbx$ through $\mbmu_{\mbphi}$ and $\mbmu_{\mbtheta}$ successively and calculating the squared prediction error, without the need for any sampling. $ERE_{2}$ requires the additional computation of the diagonal of the Hessian $\mathbf{H}_{z}$ and a relatively less expensive trace operation since the covariance is diagonal. Both $ERE_{1}$ and $ERE_{2}$ are residual based statistics that are accurate and efficient to compute, which addresses the two shortcomings we mentioned at the beginning of this section. In our experiments, we will evaluate the effectiveness of both of these statistics in comparison to previously proposed monitoring statistics for VAE.

\subsection{Profile Monitoring Procedure}

\label{sec:methodology:procedure} A typical profile monitoring follows
two phases: Phase-I analysis and Phase-II analysis. Phase-I analysis
focuses on understanding the process variability by training an appropriate
in-control mode and selecting an appropriate control limit. In our
case, Phase-I analysis results in a trained model (i.e., an encoder
and a decoder) and an Upper Control Limit (UCL) to help set up the
control chart for each of the monitoring statistics. In Phase-II,
the system is exposed to new profiles generated by the process in
real-time to decide whether these profiles are in-control or out-of-control.
Our experimentation plan, outlined below, is formulated to emulate
this scenario to effectively assess the performance of any combination
of a model, a test statistic and a disturbance scenario to generate
the out-of-control samples.
\begin{itemize}
\item Obtain in-control dataset $\dataset$ and partition it into train,
validation and test sets $\dataset^{trn}$, $\dataset^{val}$, $\dataset^{tst}$.
\item Train VAE using samples from $\dataset^{trn}$.
\item Calculate test statistic for all $\mbx\in\dataset^{val}$ and take
it's \nth{95} percentile as the UCL.
\item Start admitting profiles online from the process. Calculate test statistic
using the trained VAE. If test statistic is over UCL, identify the
sample as out-of-control.
\end{itemize}
We train 10 different model instances with different seeds to account
for inherent randomness due to the weight initialization of deep neural
networks.

\subsection{Neural Network Architectures and Training \label{subsec:Model-Architectures}}

In this work, we use convolutional neural networks for the encoders
and decoders in our VAE model to represent the spatial neighborhood
structures of the profiles. Introduced in \textcite{lecun1989backpropagation},
convolutional layers have enabled tremendous performance increase
in certain neural network applications where the data is of a certain
spatial neighborhood structure such as images or audio waveform. They
exploit an important observation of such data, where the learner should
be equivariant to translations. This is an important injection of
inductive bias into the network that largely reduces the number of
parameters compared to the fully connected network by the use of parameter
sharing. It eventually increases the statistical learning efficiency,
especially for small samples. It must be noted, however, convolutional
layers are not equivariant to scale and rotation as they are to translation.
Knowing what sort of inductive biases is injected into these layers
is important for the understanding of disentanglement, which we will
introduce later in this paper.

We use the encoder-decoder structure outlined in \ref{tab:model-architectures}.
The layers used that builds the model architectures used in this study
are summarized as follows:
\begin{itemize}
\item C($O,K,S,P$): Convolutional layer with arguments referring to the
number of output channels $O$, kernel size $K$, stride $S$ and
size of zero-padding $P$.
\item CT($O,K,S,P$): Convolutional transpose layer with arguments referring
to the number of output channels $O$, kernel size $K$, stride $S$,
and size of zero-padding $P$.
\item FC($I,O$): Fully connected layer with arguments referring to input
dimension $I$ and output dimension $O$.
\item A: Activation function. Leaky ReLU with a negative slope of $0.2$.
\end{itemize}
Here, C(), CT(), and FC() are considered the linear transformation
layers while R(), LR(), and S() are considered the nonlinear activation
layers. Strided convolutions can be used to decrease the spatial dimensions
in the encoders. Pooling layers are typically not recommended in autoencoder-like
architectures \parencite{radford2015unsupervised}. Convolutional
transpose layers are used to upscale latent codes back to ambient
dimensions.

The sequential order of the computational graphs used for this study
is summarized in \ref{tab:model-architectures}. The architecture choice is directly based on the encoder-decoder architecture that was used in \textcite{higgins2017beta}, except that we use Leaky ReLU with a negative slope of 0.2 as the activation which is advised in \textcite{radford2015unsupervised} for better gradient flow. The encoder  outputs $2r$ nodes, which is a concatenation of the inferred posterior mean $\mbmu_{\mbphi}(\mbx)$ and variance $\diag(\mbsigma(\mbx))$, both are of length $r$. The number of epochs per training is fixed at $1000$ and the learning rate and batch size are fixed at $0.001$ and $64$ respectively, both are chosen empirically to guarantee a meaningful convergence. Adam algorithm is used for first-order gradient optimization with parameters $(\beta_{1,}\beta_{2})=(0.9,0.999)$ as advised in \textcite{KingmaB14}. The model checkpoint is saved at every epoch where a better validation
loss is observed. The latest checkpoint is used as the final model.

In our experiments, the architecture and the training conditions described above are generated with respect to the convergence of the VAE objective on in-control dataset because in real life, the practitioner will not have access to out-of-control samples. The same setting worked well for both the simulation dataset and the case study dataset we consider in this paper. This gives us confidence that the selection is robust from one set to the other. However, a different dataset might benefit from adjustments to the above conditions. The adjustments should be based on monitoring the convergence of the VAE objective, as the procedure will benefit from a better approximated in-control distribution.

Image profiles comprise our simulation and case study, which will be introduced to the reader in the upcoming sections. This is why we only consider convolutional architectures. However, the monitoring statistics we propose in \ref{eq:ere-1} and \ref{eq:ere-2} are adaptable to other flavors of VAE architectures as well. 

The original VAE \parencite{Kingma2013-dl} was introduced with fully connected feedforward neural networks which are generally applicable to most kinds of process inputs. The monitoring statistics we proposed can be readily applied for that scenario without the need for further modification. 

For sequential profiles (e.g. time series), two alternative solutions can be used. First would be using one-dimensional convolutional layers. In that scenario, since we are still in the realm of convolutional architectures, no further modifications are needed. Alternatively, a recurrent neural network-based encoder and decoder structure might be efficient at capturing sequential dependencies, as outlined in \textcite{ChungKDGCB15}. A modification required for that method would be using a variational layer only between the last hidden state of the encoder and the initial generating state of the decoder, as opposed to having a VAE at every time step. In other words, the recurrent layers would be used as a way to encode the sequence-dependent signal into a vector, just as convolutional layers do the same for spatially-dependent signal. Given that modification, the proposed monitoring statistics can be used right away.

\begin{table}[!t]
\global\long\def\arraystretch{1.3}%
 \caption{Architecture details of deep neural networks used in this study\label{tab:model-architectures}}

\centering{}%
\begin{tabular}{l>{\raggedright}p{0.8\textwidth}}
\toprule 
Module & Architecture\tabularnewline
\midrule 
Encoder & C(32, 4, 2, 1) - A - C(32, 4, 2, 1) - A - C(64, 4, 2, 1) - A - C(64,
4, 2, 1) - A - C(64, 4, 1, 0) - FC(256, $2r$)\tabularnewline
Decoder & FC($r$, 256) - A - CT(64, 4, 0, 0) - A - CT(64, 4, 2, 1) - A - C(32,
4, 2, 1) - CT(32, 4, 2, 1) - A - CT(1, 4, 2, 1)\tabularnewline
\bottomrule
\end{tabular}
\end{table}


\section{Simulation Study Analysis and Results \label{sec:Simulation-Study-Analysis}}

In this section, we evaluate the proposed methodology via
a simulation study to test our claims we make in \ref{sec:proposed-statistic}
in a controlled environment over the data generating process. For
every experiment mentioned in this section, we follow the procedure
outlined in \ref{sec:methodology:procedure} and we use VAE models
with the architecture described in \ref{subsec:Model-Architectures}.

\subsection{Simulation Setup}
\label{sec:simsetting} 

We first evaluate the performance of the deep latent variable models in a simulation setting where we have explicit control over the latent variations. 
The simulation procedure produces 2D structured point clouds that resemble the scanned topology of a dome.

Let each pixel on a $64$ by $64$ grid be denoted by a tuple $\mbp=(p_{0},p_{1})$.
The values of the tuples stretch from $0$ to $1$, equally spaced, left to right and bottom-up. Each tuple takes a value based on its location through a function $\mbp\mapsto f(\mbp;c,r)+\epsilon$, where $\epsilon\sim\Norm(0,1\times10^{-2})$ is i.i.d Gaussian noise.
The function $f$ is parameterized by the horizontal location of the dome $c$, and the radius of the base of the dome $r$. The vertical location of the dome on the 2D surface is fixed at the vertical center of the surface. Given any parameter set $\{c,r\}$, each pixel $\mbp$ can be evaluated with the following logic: 
\begin{equation}
\begin{split}g(\mbp;c,r) & =1-\frac{(p_{0}-c)}{r}^{2}-\frac{(p_{1}-0.5)}{r}^{2}\\
f(\mbp;c,r) & =\begin{cases}
\sqrt{g(\mbp;c,r)} & \mbox{if }g(\mbp;c,r)\geq0\\
0 & \mbox{if }g(\mbp;c,r)<0
\end{cases}
\end{split}
\label{eq:gasketfun}
\end{equation}

The samples are best visualized as grayscale images as shown in \ref{fig:gasketgrid} below. 

\begin{figure}[t]
\centering{}\label{fig:gasketgrid} \includegraphics[width=0.5\linewidth]{gasket.pdf}
\caption{Dome profiles depicted as grayscale images simulated with radius and center location they coincide with on the axes.}
\end{figure}

The processes that generate the latent variations of in-control domes are defined as Gaussian distributions:
\begin{equation}
\begin{split}c\sim\Norm(0.5,1\times10^{-2})\\
r\sim\Norm(0.2,6.25\times10^{-4})
\end{split}
\end{equation}

As our out-of-control scenarios consider the following four distribution shifts in which $\delta$ denotes the intensity of the shift:
\begin{itemize}
\item \textbf{Location shift:} the mean of the process that generates $c$
is altered by an amount $\delta$ as in 
\[
c\sim\Norm(0.5+\delta\times10^{-2},1\times10^{-2})
\]
\item \textbf{Radius shift:} the mean of the process that generates $a$
is perturbed by an amount $\delta$ as in
\[
r\sim\Norm(0.2+\delta\times10^{-4},6.25\times10^{-4})
\]
\item \textbf{Mean shift}: all the pixels are added an additive disturbance
$\delta$ as in 
\[
f(\mbp;c,r)\leftarrow f(\mbp;c,r)+\delta
\]
\item \textbf{Magnitude shift:} all the pixels are added a multiplicative
disturbance $\delta$ as in 
\[
f(\mbp;c,r) \leftarrow f(\mbp;c,r)*\delta
\]
\end{itemize}

Note that the location shift and radius shift represent disturbances in latent distribution $p_{\delta}(\mbz)$. 
The other two cases, mean shift and magnitude shift, represent disturbances in the conditional distribution $p_{\delta}(\mbx\g\mbz)$.

We generate the training, validation, and testing sets for in-control domes as well as a set of each out-of-control scenario above.
All sets have exactly 500 distinct samples.
We generate these sets once, fix them and use them for the analyses in the subsequent sections.

\subsection{On the Incorrect Mapping of Latent Representations by the Encoder
\label{sec:simstudy:recognition}}
\begin{figure}[t]
	\subfloat[Fixed $r$, varying $c$\label{fig:Fixed-r-varying-c}]{\centering{}\includegraphics[scale=0.5]{fixed_radii.pdf}}
	
	\subfloat[Fixed $c$, varying $r$\label{fig:Fixed-c-varying-r}]{\begin{centering}
			\includegraphics[scale=0.5]{fixed_center_locs.pdf}
			\par\end{centering}
	}\caption{Figure depicting the discrepancy between the true and predicted latent representations of the encoder of a VAE with two-dimensional latent code trained with in-control samples. For each subfigure, plots on the left show where real factors of variation are sampled from and figure on the right is what the VAE encoder infers as the mean of the proposal distribution. In all figures, the regions that are considered to be in-control are represented with a dashed circle.\textbf{Top:} Real factors of variation are generated at three fixed levels of radius $r$ and varying values of center location $c$ on the left figure. Corresponding inferred means are plotted on the right graph. \textbf{Bottom:} Similar to (b) but with fixed center location $c$ at three levels and varying $r$.}
	\label{fig:proposals}
\end{figure}

In this section, we investigate the correctness of the latent representations produced by the encoder. 
In order to do so, we first train a VAE with an architecture as described in \ref{tab:model-architectures} and latent representation $ r $ is fixed at two. 
The training samples are generated by the in-control dome generation process as described in \ref{sec:simsetting}. 
We use the encoder of the trained VAE for the rest of the analysis.

After training, we generate samples from the simulation process by fixing one of the latent factor and traversing along the other. 
The plots on the left side of \ref{fig:proposals} depicts the traversals of the true latent space we sample the domes from. 
We then push these generated examples through the encoder to obtain their respective proposal distributions. 
The geometry in which the means of these distributions vary gives us an idea about the correctness of the mappings. 
This can be traced from the plots on the right in \ref{fig:proposals}. There, we observe gross incorrectness in the mapping of latent representations. For example, we can conclude right away that domes with extremely small radius $ r $ will likely go undetected if only the latent space statistic is used. The behavior along the variation of center location $ c $ is directly dependent on the fixed value of $ r $.

Overall, the findings are in agreement with our rationale behind the proposed statistic outlined in \ref{sec:proposed-statistic}. To reiterate, learned latent representations may not correctly be associated with the likelihood of observing sample due to incorrect mappings. This, in turn, will lead to an incorrect out-of-control likelihood assignment of an observed profile in Phase-II, if the monitoring statistic being used is a function of the latent representations only.

\subsection{On the Extrapolation Performance of the Decoder \label{sec:simstudy:generator}}

\ref{fig:manifold_vae} hints us about the extrapolation performance
of the decoder of the same VAE described in \ref{sec:simstudy:recognition}
trained on in-control samples described in \ref{sec:simsetting}.
It should be cross-examined with \ref{fig:proposals} above as the
encoder and decoder are tightly coupled to each other. We observe
two important behavior: the posterior gets distorted beyond two or
three standard deviations, and the representations are partially entangled
in line with the behavior of its encoder depicted in \ref{fig:proposals}.
To see how this will help to detect disturbances in latent space,
consider a dome that is extremely small in terms of the radius (i.e.,
small $r$) or at the very margins of the grid in terms of center
location (i.e., center location $c$ far from 0.5). Looking at \ref{fig:manifold_vae},
we can observe that the decoder simply cannot generate such a sample
because it does not extrapolate well in either of latent dimensions.
This will in turn, produce a larger reconstruction error, and thus
a larger monitoring statistic that will likely to fall outside the
control limit. Recall once again that the disturbance described is
purely on the latent distribution $p_{\delta}(\mbz)$ and yet our
proposed monitoring statistic will capture this behavior well thanks
to the extrapolation issues in the decoder.

\begin{figure}[H]
\includegraphics[width=0.9\linewidth]{manifold_vae.pdf} \caption{Latent space traversal and the response of the decoder of a VAE with
2-dimensional latent codes and trained with in-control dome samples.
Each row represents which latent dimension is traversed while the
other dimension is fixed at zero. Each column represents what value
is assigned to that latent dimension that is represented by the row
label. Each image in each cell is generated by the decoder using that
specific latent variable combination.}
\label{fig:manifold_vae}
\end{figure}


\subsection{On the Estimation of Log-likelihood Under Importance Sampling}

Earlier, we claimed that it would take too many Monte Carlo sampling
iterations to get a meaningful estimate of ERE defined as $\E_{\mbz\sim q_{\mbtheta}}\log\decoding$.
In this section, we test that claim on a random in-control sample
$\mbx$ using the proposal distribution $\mbz\sim\encoding$ which
is obtained via the encoder of the same VAE model we have been using
in this section. The results of the sampling-based estimation of ERE,
first-order approximation $ERE_{1}$, and second-order approximation
$ERE_{2}$ are shown in \ref{fig:Estimation-comparison-between} below.
The key observation is that it takes at least a few tens of Monte
Carlo iterations to get a stable and accurate estimation. At that
level, the single pass through the encoder is negligible. This means
using sampling will be more costly at least 50 samples to achieve
the same accuracy as the first-order approximation that we suggest
and at least 80 samples to get the accuracy of the second-order approximation.
Another important observation is that second-order approximation is
a bit more accurate than first-order approximation since it is closer
to the sample average approximation, but their difference is insignificant,
and it requires much more computation. In the next subsection, we
will evaluate the performance of $ERE_{2}$ and $ERE_{1}$ in Phase-II
monitoring to evaluate whether the added computational complexity
for $ERE_{2}$ is justifiable.

\begin{figure}[H]
\includegraphics[width=0.9\textwidth]{mc_vs_foa.pdf}

\caption{Estimation comparison between Monte Carlo sampling, first-order approximation
and second-order approximation. 95\% confidence interval band is shown
in the gray band and is based on simulations with ten different seeds.
\label{fig:Estimation-comparison-between}}
\end{figure}


\subsection{Comparison of Detection Performance of Proposed Statistics}

We now compare the proposed statistics based on how accurately they
detect profiles from out-of-control processes outlined in \ref{sec:simsetting}.
Note that for all statistics that require sampling, we obtain a single
sample and calculate the statistic based on that to keep the computational
demand the same for all statistics and emulate the computational constraints
of a real-life case. A preliminary result we must check is the robustness
of the statistics by making sure all proposed statistics have false
alarm rates on the held-out in-control test set, which should also
be less than the desired rate 5\%. \ref{tab:far} demonstrates that
this is the case for all of them.

\begin{table}[t]
\global\long\def\arraystretch{1.3}%
 \caption{False alarm rates on the held-out dataset averaged over 10 replications
per model and monitoring statistic. Standard deviations are in parentheses.\label{tab:far}}

\begin{tabular}{llllll}
\toprule 
Statistic & ERE1 & SPE/R & D & H2 & T2\tabularnewline
 & 0.041(0.006) & 0.051(0.005) & 0.044(0.004) & 0.052(0.005) & 0.043(0.009)\tabularnewline
\bottomrule
\end{tabular}
\end{table}

Through \ref{fig:disturbance_on_pxz}, we observe a clear superiority
of $ERE_{1}$ and $ERE_{2}$ over other methods when the disturbance
is on the observable space (top row). Latent space statistics
$D$, $H^{2}$ and $T^{2}$ fail in this case since that they are
purely computed using the proposal distribution latent variables.
$ERE_{1}$ and $ERE_{2}$ also outperform $SPE/R$, although by a
smaller margin it has with the latent variable-statistics. Between
$ERE_{1}$ and $ERE_{2}$, it's hard to claim which one works better
since their mean performances are quite close to each other.

For the latter two disturbances occurring purely on latent dimensions,
results are presented in the bottom row of \ref{fig:disturbance_on_pxz}.
The key observations can be listed as follows:
\begin{itemize}
\item We observe mixed results but generally $ERE_{1}$ and $ERE_{2}$,
$D$ and $H^{2}$ tend to perform better than $SPE/R$ and $T^{2}$.
A commonality between the former three is that they do not rely on
random samples, supporting our argument against this practice.
\item Observe the radius shift-type disturbance show in the bottom left
figure. Even though $H^{2}$ performs better on positive intensities
(larger radii), it completely misses negative intensities (smaller
radii). We foresaw this result in \ref{sec:simstudy:recognition}.
To reiterate, disentanglement and the lack of extrapolation in the
encoder is the reason behind this. We would also suggest that this
result can extend to all the latent-variable based statistics.
\item Unlike latent space statistics, $ERE_{1}$ and $ERE_{2}$
and $SPE/R$ behave more robustly against varying intensities. In
other words, the detection rate increase with increased intensities
consistently. Among these, we observe that $ERE_{1}$ and $ERE_{2}$
consistently outperform $SPE/R$.
\item $ERE_{1}$ and $ERE_{2}$ perform very similarly. In this case, we
conclude that the second-order information does not help too much
for the Phase-II monitoring. The reason behind this is that the second-order
information also comes from the encoder. However, given that the encoders
is trained on in-control samples and may provide inaccurate information
in the out-of-control regions, the second-order information for out-of-control
samples would be bias. Therefore, it does not provide additional gain
for monitoring performance.
\end{itemize}
\begin{figure}[!t]
\includegraphics[width=1\linewidth]{disturbance_on_pxz_vae_only.pdf}
\caption{Fault detection rates (y-axis) for varying intensities (x-axis) of
different disturbance types (quadrants). Bands represent 95\% confidence
interval estimated around mean detection rates.}
\label{fig:disturbance_on_pxz}
\end{figure}

As mentioned, in a real-life process, disturbances on the residual
space is often more likely than the disturbance in the latent space.
Therefore, we would recommend the use of residual space monitoring
statistics. Among all residual space monitoring statistics, we conclude
that $ERE_{1}$ performance the best considering the accuracy, robustness
and computational demand. This will be further validated through the
case study analysis.

\section{Case Study Analysis \& Results}

\label{sec:case-study} % TODO (@DS): how many anomaly samples do we have?
% TODO (@DS): Add a figure to illustrate both normal and abnormal samples, with one image in each class

Our dataset consists of defect image profiles from a hot-steel rolling
process, which is shown in \ref{fig:Rolling}. There are 13 classes
of surface defect types identified by the domain engineers. Four of
these classes---0,1,9 and 11---are considered minor defects and
they constitute our in-control set. There are in total 338 images
in these classes. The other nine classes make up the out-of-control
cases and they have in combination 3351 images to report detection
accuracy for. We randomly partition the in-control corpus to fix train,
validate and test sets with 60\%-20\%-20\% relative sizes, respectively.
The rest of the procedure followed is outlined in \ref{sec:methodology:procedure}.
Same as in the simulation study, to account for randomness in weight
initialization, we replicate the experiment with 10 different seeds.
For comparison, we also include the monitoring performance with the
traditional PCA method with the same residual space control chart,
denoted as PCA-Q. The results are summarized in \ref{tab:rolling_results}
below.

\begin{sidewaystable}
\global\long\def\arraystretch{1.3}%
 \caption{Summary of fault detection rates on out-of-control cases averaged
over 10 replications per model and monitoring statistic. Standard
deviations are in parentheses. Bolded values represent the maximum
average across different statistics. \label{tab:rolling_results}}

\centering{}%
\begin{tabular}{llllllll}
Model & \multicolumn{6}{c}{VAE} & \multicolumn{1}{c}{PCA}\tabularnewline
Statistic & D & H2 & T2 & SPE/R & ERE & ERE2 & Q\tabularnewline
Fault ID &  &  &  &  &  &  & \tabularnewline
\midrule 
2 & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.37(0.03) & 0.44(0.06) & \textbf{0.50}(0.06) & 0.00(0.00)\tabularnewline
3 & 0.17(0.06) & 0.23(0.04) & 0.03(0.03) & 0.84(0.01) & 0.85(0.01) & \textbf{0.86}(0.01) & 0.78(0.00)\tabularnewline
4 & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.62(0.02) & \textbf{0.75}(0.05) & 0.71(0.05) & 0.56(0.00)\tabularnewline
5 & 0.58(0.07) & 0.62(0.09) & 0.00(0.00) & \textbf{1.00}(0.00) & \textbf{1.00}(0.00) & \textbf{1.00}(0.00) & 0.99(0.00)\tabularnewline
6 & 0.06(0.03) & 0.15(0.08) & 0.05(0.05) & 0.79(0.01) & \textbf{0.80}(0.01) & \textbf{0.80}(0.00) & 0.52(0.00)\tabularnewline
7 & 0.01(0.01) & 0.01(0.01) & 0.00(0.00) & 0.13(0.01) & \textbf{0.17}(0.01) & 0.15(0.00) & 0.11(0.00)\tabularnewline
8 & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.64(0.02) & \textbf{0.70}(0.07) & 0.69(0.01) & 0.34(0.00)\tabularnewline
10 & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.49(0.03) & \textbf{0.57}(0.05) & \textbf{0.57}(0.04) & 0.29(0.00)\tabularnewline
12 & 0.00(0.00) & 0.00(0.00) & 0.00(0.00) & 0.79(0.01) & \textbf{0.80}(0.02) & \textbf{0.80}(0.02) & 0.69(0.00)\tabularnewline
13 & 0.00(0.00) & 0.00(0.00) & 0.01(0.00) & 0.71(0.04) & \textbf{0.77}(0.02) & 0.76(0.02) & 0.56(0.00)\tabularnewline
\bottomrule
\end{tabular}
\end{sidewaystable}

From \ref{tab:rolling_results}, we can observe that $ERE_{1}$ and
$ERE_{2}$ consistently outperforms all other monitoring statistic
formulations. The divide between residual space statistics and latent space statistics observed in the simulation study is further
validated here too. The inferiority of latent space statistics
are much more obvious here in the real case study, as we observe for
most out-of-control classes the detection rate is simply zero. This
observation further validates our claims that in practice, for deep
autoencoders, the change happens in the residual space rather than
the latent space. The advantage of VAE over PCA is mainly due to the
better representative power and data compression ability of deep autoencoders
compared to PCA. It is worth noting that the superiority of VAE over
PCA for process monitoring was also demonstrated in the earlier works
in various applications \parencite{Zhang2019-lu,wang2019systematic,lee2019process}.

To support our claim of the ineffectiveness of latent space
statistics, we refer the reader to \ref{fig:kde} below. We observe
how well separated the statistics are for $ERE_{1}$ and $SPE/R$
while for latent space statistics the obtained values are
mostly overlapping. Note that we omitted $ERE_{2}$ because it was
almost identical to $ERE_{1}$. To obtain a deeper understanding of
the results, we point out in \ref{fig:Output-of-the} for the original
images and their reconstructions. The decoder is persistent on generating
samples that look like in-control rolling samples with little fidelity
to how the original defect sample looks like. When \ref{fig:Original-profiles}
and \ref{fig:Reconstructions-via-VAE} are cross-examined, it is apparent
why reconstruction error would be high. On the contrary, \ref{fig:Inferred-means}
shows that most latent representations fall into the region that would
be considered in-control from a profile monitoring perspective. We
observe instances of class 3,5,6 and 7 generate the latent variables
in the out-of-control regions. However, even for these classes, $SPE/R$,
$ERE_{1}$ and $ERE_{2}$ yields much better detection power than
$D$, $H_{2}$, and $T_{2}$, as it can be seen in \ref{tab:rolling_results}.
In conclusion, we would like to suggest the use of $ERE_{1}$ for
deep autoencoders, which is consistent with our findings in the simulation
study.

\begin{figure}[H]
\begin{subfigure}[b]{0.3\textwidth}
     \centering
     \includegraphics[width=\textwidth]{SPE.pdf}
     \caption{$ERE_{1}$}
     \label{fig:kde:ERE}
 \end{subfigure}
 \hspace{0.1\textwidth}
 \begin{subfigure}[b]{0.3\textwidth}
     \centering
     \includegraphics[width=\textwidth]{R.pdf}
     \caption{$SPE/R$}
     \label{fig:kde:spe-r}
 \end{subfigure}
 
\begin{subfigure}[b]{0.3\textwidth}
     \centering
     \includegraphics[width=\textwidth]{H2.pdf}
     \caption{$H^{2}$}
     \label{fig:kde:h2}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
     \centering
     \includegraphics[width=\textwidth]{D.pdf}
     \caption{$D$}
     \label{fig:kde:D}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.3\textwidth}
     \centering
     \includegraphics[width=\textwidth]{T2.pdf}
     \caption{$T^{2}$}
     \label{fig:kde:T2}
\end{subfigure}

\caption{Kernel density estimation plots of statistics obtained for in-control
and out-of-control steel defect profiles, per each proposed statistic
type.\label{fig:kde}}
\end{figure}

\begin{figure}[H]
\begin{minipage}[t]{0.25\textwidth}%
\subfloat[Original profiles\label{fig:Original-profiles}]{\includegraphics[width=0.99\textwidth]{casestudy_orig.pdf}}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.25\textwidth}%
\subfloat[Reconstructions via VAE\label{fig:Reconstructions-via-VAE}]{\includegraphics[width=0.82\columnwidth]{casestudy_recons.pdf}
}%
\end{minipage}\hfill{}%
\begin{minipage}[t]{0.45\textwidth}%
\subfloat[Inferred means\label{fig:Inferred-means}]{\includegraphics[width=1\textwidth]{casestudy_scat.pdf}

}%
\end{minipage}\caption{Output of the VAE decoder and the encoder for randomly select rolling
profiles. \textbf{Left: }Original profiles visualized. Each row is
a class of defect profile and each column is a randomly selected from
that class. \textbf{Middle: }Reconstructions of the samples with one-to-one
correspondence to the samples on the image to the left. \textbf{Right:
}Inferred mean locations of each of the defects visualized on left.
Points are annotated by their class IDs. \label{fig:Output-of-the}}
\end{figure}


\section{Conclusion \label{sec:conclusions}}

In this paper, we focused on evaluating Phase-II monitoring statistics
proposed so far in the literature for VAE and demonstrate that they
were not performing optimally in terms of accuracy and/or computational
feasibility. First, we classified the monitoring statistics in the latent-variable space and residual space and show how these are natural extension of monitoring statistics for PCA.
Second, we further pointed out two important issues related to the
latent variables learned by the encoder of VAE, namely, entanglement
and failure to extrapolate. Based on these issues, unlike PCA, we demonstrated that latent
space statistics should be discarded altogether with both
conceptual explanations and real experiments. Third, we pointed out
that the residual space statistics based on sampling will require
too many samples to be computationally feasible. Finally, we proposed
a novel formulation by deriving the Taylor expansion of expected reconstruction
error that addresses both accuracy and computational efficiency.

To support our claim, our simulation study first demonstrated the
entanglement and lack of extrapolation of latent space statistics
and its inferior Phase-II monitoring performance. It further showed
that the derived statistics based on the residual space is more robust
and more accurate than all the other statistics proposed so far. Finally,
we validated the superiority of our formulation on a real-life case
study, where steel defect image profiles are used.

\printbibliography


\appendix
%dummy  inserted by tex2lyx to ensure that this paragraph is not empty\refalias{section}{appendix}

\section{Proof of Proposition 3.1 \label{sec:PoofOfPropTQ}}

The Kullback-Leibler divergence between two multivariate Gaussian
distributions has a closed-form solution. If we define these distributions
as $p_{0}=N(\mbz;\mbmu_{0},\mbSigma_{0})$ and $p_{1}=N(\mbz;\mbmu_{1},\mbSigma_{1})$
where $\mbmu$ and $\mbSigma$ are respective mean vectors and covariance
matrices, then according to \textcite{hershey2007approximating}
the closed-form solution will be the following: 
\begin{align}
\KL{p_{0}}{p_{1}} & =\frac{1}{2}[\log\frac{\g\mbSigma_{1}\g}{\g\mbSigma_{0}\g}+Tr(\mbSigma_{1}\inv\mbSigma_{0})-r+(\mbmu_{0}-\mbmu_{1})^{\top}\mbSigma_{1}\inv(\mbmu_{0}-\mbmu_{1})]\label{eq:kld-closed-form}
\end{align}
Since $\encoding=\Norm(\mbmu(\mbx),\mbSigma_{z})$ and $p(\mbz)=\Norm(0,\mbI)$,
we can derive that 
\begin{align}
\KL{\encoding}{p(\mbz)} & =\frac{1}{2}\left[-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r\right]+\frac{1}{2}\mbmu(\mbx)^{\top}\mbmu(\mbx)\nonumber \\
 & =\frac{1}{2}\mbmu(\mbx)^{\top}\mbmu(\mbx)+C,\label{eq:kld-prior}
\end{align}
where $C=-\log\g\mbSigma_{z}\g+Tr(\mbSigma_{z})-r$ is a constant,
which doesn't depend on $\mbx$.

To derive the SPE statistics, we will derive

\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2}\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbx^{\top}\mbx-2\mbz^{\top}\mbW\mbx+\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\label{eq: spew}
\end{align}

Here, we know that 
\begin{align}
 & \mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & \mathbb{E}_{\mbz\sim q_{\mbtheta}}tr(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
= & tr\left(\mbW^{\top}\mbW\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz\mbz^{\top})\right)\nonumber \\
= & tr\left(\mbW^{\top}\mbW(\mbmu(\mbx)\mbmu(\mbx)^{\top}+\Sigma_{z})\right)\nonumber \\
= & \mbmu(\mbx)^{\top}\mbW^{\top}\mbW\mbmu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\label{eq: tracezwwz}
\end{align}

Therefore, by plugging \ref{eq: tracezwwz} into \ref{eq: spew},
we have 
\begin{align}
\mathbb{E}_{\mbz\sim q_{\mbtheta}}\|\mbx-\mbW\mbz\|^{2} & =\mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mathbb{E}_{\mbz\sim q_{\mbtheta}}(\mbz^{\top}\mbW^{\top}\mbW\mbz)\nonumber \\
 & =\mbx^{\top}\mbx-2\mbmu(\mbx)^{\top}\mbW\mbx+\mbmu(\mbx)^{\top}\mbW^{\top}\mbW\mbmu(\mbx)+tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)\nonumber \\
 & =\|\mbx-\mbW\mbmu(\mbx)\|^{2}+C\label{eq:q-to-ere}
\end{align}
where $C=tr\left(\mbW^{\top}\mbW\Sigma_{z}\right)$ that does not
depend on $\mbx$.

\section{A Toy Example to Demonstrate Out-of-distribution Behavior of Neural
Networks \label{app:rosenbrock}}

Assume using a multilayer perceptron, we are trying to approximate
the famous Rosenbrock function $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$
given $(a,b)=(1,100)$. In this small experiment, we sample tuples
of two-dimensional points from a bounded region $(x_{i},y_{i})\in[-1,3]\times[-2,3]$.
We use a multilayer perceptron with six hidden layers and a hundred
neurons in each layer. Half of the points are used in training, and
the other half is used as a validation set to optimize hyper-parameters.
Using the trained network, we plot the actual Rosenbrock function
along with the neural network approximation in \ref{fig:Rosenbrock}.
Notice how well the function is approximated for the region $[-1,3]\times[-2,3]$,
but there is a serious discrepancy between the approximated and the
real outside of the region. This is a small yet to the point example
of out-of-distribution issues with neural networks.

\begin{figure}
\begin{centering}
\includegraphics[width=1\textwidth]{rosenbrock.pdf}
\par\end{centering}
\caption{Rosenbrock function (green surface) approximated by an multilayer
perceptron(red surface) given training (black crosses) and validation
(black dots) samples form a bounded region $(x_{i},y_{i})\in[-1,3]\times[-2,3]$.\label{fig:Rosenbrock}}
\end{figure}


\section{ERE Testing Statistic Derivation \label{app:ere}}

To derive the $ERE_{1}$ and $ERE_{2}$, we first define $R(z)=\|y-\mbmu_{\mbtheta}(z)\|^{2}$
as the reconstruction error (RE). The quantity we would like approximate
is $E_{\mbz\sim q_{\phi}}R(\mbz)$ where $\encoding=\Norm(\mbmu_{\mbphi}(\mbx),\Sigma_{z})$.
We are looking for the Taylor expansion of the expected RE (ERE) around
$z_{0}=\mbmu_{\mbphi}(\mbx)$, i.e., the first moment. For notational
simplicity, we use $H_{z}$ to denote the Hessian $R''(\mbmu_{\mbphi}(\mbx))$.
The derivation is formalized as follows:

\begin{align}
E_{\mbz\sim q_{\phi}}R(\mbz) & =R(\mbmu_{\mbphi}(\mbx))+R'(\mbmu_{\mbphi}(\mbx))E_{\mbz\sim q_{\phi}}[\mbz-\mbmu_{\mbphi}(\mbx)])\nonumber \\
 & \quad+\frac{1}{2}E_{\mbz\sim q_{\phi}}[(\mbz-\mbmu_{\mbphi}(\mbx))^{\top}\mathbf{H}_{z}(\mbz-\mbmu_{\mbphi}(\mbx))]+O(\|(\mbz-\mbmu_{\mbphi}(\mbx)\|^{3}\nonumber \\
 & \simeq R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}E_{\mbz\sim q_{\phi}}[(\mbz-\mbmu_{\mbphi}(\mbx))^{\top}\mathbf{H}_{z}(\mbz-\mbmu_{\mbphi}(\mbx))]\nonumber \\
 & =R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}E[(\mbz-\mbmu_{z})(\mbz-\mbmu_{z})^{T}])\nonumber \\
 & =R(\mbmu_{\mbphi}(\mbx))+\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\Sigma_{z})\label{eq:derived-ere2}
\end{align}

Note for $ERE_{1}$, the second term $\frac{1}{2}\mathrm{tr}(\mathbf{H}_{z}\Sigma_{z})$
is droped and we are left with $R(\mbmu_{\mbphi}(\mbx))$ only. For $ERE_{2}$,
since $\Sigma_{z}$ is a diagonal matrix, $\mathrm{tr}(\mathbf{H}_{z}S_{z})=\mathrm{tr}(diag(\mathbf{H}_{z})S_{z})=\sum_{i}(\mathbf{H}_{z})_{ii}(S_{z})_{ii}$
holds. We can utilize this result to compute $ERE_{2}$, in a more computationally
efficient manner.
\end{document}
